# -*- coding: utf-8 -*-
# =============================================================================
# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì´ˆê¸°í™”
# =============================================================================

import sys
import io
import hashlib

# Windowsì—ì„œ UTF-8 ì¶œë ¥ ì§€ì›
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import OpenDartReader
import pandas as pd
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
import json
import re
import time
import os
import requests
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
import numpy as np
import torch
import importlib.metadata as importlib_metadata
from packaging import version as pkg_version
from functools import lru_cache
from threading import Lock
from collections import defaultdict

try:
    from transformers.utils import import_utils as hf_import_utils
except ImportError:
    hf_import_utils = None

# Hugging Faceê°€ torch>=2.6 ì •ì‹ ë²„ì „ì„ ìš”êµ¬í•˜ë¯€ë¡œ, nightly(dev) ë¹Œë“œë¥¼ ì‚¬ìš© ì¤‘ì´ë©´
# ë²„ì „ ë¬¸ìì—´ì„ 2.6.0ìœ¼ë¡œ ë³´ì •í•´ ì•ˆì „ì¥ì¹˜ë¥¼ ìš°íšŒí•œë‹¤.
if hasattr(torch, "__version__"):
    _torch_ver = torch.__version__
    if isinstance(_torch_ver, str) and "dev" in _torch_ver and _torch_ver.startswith("2.6.0"):
        torch.__version__ = "2.6.0"

def _allow_dev_torch_for_hf():
    if hf_import_utils is None or not hasattr(hf_import_utils, "is_torch_greater_or_equal"):
        return
    original_fn = hf_import_utils.is_torch_greater_or_equal

    def _is_dev_sufficient(target_version: str) -> bool:
        try:
            current_ver = importlib_metadata.version("torch")
        except importlib_metadata.PackageNotFoundError:
            return False
        if "dev" not in current_ver:
            return False
        try:
            base_ver = pkg_version.parse(current_ver).base_version
            return pkg_version.parse(base_ver) >= pkg_version.parse(target_version)
        except Exception:
            return False

    @lru_cache(maxsize=None)
    def _patched(library_version: str, accept_dev: bool = False) -> bool:
        if _is_dev_sufficient(library_version):
            return True
        return original_fn(library_version, accept_dev=accept_dev)

    hf_import_utils.is_torch_greater_or_equal = _patched

_allow_dev_torch_for_hf()

try:
    from FlagEmbedding import BGEM3FlagModel  # type: ignore
except ImportError:
    BGEM3FlagModel = None

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ==============================================================================
# 1. ì„¤ì • (API Key ì…ë ¥)
# ==============================================================================
# .env íŒŒì¼ì—ì„œ API Key ë¡œë“œ
DART_API_KEY = os.getenv('DART_API_KEY')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# ê°ì²´ ì´ˆê¸°í™”
dart = OpenDartReader(DART_API_KEY)
# ë¶„ì„ìš© LLM (gpt-5-mini)
llm = ChatOpenAI(model="gpt-5-mini", openai_api_key=OPENAI_API_KEY)
# ìš”ì•½ìš© LLM (gpt-5-nano)
summary_llm = ChatOpenAI(model="gpt-5-nano", openai_api_key=OPENAI_API_KEY)

# í…ŒìŠ¤íŠ¸ ëª¨ë“œ í”Œë˜ê·¸ (LLM í˜¸ì¶œ ë¹„í™œì„±í™”ìš©)
ENABLE_LLM = False

# ë””ë²„ê¹…ìš©: íŠ¹ì • í‹°ì»¤ì˜ ë§ˆí¬ë‹¤ìš´ì„ íŒŒì¼ë¡œ ë¤í”„
DEBUG_DUMP_TICKERS = {'005930','000660','009540', '207940', '010140'}  # í•„ìš” ì‹œ ë‹¤ë¥¸ í‹°ì»¤ ì¶”ê°€

# ìºì‹œ ë° ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
CACHE_ENABLED = True
CACHE_DIR = "cache"
BATCH_SIZE = int(os.getenv('BATCH_SIZE', '50'))
BATCH_PAUSE_SECONDS = float(os.getenv('BATCH_PAUSE_SECONDS', '2.0'))
MAX_WORKERS = int(os.getenv('MAX_WORKERS', '3'))
MAX_LLM_CHARS = int(os.getenv('MAX_LLM_CHARS', '50000'))

# ë©´ì±…/ì£¼ì˜ ì„¹ì…˜ ì œê±° ê´€ë ¨ ìƒìˆ˜
DISALLOWED_HEADING_KEYWORDS = [
    "ì˜ˆì¸¡ì •ë³´",
    "ì£¼ì˜ì‚¬í•­",
    "ë©´ì±…",
    "íˆ¬ìì˜ì‚¬ê²°ì •ì—í•„ìš”í•œì‚¬í•­",
    "íˆ¬ìì˜ì‚¬ê²°ì •ì—í•„ìš”í•œ",
    "ì£¼ì˜ì‚¬í•­ì…ë‹ˆë‹¤",
]

ALLOW_HEADINGS_AFTER_SKIP = [
    "ê°œìš”",
    "ì¬ë¬´ìƒíƒœ",
    "ì˜ì—…ì‹¤ì ",
    "ìœ ë™ì„±",
    "ë¶€ì™¸ê±°ë˜",
    "ì¤‘ì ì¶”ì§„ì „ëµ",
    "ê²½ì˜ì§„ë‹¨",
    "ì‹œì¥ì „ë§",
    "ì‚¬ì—…ì „ëµ",
    "í•µì‹¬ì „ëµ",
    "ë¦¬ìŠ¤í¬ê´€ë¦¬",
]

TOXIC_CONTENT_KEYWORDS = [
    "ì˜ˆì¸¡ì •ë³´",
    "ë¯¸ë˜ì‚¬ê±´",
    "ë³´ì¥í• ìˆ˜ì—†ìœ¼ë©°",
    "ë©´ì±…",
    "ì ì¬ì ìœ„í—˜",
    "ì •ì •ë³´ê³ ì„œë¥¼ê³µì‹œí• ì˜ë¬´",
]

DISCLAIMER_REGEXES = [
    r"ì˜ˆì¸¡ì •ë³´[^#]+?ì •ì •ë³´ê³ ì„œë¥¼\s*ê³µì‹œí• \s*ì˜ë¬´ëŠ”\s*ì—†ìŠµë‹ˆë‹¤\.",
    r"ë³¸\s+ìë£ŒëŠ”\s+ë¯¸ë˜ì—\s+ëŒ€í•œ\s+ì˜ˆì¸¡ì •ë³´[^#]+?(?:ì•ŠìŠµë‹ˆë‹¤|ì—†ìŠµë‹ˆë‹¤)\.",
]

FINAL_DISCLAIMER_KEYWORDS = ["ì˜ˆì¸¡ì •ë³´", "ì£¼ì˜ì‚¬í•­", "ì •ì •ë³´ê³ ì„œë¥¼", "ë©´ì±…"]

# ==============================================================================
# 1.5 ì„ë² ë”© ê¸°ë°˜ í•„í„°ë§ ì„¤ì •
# ==============================================================================
USE_EMBEDDING_FILTER = True
EMBEDDING_MODEL_NAME = 'BAAI/bge-m3'
EMBEDDING_BATCH_SIZE = 24
EMBEDDING_TOP_K = 6
EMBEDDING_MIN_SIM = 0.28
HEADER_MIN_SIM = 0.45
HEADER_TOP_K = 6

TOPIC_DESCRIPTIONS = {
    'products_services': [
        "ì´ íšŒì‚¬ëŠ” ì–´ë–¤ ì œí’ˆì´ë‚˜ ì„œë¹„ìŠ¤ë¥¼ ì–´ë–¤ ê³ ê°ì—ê²Œ ì œê³µí•˜ë©° ê²½ìŸ ìš°ìœ„ë¥¼ ì–´ë–»ê²Œ ì„¤ëª…í•˜ëŠ”ì§€ ì–¸ê¸‰í•œ ë¬¸ë‹¨",
        "ì£¼ìš” ì‚¬ì—…ë¶€ë³„ ì œí’ˆ ë¼ì¸ì—…ê³¼ íŒë§¤ ì±„ë„, ëŒ€í‘œ ê³ ê° ì‚¬ë¡€ë¥¼ ì„œìˆ í•œ ë¬¸ë‹¨",
        "ë§¤ì¶œ ë¹„ì¤‘, í’ˆëª©ë³„ ë§¤ì¶œ, ë‚´ìˆ˜ ë° ìˆ˜ì¶œ êµ¬ì„±, ì£¼ìš” ìƒí‘œ, ìƒí’ˆ ë° ìš©ì—­ ë‚´ì—­ì„ ë‚˜ì—´í•œ ë¬¸ë‹¨",
        "ë°˜ë„ì²´ ë©”ëª¨ë¦¬, ë¹„ë©”ëª¨ë¦¬, íŒŒìš´ë“œë¦¬, íŒ¨í‚¤ì§• ê³µì • ë° ì£¼ë ¥ ì œí’ˆêµ° ì„¤ëª…",
        "2ì°¨ì „ì§€ ì–‘ê·¹ì¬, ìŒê·¹ì¬, ë¶„ë¦¬ë§‰, ì „í•´ì•¡, ì…€, ëª¨ë“ˆ, íŒ© ì œí’ˆ ë¼ì¸ì—…",
        "ìë™ì°¨ ì „ë™í™” ëª¨ë¸(EV), ë‚´ì—°ê¸°ê´€, PBV, ìƒìš©ì°¨ ë° ë¶€í’ˆ í¬íŠ¸í´ë¦¬ì˜¤",
        "ì¡°ì„  LNGì„ , ì»¨í…Œì´ë„ˆì„ , VLCC, ì¹œí™˜ê²½ ì„ ë°•, í•´ì–‘í”ŒëœíŠ¸ ê±´ì¡° ì‹¤ì ",
        "ë°”ì´ì˜¤ ì˜ì•½í’ˆ íŒŒì´í”„ë¼ì¸, ì„ìƒ ë‹¨ê³„, ë¼ì´ì„ ìŠ¤ ì•„ì›ƒ, CDMO ì„œë¹„ìŠ¤ í˜„í™©",
        "ì¸í„°ë„· í”Œë«í¼ ì„œë¹„ìŠ¤, ê´‘ê³ , ì»¤ë¨¸ìŠ¤, ì½˜í…ì¸ , ê²Œì„ IP ë° í¼ë¸”ë¦¬ì‹± í˜„í™©",
        "ê¸ˆìœµ ì˜ˆì ê¸ˆ, ëŒ€ì¶œ ìƒí’ˆ, ì‹ ìš©ì¹´ë“œ, ë³´í—˜, ì¦ê¶Œ ì¤‘ê°œ ë° IB ì„œë¹„ìŠ¤",
        "ê±´ì„¤ ì£¼íƒ ë¶„ì–‘, í† ëª© ê³µì‚¬, í”ŒëœíŠ¸ ìˆ˜ì£¼, ê°œë°œì‚¬ì—… í¬íŠ¸í´ë¦¬ì˜¤"
    ],
    'supply_chain': [
        "í•µì‹¬ ì›ì¬ë£Œë‚˜ ë¶€í’ˆ, ì¡°ë‹¬ì²˜, ê³µê¸‰ ë¦¬ìŠ¤í¬, ìƒì‚°ì„¤ë¹„ í˜„í™©ì„ ì„¤ëª…í•œ ë¬¸ë‹¨",
        "ì›ì¬ë£Œ ë§¤ì…ì•¡, ê°€ê²© ë³€ë™ ì¶”ì´, ì£¼ìš” ë§¤ì…ì²˜, ê³µê¸‰ ê³„ì•½, ê°€ë™ë¥ , ìƒì‚°ëŠ¥ë ¥, CAPEXë¥¼ ë‹¤ë£¬ ë¬¸ë‹¨",
        "ë°˜ë„ì²´ ì›¨ì´í¼, ë…¸ê´‘ì¥ë¹„, íŠ¹ìˆ˜ê°€ìŠ¤ ì¡°ë‹¬ ë° ê³µê¸‰ë§ ê´€ë¦¬",
        "ë°°í„°ë¦¬ í•µì‹¬ê´‘ë¬¼(ë¦¬íŠ¬, ë‹ˆì¼ˆ, ì½”ë°œíŠ¸) ì¡°ë‹¬ ë° ì „êµ¬ì²´ ê³µê¸‰ë§",
        "ìë™ì°¨ ì°¨ëŸ‰ìš© ë°˜ë„ì²´, êµ¬ë™ëª¨í„°, ë°°í„°ë¦¬ ì…€ ìˆ˜ê¸‰ ë° ë¶€í’ˆ í˜‘ë ¥ì‚¬ ê´€ë¦¬",
        "ì¡°ì„  í›„íŒ(ê°•ì¬), ì„ ë°• ì—”ì§„, ê¸°ìì¬ ì¡°ë‹¬ ë° ë„í¬ ê°€ë™ë¥  í˜„í™©",
        "ê±´ì„¤ ì² ê·¼, ì‹œë©˜íŠ¸, ë ˆë¯¸ì½˜ ë“± ì£¼ìš” ìì¬ ê°€ê²© ë³€ë™ ë° ìˆ˜ê¸‰ ì¶”ì´"
    ],
    'sales_orders': [
        "ë§¤ì¶œ êµ¬ì„±, ìˆ˜ì£¼ ì”ê³ , ì§€ì—­ ë° ê³ ê°ë³„ ë§¤ì¶œ ë¹„ì¤‘ ë³€í™”ë¥¼ ì„¤ëª…í•œ ë¬¸ë‹¨",
        "ìˆ˜ì£¼ ì”ê³ , ìˆ˜ì£¼ ìƒí™©, íŒë§¤ ê²½ë¡œ, íŒë§¤ ë°©ë²• ë° ì¡°ê±´, ì£¼ìš” ë§¤ì¶œì²˜ ë¹„ì¤‘ì„ ë‚˜ì—´í•œ ë¬¸ë‹¨",
        "ìˆ˜ì£¼ ì‚°ì—…(ì¡°ì„ , ê±´ì„¤, ë°©ì‚°)ì˜ ì‹ ê·œ ìˆ˜ì£¼, ìˆ˜ì£¼ ì”ê³ , ì¸ë„ ì¼ì • ë° ì§„í–‰ë¥ ",
        "ìë™ì°¨ íŒë§¤ëŒ€ìˆ˜, ì§€ì—­ë³„ íŒë§¤ ì‹¤ì , ASP(í‰ê· íŒë§¤ë‹¨ê°€) ë³€ë™ ì¶”ì´",
        "í”Œë«í¼ MAU, DAU, ARPU, ìœ ë£Œ ê°€ì…ì ìˆ˜, GMV(ê±°ë˜ì•¡) ë“± í•µì‹¬ ì§€í‘œ",
        "ê¸ˆìœµ ìˆœì´ìë§ˆì§„(NIM), ì˜ˆëŒ€ìœ¨, ìˆ˜ìˆ˜ë£Œ ìˆ˜ìµ, ë³´í—˜ë£Œ ìˆ˜ìµ ì¶”ì´"
    ],
    'research_development': [
        "ì—°êµ¬ê°œë°œ(R&D) ì¡°ì§ êµ¬ì„±, ì—°êµ¬ê°œë°œë¹„ ì§€ì¶œ ì¶”ì´ ë° ë§¤ì¶œì•¡ ëŒ€ë¹„ ë¹„ì¤‘",
        "ì£¼ìš” ì—°êµ¬ ì‹¤ì , ì‹ ì œí’ˆ ê°œë°œ í˜„í™©, íŠ¹í—ˆ ë° ì§€ì‹ì¬ì‚°ê¶Œ ë³´ìœ  í˜„í™©",
        "ì‹ ì•½ í›„ë³´ë¬¼ì§ˆ íƒìƒ‰, ì„ìƒì‹œí—˜ ì§„í–‰ ê²½ê³¼, ì‹ ê¸°ìˆ  ìƒìš©í™” ë¡œë“œë§µ",
        "ìë™ì°¨/ë°°í„°ë¦¬ ì°¨ì„¸ëŒ€ í”Œë«í¼ ê°œë°œ, ê³ ì²´ ë°°í„°ë¦¬, ììœ¨ì£¼í–‰ ê¸°ìˆ  ì—°êµ¬"
    ],
    'strategy_outlook': [
        "ê²½ì˜ì§„ì´ ì œì‹œí•œ ì¤‘ì¥ê¸° ì „ëµ, ì‹ ì‚¬ì—… ê³„íš, íˆ¬ì ê³„íš, ì„±ì¥ ë™ë ¥ì„ ë‹¤ë£¬ ë¬¸ë‹¨",
        "ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨ ë° ë¶„ì„ì˜ê²¬, ì¬ë¬´ìƒíƒœ ë° ì˜ì—…ì‹¤ì  ë¶„ì„, ì‹œì¥ ì „ë§, ì™¸ë¶€ ë³€ìˆ˜ ëŒ€ì‘ì„ ì„¤ëª…í•œ ë¬¸ë‹¨",
        "í”Œë«í¼/ì½˜í…ì¸  ê¸°ì—…ì˜ ì‹ ì‘ ì¶œì‹œ ì¼ì •, IP í™•ì¥, ê¸€ë¡œë²Œ ì§„ì¶œ ì „ëµ, AIÂ·í´ë¼ìš°ë“œ íˆ¬ì ê³„íšì„ ë‹¤ë£¬ ë¬¸ë‹¨"
    ],
    'financial_summary': [
        "ì‚¬ì—…ë¶€ë¬¸ë³„ ì‹¤ì  ìš”ì•½, ì†ìµ ë° ìì‚°ì§€í‘œ, ìë³¸/ìœ ë™ì„± ê´€ë¦¬ ë‚´ìš©ì„ ì •ë¦¬í•œ ë¬¸ë‹¨",
        "ìˆœì´ìë§ˆì§„(NIM), ì˜ˆëŒ€ë§ˆì§„, ìˆ˜ìˆ˜ë£Œ ìˆ˜ìµ, ì˜ˆìƒì†ì‹¤ì¶©ë‹¹ê¸ˆ, BIS ë¹„ìœ¨ ë“± ê¸ˆìœµì§€í‘œë¥¼ ë‚˜ì—´í•œ ë¬¸ë‹¨",
        "EBITDA, ì˜ì—…ì´ìµë¥ , CAPEX, ë°°ë‹¹ ì •ì±…, í˜„ê¸ˆíë¦„, ë ˆë²„ë¦¬ì§€ ë¹„ìœ¨ ë“± ì¬ë¬´ ìš”ì•½ì„ ê¸°ìˆ í•œ ë¬¸ë‹¨"
    ],
    'risk_management': [
        "ìœ„í—˜ ê´€ë¦¬ ì •ì±…, íŒŒìƒìƒí’ˆÂ·í—·ì§€ ì „ëµ, í™˜ìœ¨ ë° ê¸ˆë¦¬ ë¯¼ê°ë„ì— ëŒ€í•´ ì„¤ëª…í•œ ë¬¸ë‹¨",
        "ì‹œì¥ìœ„í—˜ê´€ë¦¬, ì‹ ìš©ìœ„í—˜, ìœ ë™ì„±ìœ„í—˜, ìë³¸ê´€ë¦¬, í—·ì§€ ë¹„ìœ¨, ì¶©ë‹¹ê¸ˆ ì •ì±…ì„ ë‚˜ì—´í•œ ë¬¸ë‹¨",
        "ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨ ë° ë¶„ì„ì˜ê²¬ì—ì„œ ì¬ë¬´ìƒíƒœì™€ ì˜ì—…ì‹¤ì ì„ ë¶„ì„í•˜ë©° ì¤‘ìš” ìœ„í—˜ìš”ì¸ì„ ì–¸ê¸‰í•œ ë¬¸ë‹¨"
    ]
}

HEADER_TARGETS = {
    'products_services': [
        "ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤", "ì˜ì—…ì˜ í˜„í™©", "ë§¤ì¶œ ë¹„ì¤‘", "ìƒí’ˆ", "ìš©ì—­", "ìˆ˜ìˆ˜ë£Œ ìˆ˜ìµ",
        "ì„œë¹„ìŠ¤ ë°ì´í„°", "í”Œë«í¼ ì§€í‘œ", "ì‚¬ì—…ë¶€ë³„ ë§¤ì¶œ êµ¬ì„±",
        "ì°¨ì¢… ë¼ì¸ì—…", "í”Œë«í¼ ì´ìš©ì§€í‘œ", "ì½˜í…ì¸ ", "ê²Œì„ í¬íŠ¸í´ë¦¬ì˜¤",
        "ì‹ íƒ", "ëŒ€ì¶œ", "ì˜ˆìˆ˜ê¸ˆ", "ë³´í—˜ë£Œ"
    ],
    'supply_chain': [
        "ì›ì¬ë£Œ ë° ìƒì‚°ì„¤ë¹„", "ë§¤ì…", "ì¡°ë‹¬", "ìê¸ˆì¡°ë‹¬ ë° ìš´ìš©", "ë¹„ìš© êµ¬ì¡°",
        "í›„íŒ", "ì›¨ì´í¼", "ë¦¬íŠ¬", "ì–‘ê·¹ì¬", "ê³µê¸‰ë§",
        "ë¶€í’ˆ ì¡°ë‹¬", "ë„í¬ ê°€ë™ë¥ ", "ê¸°ìì¬", "ì—”ì§„ ìˆ˜ê¸‰", "ì›ì¬ë£Œ ê°€ê²©",
        "ìƒì‚°ëŠ¥ë ¥", "ìƒì‚°ì‹¤ì ", "ê°€ë™ë¥ "
    ],
    'sales_orders': [
        "ë§¤ì¶œ ë° ìˆ˜ì£¼ìƒí™©", "íŒë§¤ ê²½ë¡œ", "ìˆ˜ì£¼ ì”ê³ ", "ì˜ì—…ì‹¤ì ",
        "íŒë§¤ ê³„íš", "ì¸ë„ ì¼ì •", "ê³ ê°ë³„ ë§¤ì¶œ ë¹„ì¤‘",
        "íŒë§¤ëŒ€ìˆ˜", "ë„Â·ì†Œë§¤ íŒë§¤ ì‹¤ì ", "ì„ ê°€", "ìˆ˜ì£¼ì”ê³ ", "ë°œì£¼ í˜„í™©",
        "ê°€ì…ì", "ì´ìš©ì", "íŠ¸ë˜í”½", "ì§€í‘œ"
    ],
    'research_development': [
        "ì—°êµ¬ê°œë°œí™œë™", "ì—°êµ¬ê°œë°œë¹„", "ì—°êµ¬ê°œë°œì‹¤ì ", "ì§€ì ì¬ì‚°ê¶Œ",
        "íŠ¹í—ˆ", "ì„ìƒì‹œí—˜", "ì‹ ì•½", "íŒŒì´í”„ë¼ì¸", "ê¸°ìˆ ì œíœ´"
    ],
    'strategy_outlook': [
        "ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨ ë° ë¶„ì„ì˜ê²¬", "ì‚¬ì—…ì˜ ê°œìš”", "ì¤‘ì  ì¶”ì§„ ì „ëµ", "ì‹ ê·œ ì‚¬ì—…",
        "ì‹œì¥ ì „ë§", "ì‚°ì—… í™˜ê²½", "ì„±ì¥ ì „ëµ", "ê²½ìŸ ìš°ìœ„"
    ],
    'risk_management': [
        "ìœ„í—˜ê´€ë¦¬", "ì‹œì¥ìœ„í—˜", "íŒŒìƒìƒí’ˆ", "ìš°ë°œì±„ë¬´", "ì œì¬",
        "í™˜ìœ¨", "ìœ ê°€", "ê¸ˆë¦¬", "ë¦¬ìŠ¤í¬", "ìë³¸ì ì •ì„±"
    ],
    'financial_summary': [
        "ì¬ë¬´ì— ê´€í•œ ì‚¬í•­", "ìê¸ˆ ì¡°ë‹¬ ë° ìš´ìš©", "ì¬ë¬´ìƒíƒœ", "ìë³¸ ê´€ë¦¬",
        "ìš”ì•½ ì¬ë¬´ì •ë³´", "ì†ìµ", "ë°°ë‹¹"
    ]
}

_embedding_model = None
_topic_vectors = None
_header_vectors = None
_embedding_lock = Lock()

def ensure_embedding_model():
    global _embedding_model, _topic_vectors, _header_vectors
    if not USE_EMBEDDING_FILTER:
        return False
    if BGEM3FlagModel is None:
        print("  -> [Embed] FlagEmbedding ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. `pip install FlagEmbedding` ì´í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return False
    if _embedding_model is None:
        with _embedding_lock:
            if _embedding_model is None:
                print("  -> [Embed] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (ì´ ì‘ì—…ì€ 1íšŒë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤)")
                device = "cuda:0" if torch.cuda.is_available() else "cpu"
                use_fp16 = device != "cpu"
                try:
                    _embedding_model = BGEM3FlagModel(
                        EMBEDDING_MODEL_NAME,
                        use_fp16=use_fp16,
                        devices=device
                    )
                    print(f"  -> [Embed] {device} ì¥ì¹˜ì— ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (fp16={use_fp16})")
                except Exception as exc:
                    print(f"  -> [Error] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {exc}")
                    return False

                def build_vectors(source_dict):
                    entries = []
                    for key, sentences in source_dict.items():
                        for sentence in sentences:
                            entries.append((key, sentence))
                    if not entries:
                        return []
                    texts = [entry[1] for entry in entries]
                    encoded = _embedding_model.encode(texts, batch_size=len(texts), max_length=8192)
                    dense = encoded['dense_vecs']
                    vectors = []
                    for idx, vec in enumerate(dense):
                        norm = np.linalg.norm(vec)
                        normalized = vec if norm == 0 else vec / norm
                        vectors.append({'topic': entries[idx][0], 'vector': normalized})
                    return vectors

                _topic_vectors = build_vectors(TOPIC_DESCRIPTIONS)
                _header_vectors = build_vectors(HEADER_TARGETS)
                print("  -> [Embed] ì„ë² ë”© ëª¨ë¸ ë° ì£¼ì œ ë²¡í„° ì¤€ë¹„ ì™„ë£Œ")
    return True

def embed_texts(texts):
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
    if not ensure_embedding_model():
        return None
    encoded = _embedding_model.encode(texts, batch_size=min(EMBEDDING_BATCH_SIZE, len(texts)), max_length=8192)
    dense = encoded['dense_vecs']
    dense = np.array(dense)
    norms = np.linalg.norm(dense, axis=1, keepdims=True)
    norms[norms == 0] = 1e-9
    return dense / norms

def split_markdown_into_chunks(markdown_text):
    """í—¤ë”(#)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§ˆí¬ë‹¤ìš´ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"""
    lines = markdown_text.split('\n')
    chunks = []
    current_heading = "ê°œìš”"
    buffer = []
    for line in lines:
        stripped = line.strip()
        if stripped.startswith('#'):
            if buffer:
                chunk_text = '\n'.join(buffer).strip()
                if chunk_text:
                    chunks.append({'heading': current_heading, 'text': chunk_text})
                buffer = []
            current_heading = stripped
            buffer.append(line)
        else:
            buffer.append(line)
    if buffer:
        chunk_text = '\n'.join(buffer).strip()
        if chunk_text:
            chunks.append({'heading': current_heading, 'text': chunk_text})
    return chunks

def clean_heading_label(heading_line):
    if not heading_line:
        return "ê°œìš”"
    label = heading_line.replace('#', '').strip()
    label = re.sub(r'^\d+[\.\)]\s*', '', label)
    label = re.sub(r'^[IVXLCDM]+\.\s*', '', label, flags=re.IGNORECASE)
    label = re.sub(r'^[ê°€-í£]+\.\s*', '', label)
    return label.strip() or "ê°œìš”"

def semantic_select_sections(markdown_text, ticker=None):
    """í—¤ë” ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ì„¹ì…˜ ì„ ë³„"""
    if not USE_EMBEDDING_FILTER or not markdown_text.strip():
        return None
    if not ensure_embedding_model():
        return None
    if not _header_vectors:
        return None
    chunks = split_markdown_into_chunks(markdown_text)
    chunks = filter_disallowed_chunks(chunks)
    headings = []
    valid_chunks = []
    for chunk in chunks:
        heading = chunk.get('heading')
        if heading:
            headings.append(clean_heading_label(heading))
            valid_chunks.append(chunk)
    if not headings:
        return None
    embeddings = embed_texts(headings)
    if embeddings is None:
        return None
    scored = []
    for idx, emb in enumerate(embeddings):
        scores = [(entry['topic'], float(np.dot(emb, entry['vector']))) for entry in _header_vectors]
        if not scores:
            continue
        best_topic, best_score = max(scores, key=lambda x: x[1])
        if best_score >= HEADER_MIN_SIM:
            scored.append({
                'topic': best_topic,
                'score': best_score,
                'heading': headings[idx],
                'text': valid_chunks[idx]['text']
            })
    if not scored:
        print("  -> [Embed] í—¤ë” ì‹œë§¨í‹± ë¼ìš°íŒ… ê²°ê³¼ ì—†ìŒ, ë°±ì—… ê²½ë¡œ ì‚¬ìš©")
        return None
    scored.sort(key=lambda x: x['score'], reverse=True)
    selected = scored[:HEADER_TOP_K]
    print("  -> [Embed] í—¤ë” ê¸°ë°˜ ì„ íƒ ê²°ê³¼:")
    for item in selected:
        preview = item['heading'][:60]
        print(f"     â€¢ {preview} -> {item['topic']} (score={item['score']:.3f}, len={len(item['text'])}ì)")
    combined = '\n\n'.join(item['text'] for item in selected if item['text'].strip())
    if ticker:
        print(f"  -> [Embed] {ticker} í—¤ë” ê¸°ë°˜ ì¶”ì¶œ ê¸¸ì´: {len(combined)}ì")
    return combined if combined.strip() else None

def select_relevant_chunks(markdown_text, ticker=None):
    """ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ì²­í¬ ì„ ë³„"""
    if not markdown_text or not markdown_text.strip():
        return ""

    semantic_text = None
    if USE_EMBEDDING_FILTER:
        semantic_text = semantic_select_sections(markdown_text, ticker=ticker)
        if semantic_text and len(semantic_text) > 200:
            return semantic_text

    chunks = split_markdown_into_chunks(markdown_text)
    chunks = filter_disallowed_chunks(chunks)
    if not chunks:
        return markdown_text

    chunk_texts = [chunk['text'] for chunk in chunks]
    embeddings = embed_texts(chunk_texts) if USE_EMBEDDING_FILTER else None
    if embeddings is None:
        return markdown_text

    scored_chunks = []
    topic_buckets = defaultdict(list)
    for idx, emb in enumerate(embeddings):
        topic_scores = [(entry['topic'], float(np.dot(emb, entry['vector']))) for entry in _topic_vectors]
        best_topic, best_score = max(topic_scores, key=lambda x: x[1])
        chunk_info = {
            'heading': chunks[idx]['heading'],
            'text': chunks[idx]['text'],
            'score': best_score,
            'topic': best_topic
        }
        scored_chunks.append(chunk_info)
        topic_buckets[best_topic].append(chunk_info)

    if not scored_chunks:
        return markdown_text

    if topic_buckets:
        print("  -> [Embed] ì£¼ì œë³„ í›„ë³´ ì²­í¬ ìš”ì•½:")
        for topic, items in topic_buckets.items():
            items.sort(key=lambda x: x['score'], reverse=True)
            total_chars = sum(len(it['text']) for it in items)
            top_preview = ", ".join(
                f"{clean_heading_label(it['heading'])[:25]}({it['score']:.3f})"
                for it in items[:3]
            )
            print(f"     â€¢ {topic}: {len(items)}ê°œ / {total_chars}ì | ìƒìœ„: {top_preview}")

    scored_chunks.sort(key=lambda x: x['score'], reverse=True)
    filtered = [chunk for chunk in scored_chunks if chunk['score'] >= EMBEDDING_MIN_SIM]
    if not filtered:
        filtered = scored_chunks[:EMBEDDING_TOP_K]
    else:
        filtered = filtered[:EMBEDDING_TOP_K]

    print("  -> [Embed] ì²­í¬ ê¸°ë°˜ ë°±ì—… ì„ íƒ ê²°ê³¼:")
    for chunk in filtered:
        preview = clean_heading_label(chunk['heading'])[:40]
        print(f"     â€¢ {preview} | topic={chunk['topic']} | score={chunk['score']:.3f} | len={len(chunk['text'])}ì")

    selected_text = '\n\n'.join(chunk['text'] for chunk in filtered)
    selected_len = sum(len(chunk['text']) for chunk in filtered)
    original_len = len(markdown_text)
    ratio = (selected_len / original_len * 100) if original_len else 0
    print(f"  -> [Embed] ì„ íƒ í…ìŠ¤íŠ¸ ê¸¸ì´: {selected_len}ì (ì›ë¬¸ ëŒ€ë¹„ {ratio:.1f}%)")

    if ticker:
        print(f"  -> [Embed] {ticker} ë°±ì—… ì„ íƒ ì²­í¬ ìˆ˜: {len(filtered)} (ì´ {len(chunks)}ê°œ ì¤‘)")
        debug_dump_markdown(ticker, "ì„ë² ë”©ì„ íƒ", selected_text, prefix='selected_')
    final_text = selected_text if selected_text.strip() else markdown_text
    return drop_unwanted_sections(final_text)

# í…ŒìŠ¤íŠ¸í•  10ê°œ ê¸°ì—… ë¦¬ìŠ¤íŠ¸ (ë‹¤ì–‘í•œ ì„¹í„° êµ¬ì„±)
DEFAULT_TARGET_COMPANIES = {
    '005930': 'ì‚¼ì„±ì „ì',       # ë°˜ë„ì²´/ê°€ì „
    '000660': 'SKí•˜ì´ë‹‰ìŠ¤',     # ë°˜ë„ì²´
    '373220': 'LGì—ë„ˆì§€ì†”ë£¨ì…˜',  # 2ì°¨ì „ì§€
    '005380': 'í˜„ëŒ€ì°¨',         # ìë™ì°¨
    '035420': 'NAVER',         # í”Œë«í¼
    '003490': 'ëŒ€í•œí•­ê³µ',       # í•­ê³µ/ìš´ì†¡ (ìœ ê°€ ë¯¼ê°)
    '009540': 'HDí˜„ëŒ€ì¤‘ê³µì—…',    # ì¡°ì„  (ìˆ˜ì£¼ ì‚°ì—…)
    '207940': 'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', # ë°”ì´ì˜¤ (CDMO)
    '010140': 'ì‚¼ì„±ì¤‘ê³µì—…',      # ì¡°ì„ 
    '035720': 'ì¹´ì¹´ì˜¤'          # í”Œë«í¼
}

# ì¶”ê°€ ê²€ì¦ìš© 10ê°œ ê¸°ì—… (ë‹¤ë¥¸ ì„¹í„°/ê¸°ì—… êµ¬ì„±)
ADDITIONAL_TARGET_COMPANIES = {
    '051910': 'LGí™”í•™',        # í™”í•™/2ì°¨ì „ì§€
    '068270': 'ì…€íŠ¸ë¦¬ì˜¨',      # ë°”ì´ì˜¤
    '000270': 'ê¸°ì•„',          # ìë™ì°¨
    '028260': 'ì‚¼ì„±ë¬¼ì‚°',      # ë³µí•©ìƒì‚¬/ê±´ì„¤
    '105560': 'KBê¸ˆìœµ',        # ê¸ˆìœµì§€ì£¼
    '055550': 'ì‹ í•œì§€ì£¼',      # ê¸ˆìœµì§€ì£¼
    '017670': 'SKí…”ë ˆì½¤',      # í†µì‹ 
    '034730': 'SK',            # ì§€ì£¼/ì—ë„ˆì§€
    '096770': 'SKì´ë…¸ë² ì´ì…˜',  # ì—ë„ˆì§€/í™”í•™
    '051900': 'LGìƒí™œê±´ê°•'     # ì†Œë¹„ì¬
}

KRX_LIST_PATH = "news-insight-backend/data/krx_sector_industry.csv"

def load_all_companies():
    """KRX ì „ì²´ ìƒì¥ì‚¬ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ"""
    # ê²½ë¡œê°€ í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€ ìƒëŒ€ê²½ë¡œì¸ì§€ í™•ì¸
    # test_file.pyëŠ” fintech í´ë”ì— ìˆê³ , news-insight-backendëŠ” ê·¸ í•˜ìœ„ì— ìˆìŒ
    # ë”°ë¼ì„œ fintech/news-insight-backend/... ê°€ ì•„ë‹ˆë¼ news-insight-backend/... ê°€ ë§ìŒ (ì‹¤í–‰ ìœ„ì¹˜ ê¸°ì¤€)
    # í•˜ì§€ë§Œ ì ˆëŒ€ ê²½ë¡œë¡œ ì°¾ëŠ”ê²Œ ì•ˆì „í•¨
    script_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(script_dir, KRX_LIST_PATH)
    
    if not os.path.exists(file_path):
        # í˜¹ì‹œ ì‹¤í–‰ ìœ„ì¹˜ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì ˆëŒ€ ê²½ë¡œ ì¬ì‹œë„
        print(f"  -> [Warn] íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨: {file_path}")
        # workspace root ê¸°ì¤€ ì‹œë„
        workspace_root = os.path.dirname(script_dir) # C:\Users\Admin\WORKSPACE\Cursor
        file_path_v2 = os.path.join(workspace_root, "fintech", KRX_LIST_PATH)
        if os.path.exists(file_path_v2):
            file_path = file_path_v2
        else:
            print(f"  -> [Error] ê¸°ì—… ëª©ë¡ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {KRX_LIST_PATH}")
            return {}
    
    try:
        df = pd.read_csv(file_path, dtype={'Code': str})
        companies = {}
        for _, row in df.iterrows():
            code = str(row['Code']).strip()
            name = str(row['Name']).strip()
            # ìš°ì„ ì£¼ ì œì™¸ (ì½”ë“œ ëì´ 0ì´ ì•„ë‹Œ ê²½ìš°)
            if len(code) == 6 and code.endswith('0'):
                companies[code] = name
                
        print(f"  -> [Info] ì´ {len(companies)}ê°œ ê¸°ì—… ë¡œë“œ ì™„ë£Œ (ìš°ì„ ì£¼ ì œì™¸)")
        return companies
    except Exception as e:
        print(f"  -> [Error] ê¸°ì—… ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

# í™˜ê²½ ë³€ìˆ˜ë¡œ ì‹¤í–‰ ëŒ€ìƒ ì„¸íŠ¸ë¥¼ ì „í™˜í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
TARGET_SET = os.getenv('TARGET_SET', 'default').lower()

if TARGET_SET == 'all':
    print(f"  -> [Mode] ì „ì²´ ê¸°ì—… ë¶„ì„ ëª¨ë“œ (Source: CSV)")
    TARGET_COMPANIES = load_all_companies()
    if not TARGET_COMPANIES:
        print("  -> [Warn] ì „ì²´ ê¸°ì—… ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©")
        TARGET_COMPANIES = DEFAULT_TARGET_COMPANIES
elif TARGET_SET == 'additional':
    TARGET_COMPANIES = ADDITIONAL_TARGET_COMPANIES
else:
    TARGET_COMPANIES = DEFAULT_TARGET_COMPANIES

# ==============================================================================
# 2. [Cleaning] HTML -> Markdown ë³€í™˜
# ==============================================================================
def clean_html_to_markdown(html_content):
    if not html_content:
        return ""

    soup = BeautifulSoup(html_content, 'html.parser')
    for tag in soup(['script', 'style', 'img', 'svg', 'path']):
        tag.decompose()
    
    cleaned_html = str(soup)
    text = md(cleaned_html, heading_style="ATX", strip=['a'], newline_style="BACKSLASH")
    text = re.sub(r'\n\s+\n', '\n\n', text)
    text = re.sub(r' +', ' ', text)
    
    # ì¶”ê°€: ë°˜ë³µ êµ¬ì¡° ì œê±° (í† í° ì ˆì•½)
    # 1. ì—°ì†ëœ ë¹ˆ ì¤„ ì••ì¶•
    text = re.sub(r'\n{3,}', '\n\n', text)
    # 2. í‘œì˜ ë¹ˆ ì…€ ì œê±°
    text = re.sub(r'\|\s*\|\s*\|', '|', text)
    
    text = filter_boilerplate_references(text)
    
    return text

def filter_boilerplate_references(text):
    """'~ì°¸ì¡° ë°”ëë‹ˆë‹¤' ë“± ë„¤ë¹„ê²Œì´ì…˜ ë¬¸ì¥ì„ ì œê±°í•˜ì—¬ í† í° ì ˆì•½"""
    if not text:
        return ""
    
    nav_patterns = [
        r"ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤",
        r"ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤",
        r"ì°¸ì¡° ë°”ëë‹ˆë‹¤",
        r"ì°¸ì¡°ë°”ëë‹ˆë‹¤",
        r"ì°¸ê³  ë°”ëë‹ˆë‹¤",
        r"ì°¸ê³ ë°”ëë‹ˆë‹¤",
        r"ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤",
        r"í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤",
        r"ê¸°ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤",
        r"ê¸°ì¬ë˜ì–´ìˆìŠµë‹ˆë‹¤"
    ]
    value_keywords = [
        "í†µí™”ì„ ë„", "ìŠ¤ì™‘", "ìŠ¤ì™€í”„", "ì„ ë¬¼", "ì˜µì…˜", "íŒŒìƒ", "í—·ì§€", "í—¤ì§€",
        "ìœ„í—˜íšŒí”¼", "ë§¤ë§¤", "ê³„ì•½", "ì²´ê²°", "í‰ê°€", "ì†ìµ", "ì”ì•¡",
        "%", "ì›", "ë‹¬ëŸ¬", "ì–µì›", "ë°°ëŸ´", "í†¤"
    ]
    ref_patterns = [
        r"['\"ã€Œ].+['\"ã€]\s*(ì„|ë¥¼)?\s*ì°¸(ì¡°|ê³ )",
        r"ìƒì„¸ ë‚´ìš©ì€\s*['\"ã€Œ].+['\"ã€]"
    ]
    
    lines = text.split('\n')
    filtered_lines = []
    for line in lines:
        clean_line = line.strip()
        if len(clean_line) < 2:
            continue
        is_nav = any(re.search(pat, clean_line) for pat in nav_patterns)
        has_value = any(keyword in clean_line for keyword in value_keywords)
        ref_hit = any(re.search(pat, clean_line) for pat in ref_patterns)
        if (is_nav or ref_hit):
            if has_value:
                ref_positions = [idx for idx in (clean_line.find("ì°¸ê³ "), clean_line.find("ì°¸ì¡°")) if idx != -1]
                if ref_positions:
                    cut_idx = min(ref_positions)
                    trimmed = clean_line[:cut_idx].rstrip(" ,.-")
                    if trimmed:
                        filtered_lines.append(trimmed)
                    continue
            else:
                continue
        filtered_lines.append(line)
    
    cleaned = '\n'.join(filtered_lines).strip()
    return cleaned or ""

def ensure_directory(path):
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)

def cache_file_path(ticker, text_hash):
    ensure_directory(CACHE_DIR)
    return os.path.join(CACHE_DIR, f"{ticker}_{text_hash}.json")

def load_cached_result(ticker, text_hash):
    if not CACHE_ENABLED:
        return None
    path = cache_file_path(ticker, text_hash)
    if not os.path.exists(path):
        return None
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return None

def save_cached_result(ticker, text_hash, data):
    if not CACHE_ENABLED or not data:
        return
    path = cache_file_path(ticker, text_hash)
    payload = {
        "ticker": ticker,
        "hash": text_hash,
        "data": data
    }
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

HEADING_REGEXES = [
    r'^#{1,6}\s+',
    r'^\d+[\.\)]\s*',
    r'^[IVXLCDM]+\.\s*',
    r'^[ê°€-í£]\.\s*',
    r'^[A-Z]\.\s*'
]

UNWANTED_SECTION_KEYWORDS = [
    "ì˜ˆì¸¡ì •ë³´ì— ëŒ€í•œ ì£¼ì˜ì‚¬í•­",
    "ë²•ê·œìƒì˜ ê·œì œ",
    "ë²•ê·œìƒì˜ ê·œì œì— ê´€í•œ ì‚¬í•­",
    "í™˜ê²½ ë° ê¸°íƒ€ ë²•ê·œìƒì˜ ê·œì œ",
    "íŒŒìƒìƒí’ˆ ë° ìœ„í—˜ê´€ë¦¬ì •ì±…",
    "íŒŒìƒìƒí’ˆ ë° ìœ„í—˜ ê´€ë¦¬ì •ì±…"
]

def _looks_like_heading(stripped_line):
    if not stripped_line:
        return False
    for pattern in HEADING_REGEXES:
        if re.match(pattern, stripped_line):
            return True
    return False

def _normalize_heading_text(raw_heading):
    if not raw_heading:
        return ""
    normalized = raw_heading.replace('#', ' ').strip()
    normalized = re.sub(r'^\d+[\.\)]\s*', '', normalized)
    normalized = re.sub(r'^[IVXLCDM]+\.\s*', '', normalized, flags=re.IGNORECASE)
    normalized = re.sub(r'^[ê°€-í£]\.\s*', '', normalized)
    normalized = re.sub(r'\s+', '', normalized)
    return normalized

def is_disallowed_heading(heading_line):
    normalized = _normalize_heading_text(heading_line)
    return any(keyword in normalized for keyword in DISALLOWED_HEADING_KEYWORDS)

def filter_disallowed_chunks(chunks):
    if not chunks:
        return []
    filtered = []
    for chunk in chunks:
        heading = chunk.get('heading')
        if heading and is_disallowed_heading(heading):
            continue
        filtered.append(chunk)
    return filtered

def remove_disclaimer_paragraphs(text):
    if not text:
        return text
    cleaned = text
    for pattern in DISCLAIMER_REGEXES:
        cleaned = re.sub(pattern, '\n', cleaned, flags=re.S)
    return cleaned

def final_disclaimer_sweep(text, ticker=None):
    if not text or not text.strip():
        return text
    cleaned = drop_unwanted_sections(text)
    if any(keyword in cleaned for keyword in FINAL_DISCLAIMER_KEYWORDS):
        cleaned = remove_disclaimer_paragraphs(cleaned)
        if any(keyword in cleaned for keyword in FINAL_DISCLAIMER_KEYWORDS):
            label = ticker or "ë¬¸ì„œ"
            print(f"  -> [Warn] {label} ë©´ì±… ë¬¸ë‹¨ì´ ë‚¨ì•„ìˆì–´ ì¬ì°¨ ì œê±°í–ˆìœ¼ë‚˜ ì¼ë¶€ê°€ ì”ì¡´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    return cleaned

def truncate_text(text, limit=MAX_LLM_CHARS, label=None):
    if not text or limit <= 0:
        return text
    if len(text) <= limit:
        return text
    tag = f"{label} " if label else ""
    print(f"  -> [Info] {tag}í…ìŠ¤íŠ¸ ê¸¸ì´ {len(text)}ì â†’ {limit}ìë¡œ ì ˆë‹¨")
    return text[:limit]

def drop_unwanted_sections(text):
    if not text or not text.strip():
        return text
    lines = text.split('\n')
    result = []
    skip = False
    for line in lines:
        stripped = line.strip()
        if not stripped:
            continue
        if _looks_like_heading(stripped):
            normalized = _normalize_heading_text(stripped)
            if any(keyword in normalized for keyword in DISALLOWED_HEADING_KEYWORDS):
                skip = True
                continue
            if skip and not any(keyword in normalized for keyword in ALLOW_HEADINGS_AFTER_SKIP):
                continue
            if skip and any(keyword in normalized for keyword in ALLOW_HEADINGS_AFTER_SKIP):
                skip = False
        if any(keyword in stripped for keyword in UNWANTED_SECTION_KEYWORDS):
            skip = True
            continue
        if skip:
            continue
        result.append(line)
    cleaned = '\n'.join(result).strip()
    cleaned = remove_disclaimer_paragraphs(cleaned)
    return cleaned or text

def debug_dump_markdown(ticker, section_name, content, prefix="", force=False):
    """ì§€ì •ëœ í‹°ì»¤ì— ëŒ€í•´ ë§ˆí¬ë‹¤ìš´ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì•ë¶€ë¶„ì„ ì¶œë ¥"""
    if not force and ticker not in DEBUG_DUMP_TICKERS:
        return
    safe_section = section_name.replace(' ', '_').replace('/', '_')
    filename = f"debug_{ticker}_{prefix}{safe_section}.md"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(content)
    preview = content[:500].replace('\n', ' ')  # í•œ ì¤„ë¡œ ì¶œë ¥
    print(f"  -> [Debug] {ticker} {section_name} ë§ˆí¬ë‹¤ìš´ ì €ì¥: {filename}")
    print(f"     ë¯¸ë¦¬ë³´ê¸°: {preview}...")

# ==============================================================================
# 2.5. [Extraction] í•µì‹¬ í•˜ìœ„ ì„¹ì…˜ë§Œ ì¶”ì¶œ (í† í° ì ˆì•½)
# ==============================================================================
def extract_key_subsections(markdown_text, section_type='business', debug_context=None):
    """
    ì‚¬ì—…ë³´ê³ ì„œì˜ í•µì‹¬ í•˜ìœ„ ì„¹ì…˜ë§Œ ì¶”ì¶œí•˜ì—¬ í† í° ì ˆì•½
    
    Args:
        markdown_text: ì „ì²´ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
        section_type: 'business' (ì‚¬ì—…ì˜ ë‚´ìš©) ë˜ëŠ” 'mda' (ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨)
    
    Returns:
        ì¶”ì¶œëœ í•µì‹¬ ì„¹ì…˜ë§Œ í¬í•¨í•œ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
    """
    def parse_heading(line):
        stripped = line.strip()
        if not stripped:
            return None
        
        heading_patterns = [
            (r'^(#{1,6})\s*(.+)$', lambda m: len(m.group(1))),   # Markdown í—¤ë”©
            (r'^([IVXLCDM]+)\.\s*(.+)$', lambda m: 2),           # ë¡œë§ˆìˆ«ì (ëŒ€ë¬¸ì)
            (r'^([ivxlcdm]+)\.\s*(.+)$', lambda m: 2),           # ë¡œë§ˆìˆ«ì (ì†Œë¬¸ì)
            (r'^(\d+)\.\s*(.+)$', lambda m: 3),                  # ìˆ«ì.
            (r'^(\d+)\)\s*(.+)$', lambda m: 4),                  # ìˆ«ì) â†’ í•˜ìœ„ ë ˆë²¨ë¡œ ê°„ì£¼
            (r'^([A-Z])\.\s*(.+)$', lambda m: 4),                # ì•ŒíŒŒë²³.
            (r'^([ê°€-í£])\.\s*(.+)$', lambda m: 5),              # í•œê¸€.
        ]
        
        for pattern, level_fn in heading_patterns:
            match = re.match(pattern, stripped)
            if match:
                return level_fn(match), match.group(2)

        complex_match = re.match(r'^((?:\d+|[IVXLCDM]+|[ivxlcdm]+|[ê°€-í£]|[A-Z])[\-\.]\s*(?:\d+|[IVXLCDM]+|[ivxlcdm]+|[ê°€-í£]|[A-Z]))\s*(.+)$', stripped)
        if complex_match:
            return 4, complex_match.group(2)
        return None
    
    context_ticker = debug_context.get('ticker') if debug_context else None
    context_section_label = debug_context.get('label') if debug_context else section_type
    
    if section_type == 'mda':
        # ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨: '5. íšŒê³„ê°ì‚¬ì¸ì˜ ê°ì‚¬ì˜ê²¬ ë“±' ì´ì „ê¹Œì§€ë§Œ ì¶”ì¶œ + íŠ¹ì • ì„¹ì…˜/í‘œ ì œì™¸
        lines = markdown_text.split('\n')
        result_lines = []
        stop_patterns = [
            r'íšŒê³„ê°ì‚¬ì¸ì˜\s*ê°ì‚¬ì˜ê²¬'
        ]
        skip_patterns = [
            r'ì˜ˆì¸¡ì •ë³´ì—\s*ëŒ€í•œ\s*ì£¼ì˜ì‚¬í•­',
            r'ì¤‘ìš”í•œ\s*íšŒê³„.*ì¶”ì •',
            r'íšŒê³„ì •ì±….*ì¶”ì •',
            r'í™˜ê²½.*ì¢…ì—…ì›'
        ]
        in_skip_section = False
        
        for line in lines:
            stripped = line.strip()
            
            # í‘œ(ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”)ëŠ” ëª¨ë‘ ì œì™¸
            if stripped.startswith('|'):
                continue
            
            heading_info = parse_heading(line)
            if heading_info:
                _, heading_text = heading_info
                clean_heading = re.sub(r'\([^)]*\)', '', heading_text).strip()
                clean_heading = re.sub(r'^\d+[\.\)]?\s*', '', clean_heading)
                clean_heading = re.sub(r'^[IVXLCDM]+\.\s*', '', clean_heading, flags=re.IGNORECASE)
                clean_heading = re.sub(r'\s+', '', clean_heading)
                # ì¤‘ì§€ íŒ¨í„´ í™•ì¸
                if any(re.search(pattern, clean_heading, re.IGNORECASE) for pattern in stop_patterns):
                    break
                
                # ìŠ¤í‚µ ëŒ€ìƒ ì„¹ì…˜ (ì˜ˆì¸¡ì •ë³´ ì£¼ì˜ì‚¬í•­ ë“±)
                if any(re.search(pattern, clean_heading, re.IGNORECASE) for pattern in skip_patterns):
                    in_skip_section = True
                    continue
                else:
                    in_skip_section = False
            
            if in_skip_section:
                continue
            
            result_lines.append(line)
        
        extracted_text = '\n'.join(result_lines)
        extracted_text = drop_unwanted_sections(extracted_text)
        if len(extracted_text.strip()) < 100:
            msg = "ê²½ì˜ì§„ë‹¨ ì„¹ì…˜ ì¶”ì¶œ ì‹¤íŒ¨, ì›ë¬¸ ì‚¬ìš©"
            if context_ticker:
                msg = f"{msg} ({context_ticker})"
            print(f"  -> [Warning] {msg}")
            if context_ticker:
                debug_dump_markdown(context_ticker, context_section_label, markdown_text, prefix='failure_', force=True)
            return markdown_text
        
        print(f"  -> [Extract] ê²½ì˜ì§„ë‹¨ í•µì‹¬ ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ (ì›ë³¸: {len(markdown_text)}ì â†’ ì¶”ì¶œ: {len(extracted_text)}ì)")
        # ë””ë²„ê¹…: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ ë§ˆì§€ë§‰ 300ì ì¶œë ¥ (ì¤‘ì§€ ì§€ì  í™•ì¸)
        if len(extracted_text) > 300:
            print(f"  -> [Debug] ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë§ˆì§€ë§‰ ë¶€ë¶„:\n...{extracted_text[-300:]}")
        return extracted_text
    
    elif section_type == 'business':
        # ì‚¬ì—…ì˜ ë‚´ìš©: íŠ¹ì • í•˜ìœ„ ì„¹ì…˜ë§Œ ì¶”ì¶œ
        # í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ë” ìœ ì—°í•˜ê²Œ ë§¤ì¹­ (ì •í™•í•œ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ë¡œ íŒë‹¨)
        target_keyword_sets = [
            ['ì£¼ìš”', 'ì œí’ˆ'], ['ì£¼ìš”', 'ì„œë¹„ìŠ¤'], ['ì˜ì—…', 'í˜„í™©'],
            ['ì›ì¬ë£Œ', 'ìƒì‚°'], ['ìƒì‚°', 'ì„¤ë¹„'], ['ìƒì‚°', 'ì‹¤ì '], ['ê°€ë™ë¥ '],
            ['ë§¤ì¶œ', 'ìˆ˜ì£¼'], ['íŒë§¤', 'ê²½ë¡œ'], ['íŒë§¤', 'ì „ëµ'],
            ['íŒë§¤', 'ëŒ€ìˆ˜'], ['ë„ë§¤', 'íŒë§¤'], ['ì†Œë§¤', 'íŒë§¤'],
            ['ì„ ê°€'], ['ìˆ˜ì£¼', 'ì”ê³ '], ['ìˆ˜ì£¼', 'í˜„í™©'], ['ë°œì£¼', 'í˜„í™©'], ['ì¸ë„', 'ì¼ì •'],
            ['ì—°êµ¬', 'ê°œë°œ'], ['R&D'], ['ì‹œì¥', 'ìœ„í—˜'], ['ìœ„í—˜', 'ê´€ë¦¬'],
            ['íŒŒìƒ', 'ìƒí’ˆ'], ['ì¬ë¬´', 'ìƒíƒœ'], ['ìê¸ˆ', 'ì¡°ë‹¬'],
            ['ì‹ ê·œ', 'ì‚¬ì—…'], ['í”Œë«í¼'], ['ê°€ì…ì'], ['ì´ìš©ì'],
            ['ì„ìƒ'], ['íŒŒì´í”„ë¼ì¸'], ['í’ˆëª©', 'í—ˆê°€'], ['ìˆ˜ìµ'], ['ë³´í—˜ë£Œ']
        ]
        
        lines = markdown_text.split('\n')
        result_lines = []
        in_target_section = False
        current_section_level = 0
        found_sections = []
        
        # ë””ë²„ê¹…: ì²˜ìŒ 50ì¤„ì—ì„œ í—¤ë”© í™•ì¸
        debug_headings = []
        for i, line in enumerate(lines[:50]):
            if line.strip().startswith('#'):
                debug_headings.append(f"ì¤„{i+1}: {line.strip()[:80]}")
        if debug_headings:
            print(f"  -> [Debug] ì²˜ìŒ 50ì¤„ ë‚´ í—¤ë”© ì˜ˆì‹œ: {debug_headings[:5]}")
        
        for i, line in enumerate(lines):
            # í—¤ë”© ë ˆë²¨ í™•ì¸ (# ê°œìˆ˜)
            heading_info = parse_heading(line)
            
            if heading_info:
                heading_level, heading_text = heading_info
                
                # ê´„í˜¸ ë° ë²ˆí˜¸ ì œê±°
                clean_heading = re.sub(r'\([^)]*\)', '', heading_text).strip()
                clean_heading = re.sub(r'^\d+[\.\)]?\s*', '', clean_heading)
                clean_heading = re.sub(r'^[IVXLCDM]+\.\s*', '', clean_heading, flags=re.IGNORECASE)
                clean_heading_no_space = re.sub(r'\s+', '', clean_heading)
                
                def match_keywords(keyword_set):
                    return all(keyword in clean_heading_no_space for keyword in keyword_set)
                
                is_target = any(match_keywords(keyword_set) for keyword_set in target_keyword_sets)
                
                if is_target:
                    in_target_section = True
                    current_section_level = heading_level
                    result_lines.append(line)
                    found_sections.append(heading_text)
                    print(f"  -> [Extract] âœ… í•˜ìœ„ ì„¹ì…˜ ë°œê²¬: {heading_text}")
                else:
                    # ë‹¤ë¥¸ ì„¹ì…˜ ì‹œì‘ í™•ì¸
                    if heading_level <= 2:  # ## ë ˆë²¨ ì´í•˜ë¡œ ê°€ë©´ ë‹¤ë¥¸ ëŒ€ì„¹ì…˜
                        if in_target_section:
                            in_target_section = False
                            current_section_level = 0
                    elif in_target_section and heading_level <= current_section_level:
                        # ê°™ì€ ë ˆë²¨ ë˜ëŠ” ìƒìœ„ ë ˆë²¨ì˜ ë‹¤ë¥¸ ì„¹ì…˜ ì‹œì‘
                        in_target_section = False
                        current_section_level = 0
            elif in_target_section:
                # íƒ€ê²Ÿ ì„¹ì…˜ ë‚´ì˜ ë‚´ìš©ì€ ëª¨ë‘ í¬í•¨
                result_lines.append(line)
        
        extracted_text = '\n'.join(result_lines)
        extracted_text = drop_unwanted_sections(extracted_text)
        
        # ì¶”ì¶œëœ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì›ë¬¸ ë°˜í™˜ (ì•ˆì „ì¥ì¹˜)
        if len(extracted_text.strip()) < 100:
            msg = f"í•µì‹¬ ì„¹ì…˜ ì¶”ì¶œ ì‹¤íŒ¨ (ë°œê²¬ëœ ì„¹ì…˜: {found_sections}), ì›ë¬¸ ì‚¬ìš©"
            if context_ticker:
                msg = f"{msg} ({context_ticker})"
            print(f"  -> [Warning] {msg}")
            if context_ticker:
                debug_dump_markdown(context_ticker, context_section_label, markdown_text, prefix='failure_', force=True)
            return markdown_text
        
        # ì¶”ì¶œëœ ì„¹ì…˜ì˜ í—¤ë”© í™•ì¸
        extracted_headings = [line for line in extracted_text.split('\n') if line.strip().startswith('#')]
        print(f"  -> [Extract] í•µì‹¬ ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ (ì›ë³¸: {len(markdown_text)}ì â†’ ì¶”ì¶œ: {len(extracted_text)}ì)")
        print(f"  -> [Extract] ì¶”ì¶œëœ ì„¹ì…˜ ëª©ë¡: {extracted_headings[:5]}")
        return extracted_text
    else:
        return markdown_text

# ==============================================================================
# 3. [Targeting] DART ëª©ì°¨ë³„ í•€ì…‹ ì¶”ì¶œ (ë¡œì§ ê°•í™”)
# ==============================================================================
def fetch_dart_sections(ticker, company_name):
    print(f"--- [{company_name}] ({ticker}) ë³´ê³ ì„œ íƒìƒ‰ ì¤‘ ---")
    
    try:
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ 1ë…„ ì „ (2024ë…„ ê³ ì •)
        target_year = 2024
        # ì‚¬ì—…ë³´ê³ ì„œëŠ” ë³´í†µ ë‹¤ìŒ í•´ 3ì›”ì— ì œì¶œë˜ë¯€ë¡œ ì ‘ìˆ˜ì¼ì ë²”ìœ„ í™•ì¥
        start_date = f'{target_year}-01-01'  # 2024-01-01
        end_date = f'{target_year + 1}-06-30'  # 2025-06-30 (2024ë…„ ì‚¬ì—…ë³´ê³ ì„œëŠ” 2025ë…„ 3ì›” ì œì¶œ)
        
        # ì •ê¸°ê³µì‹œ(A) ê²€ìƒ‰
        reports = dart.list(ticker, start=start_date, end=end_date, kind='A', final=True)
        
        if reports is None or len(reports) == 0:
            print(f"  -> [Pass] {target_year}ë…„ ë³´ê³ ì„œ ì—†ìŒ")
            return None
        
        # ì‚¬ì—…ë³´ê³ ì„œë§Œ í•„í„°ë§ (pblntf_detail_ty='A001' ë˜ëŠ” report_nmì— 'ì‚¬ì—…ë³´ê³ ì„œ' í¬í•¨)
        # report_nm í•„ë“œì—ì„œ "ì‚¬ì—…ë³´ê³ ì„œ"ê°€ í¬í•¨ëœ ê²ƒë§Œ ì„ íƒ
        business_reports = reports[reports['report_nm'].str.contains('ì‚¬ì—…ë³´ê³ ì„œ', na=False)]
        
        if len(business_reports) == 0:
            print(f"  -> [Alert] {target_year}ë…„ ì‚¬ì—…ë³´ê³ ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            print(f"  -> [Debug] ì‚¬ìš© ê°€ëŠ¥í•œ ë³´ê³ ì„œ: {reports['report_nm'].tolist()[:5]}")
            return None
        
        # ìµœì‹  ì‚¬ì—…ë³´ê³ ì„œ ì„ íƒ (ì¼ë°˜ì ìœ¼ë¡œ ì²« ë²ˆì§¸ê°€ ìµœì‹ )
        target_report = business_reports.iloc[0]
        rcept_no = target_report['rcept_no']
        report_title = target_report['report_nm']
        print(f"  -> ëŒ€ìƒ ë³´ê³ ì„œ: {report_title} (No: {rcept_no})")
        
        sub_docs = dart.sub_docs(rcept_no)
        
        # ë””ë²„ê¹…: sub_docs êµ¬ì¡° í™•ì¸
        print(f"  -> [Debug] sub_docs êµ¬ì¡°: {type(sub_docs)}, ì»¬ëŸ¼: {list(sub_docs.columns)}")
        if len(sub_docs) > 0:
            print(f"  -> [Debug] ì´ {len(sub_docs)}ê°œ ì„¹ì…˜ ë°œê²¬")
            if 'title' in sub_docs.columns:
                # ì²˜ìŒ 10ê°œ ì„¹ì…˜ ì œëª© ì¶œë ¥
                titles = sub_docs['title'].head(10).tolist()
                print(f"  -> [Debug] ì„¹ì…˜ ì œëª© ì˜ˆì‹œ: {titles}")

        combined_text = ""
        found_count = 0
        
        # ì˜¬ë°”ë¥¸ ì ‘ê·¼: ê° í–‰(row)ì„ ìˆœíšŒí•˜ë©´ì„œ 'title' ì»¬ëŸ¼ì—ì„œ ì„¹ì…˜ ì°¾ê¸°
        if 'title' in sub_docs.columns and 'url' in sub_docs.columns:
            for idx, row in sub_docs.iterrows():
                title_str = str(row['title'])
                clean_title = title_str.replace(" ", "").replace(".", "").strip()
                
                # 1) ì‚¬ì—…ì˜ ë‚´ìš©
                if 'ì‚¬ì—…ì˜ë‚´ìš©' in clean_title or 'ì‚¬ì—…ë‚´ìš©' in clean_title:
                    if pd.notna(row['url']) and row['url']:
                        try:
                            # URLì—ì„œ ì‹¤ì œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                            doc_url = row['url']
                            response = requests.get(doc_url, timeout=30)
                            response.raise_for_status()
                            html = response.text
                            if html:
                                md_text = clean_html_to_markdown(html)
                                # í•µì‹¬ í•˜ìœ„ ì„¹ì…˜ë§Œ ì¶”ì¶œ (í† í° ì ˆì•½)
                                md_text = extract_key_subsections(
                                    md_text,
                                    section_type='business',
                                    debug_context={'ticker': ticker, 'label': f'{report_title}_ì‚¬ì—…ì˜ë‚´ìš©'}
                                )
                                debug_dump_markdown(ticker, 'ì‚¬ì—…ì˜ ë‚´ìš©', md_text, prefix='business_')
                                combined_text += f"# 1. ì‚¬ì—…ì˜ ë‚´ìš©\n{md_text}\n\n"
                                found_count += 1
                                print(f"  -> [V] 'ì‚¬ì—…ì˜ ë‚´ìš©' ì¶”ì¶œ ì„±ê³µ (ì„¹ì…˜: {title_str[:50]}, ê¸¸ì´: {len(md_text)}ì)")
                        except Exception as e:
                            print(f"  -> [Warning] ì„¹ì…˜ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({title_str[:30]}): {e}")
                            continue
                
                # 2) ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨ (MD&A)
                elif 'ì´ì‚¬ì˜ê²½ì˜ì§„ë‹¨' in clean_title or 'ê²½ì˜ì§„ë‹¨' in clean_title or 'ë¶„ì„ì˜ê²¬' in clean_title:
                    if pd.notna(row['url']) and row['url']:
                        try:
                            # URLì—ì„œ ì‹¤ì œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                            doc_url = row['url']
                            response = requests.get(doc_url, timeout=30)
                            response.raise_for_status()
                            html = response.text
                            if html:
                                md_text = clean_html_to_markdown(html)
                                # '5. íšŒê³„ê°ì‚¬ì¸ì˜ ê°ì‚¬ì˜ê²¬ ë“±' ì´ì „ê¹Œì§€ë§Œ ì¶”ì¶œ (í† í° ì ˆì•½)
                                md_text = extract_key_subsections(
                                    md_text,
                                    section_type='mda',
                                    debug_context={'ticker': ticker, 'label': f'{report_title}_ì´ì‚¬ì˜ê²½ì˜ì§„ë‹¨'}
                                )
                                debug_dump_markdown(ticker, 'ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨', md_text, prefix='mda_')
                                combined_text += f"# 2. ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨\n{md_text}\n\n"
                                found_count += 1
                                print(f"  -> [V] 'ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨' ì¶”ì¶œ ì„±ê³µ (ì„¹ì…˜: {title_str[:50]}, ê¸¸ì´: {len(md_text)}ì)")
                        except Exception as e:
                            print(f"  -> [Warning] ì„¹ì…˜ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({title_str[:30]}): {e}")
                            continue
        else:
            print(f"  -> [Error] sub_docsì— 'title' ë˜ëŠ” 'url' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
                
        if found_count == 0:
            print("  -> [Alert] íƒ€ê²Ÿ ëª©ì°¨ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            if 'title' in sub_docs.columns:
                all_titles = sub_docs['title'].tolist()
                print(f"  -> [Debug] ì „ì²´ ì„¹ì…˜ ëª©ë¡ ({len(all_titles)}ê°œ):")
                for i, t in enumerate(all_titles[:20], 1):  # ì²˜ìŒ 20ê°œë§Œ ì¶œë ¥
                    print(f"      {i}. {t}")
            return None
            
        return combined_text

    except Exception as e:
        print(f"  -> [Error] DART API ì˜¤ë¥˜: {e}")
        return None

# ==============================================================================
# 3.5. [Summarization] ê¸´ í…ìŠ¤íŠ¸ ìš”ì•½ (í† í° ì ˆì•½)
# ==============================================================================
def summarize_with_llm(markdown_text):
    """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜ (gpt-5-nano ì‚¬ìš©) - ë™ì  ê¸¸ì´ ì¡°ì •"""
    # 1. ìš”ì•½ í’ˆì§ˆ ê°œì„ : í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ëª©í‘œ ê¸¸ì´ ì¡°ì •
    text_length = len(markdown_text)
    if text_length > 50000:
        max_length = 15000  # ë§¤ìš° ê¸´ ê²½ìš°
    elif text_length > 30000:
        max_length = 12000  # ê¸´ ê²½ìš°
    else:
        max_length = 10000  # ë³´í†µ
    
    print(f"  -> ğŸ“ ê¸´ í…ìŠ¤íŠ¸ ìš”ì•½ ì¤‘... (ì›ë³¸: {text_length}ì â†’ ëª©í‘œ: {max_length}ì)")
    
    prompt = f"""
    ë‹¤ìŒ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ìš©ì„ {max_length}ì ì´ë‚´ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ì¤˜.
    
    [í¬í•¨í•  í•µì‹¬ ì •ë³´]
    - ì‚¬ì—… ë‚´ìš© ë° ì£¼ìš” ì œí’ˆ/ì„œë¹„ìŠ¤
    - ì£¼ìš” ê³ ê°ì‚¬/ë§¤ì¶œì²˜
    - í•µì‹¬ ì›ì¬ë£Œ
    - ì„¤ë¹„íˆ¬ì ê³„íš
    - ë¹„ìš© êµ¬ì¡°
    
    [ì£¼ì˜ì‚¬í•­]
    - ì œê³µëœ ë‚´ìš©ì—ì„œë§Œ ì¶”ì¶œí•  ê²ƒ (ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ)
    - êµ¬ì²´ì ì¸ ìˆ«ìì™€ ì‚¬ì‹¤ë§Œ í¬í•¨í•  ê²ƒ
    
    ì›ë¬¸:
    {markdown_text[:50000]}  # ìµœëŒ€ 50,000ìë§Œ ì „ë‹¬
    """
    
    try:
        response = summary_llm.invoke([HumanMessage(content=prompt)])
        summarized = response.content.strip()
        print(f"  -> âœ… ìš”ì•½ ì™„ë£Œ (ìš”ì•½ë³¸: {len(summarized)}ì)")
        return summarized
    except Exception as e:
        # 2. ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”: ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ì˜ ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ê°€ ì•ì— ìˆìŒ)
        print(f"  -> [Warning] ìš”ì•½ ì‹¤íŒ¨: {e}, ì›ë¬¸ ì•ë¶€ë¶„ ì‚¬ìš©")
        # ì›ë¬¸ì˜ ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ê°€ ì•ì— ìˆìŒ)
        return markdown_text[:max_length]

# ==============================================================================
# 4. [Extraction] LLM êµ¬ì¡°í™”
# ==============================================================================
def extract_data_with_llm(markdown_text):
    print("  -> AI ë¶„ì„ ë° JSON êµ¬ì¡°í™” ì¤‘...")
    
    # ë””ë²„ê¹…: LLMì— ì „ë‹¬ë  í…ìŠ¤íŠ¸ì˜ êµ¬ì¡° í™•ì¸
    lines = markdown_text.split('\n')
    headings = [line for line in lines if line.strip().startswith('#')]
    print(f"  -> [Debug] ì¶”ì¶œëœ ì„¹ì…˜ í—¤ë”©: {headings[:10]}")  # ì²˜ìŒ 10ê°œ í—¤ë”©ë§Œ ì¶œë ¥
    
    # 1ë‹¨ê³„: ê¸´ í…ìŠ¤íŠ¸ë¥¼ ë¨¼ì € ìš”ì•½ (í† í° ì ˆì•½)
    if len(markdown_text) > 20000:  # 20,000ì ì´ìƒì´ë©´ ìš”ì•½
        summarized_text = summarize_with_llm(markdown_text)
        # ìš”ì•½ í›„ì—ë„ í—¤ë”© í™•ì¸
        summary_headings = [line for line in summarized_text.split('\n') if line.strip().startswith('#')]
        print(f"  -> [Debug] ìš”ì•½ í›„ í—¤ë”©: {summary_headings[:10]}")
    else:
        summarized_text = markdown_text
    
    prompt = """
    ë„ˆëŠ” 10ë…„ ì°¨ í€ë“œë§¤ë‹ˆì €ì•¼. ì œê³µëœ ê¸°ì—…ì˜ [ì‚¬ì—…ë³´ê³ ì„œ] ë‚´ìš©ì„ ë¶„ì„í•´ì„œ 
    íˆ¬ì íŒë‹¨ì— í•„ìš”í•œ í•µì‹¬ ì •ë³´ë¥¼ ì•„ë˜ JSON í¬ë§·ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•´.
    
    [ì¤‘ìš” ì§€ì¹¨]
    - ì œê³µëœ í…ìŠ¤íŠ¸ì—ì„œë§Œ ì •ë³´ë¥¼ ì¶”ì¶œí•  ê²ƒ (ì¶”ì¸¡í•˜ì§€ ë§ê²ƒ)
    - ëª…í™•íˆ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ì •ë³´ëŠ” "ì •ë³´ì—†ìŒ"ìœ¼ë¡œ í‘œì‹œ
    - êµ¬ì²´ì ì¸ ìˆ«ì, ì´ë¦„, ì‚¬ì‹¤ë§Œ í¬í•¨í•  ê²ƒ
    
    [ì¶”ì¶œ í•­ëª©]
    1. business_summary: ë™ì‚¬ê°€ ì˜ìœ„í•˜ëŠ” ì‚¬ì—… ë‚´ìš©ì„ ì´ˆë“±í•™ìƒë„ ì´í•´í•˜ê²Œ 3ì¤„ ìš”ì•½. (ì œê³µëœ ë‚´ìš©ì—ì„œë§Œ ì¶”ì¶œ)
    2. major_products: ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤ ë¦¬ìŠ¤íŠ¸ (êµ¬ì²´ì  ë¸Œëœë“œë‚˜ ëª¨ë¸ëª… í¬í•¨). ë³´ê³ ì„œì— ëª…ì‹œëœ ê²ƒë§Œ.
    3. major_clients: ì£¼ìš” ë§¤ì¶œì²˜/ê³ ê°ì‚¬ ì‹¤ëª… (ì˜ˆ: Apple, í˜„ëŒ€ì°¨). ë³´ê³ ì„œì— ëª…ì‹œëœ ê²ƒë§Œ. ì—†ìœ¼ë©´ "ì •ë³´ì—†ìŒ".
    4. raw_materials: ì œí’ˆ ìƒì‚°ì— í•„ìš”í•œ í•µì‹¬ ì›ì¬ë£Œ. ë³´ê³ ì„œì— ëª…ì‹œëœ ê²ƒë§Œ.
    5. capax_investment: ì„¤ë¹„íˆ¬ì(CAPEX)ë‚˜ ì‹ ê·œ ì‹œì„¤ íˆ¬ì ê³„íš ì–¸ê¸‰ ìš”ì•½. ë³´ê³ ì„œì— ëª…ì‹œëœ ê²ƒë§Œ.
    6. cost_structure: ë¹„ìš© êµ¬ì¡°ì—ì„œ ê°€ì¥ í° ë¹„ì¤‘ì„ ì°¨ì§€í•˜ëŠ” ê²ƒ (ì˜ˆ: ì›ì¬ë£Œë¹„, ì¸ê±´ë¹„). ë³´ê³ ì„œì— ëª…ì‹œëœ ê²ƒë§Œ.
    7. keywords: ê¸°ì—…ì„ ì„¤ëª…í•˜ëŠ” í•µì‹¬ í•´ì‹œíƒœê·¸ 5ê°œ. (ë³´ê³ ì„œ ë‚´ìš© ê¸°ë°˜)

    [ë°˜í™˜ í˜•ì‹]
    ì˜¤ì§ JSON í˜•ì‹ë§Œ ë°˜í™˜í•  ê²ƒ. (Markdown code block ì—†ì´)
    JSON í˜•ì‹:
    {
        "business_summary": "...",
        "major_products": [...],
        "major_clients": "...",
        "raw_materials": [...],
        "capax_investment": "...",
        "cost_structure": "...",
        "keywords": [...]
    }
    """

    messages = [
        SystemMessage(content="You are a precise financial analyst. Extract only factual information from the provided text. Do not hallucinate or make assumptions. Output JSON only."),
        HumanMessage(content=prompt + "\n\n" + summarized_text)
    ]
    
    try:
        response = llm.invoke(messages)
        content = response.content.replace("```json", "").replace("```", "").strip()
        parsed_data = json.loads(content)
        
        # risk_factors í•„ë“œê°€ ìˆìœ¼ë©´ ì œê±° (í† í° ì ˆì•½)
        if 'risk_factors' in parsed_data:
            del parsed_data['risk_factors']
        
        # 3. ê²°ê³¼ ê²€ì¦ ë¡œì§ ì¶”ê°€
        required_fields = ['business_summary', 'major_products', 'major_clients', 
                          'raw_materials', 'capax_investment', 'cost_structure', 'keywords']
        missing_fields = [field for field in required_fields if field not in parsed_data]
        
        if missing_fields:
            print(f"  -> [Warning] í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {missing_fields}")
            # ëˆ„ë½ëœ í•„ë“œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            for field in missing_fields:
                if field in ['major_products', 'raw_materials', 'keywords']:
                    parsed_data[field] = []
                else:
                    parsed_data[field] = "ì •ë³´ì—†ìŒ"
        
        return parsed_data
    except json.JSONDecodeError as e:
        print(f"  -> [Error] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        if 'response' in locals():
            print(f"  -> [Debug] ì‘ë‹µ ë‚´ìš©: {response.content[:500]}")
        return None
    except Exception as e:
        print(f"  -> [Error] LLM ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        if 'response' in locals():
            print(f"  -> [Debug] ì‘ë‹µ ë‚´ìš©: {response.content[:500]}")
        return None

# ==============================================================================
# 5. ë©”ì¸ ì‹¤í–‰ ë£¨í”„ (ë³‘ë ¬ ì²˜ë¦¬)
# ==============================================================================
def process_company(ticker, name):
    """ë‹¨ì¼ ê¸°ì—… ì²˜ë¦¬ í•¨ìˆ˜"""
    try:
        # 1. DART íŒŒì‹±
        report_text = fetch_dart_sections(ticker, name)
        
        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ LLM í˜¸ì¶œ ìŠ¤í‚µ
        if report_text:
            filtered_text = select_relevant_chunks(report_text, ticker)
            effective_text = filtered_text if filtered_text else report_text
            effective_text = drop_unwanted_sections(effective_text)
            effective_text = final_disclaimer_sweep(effective_text, ticker)
            effective_text = truncate_text(effective_text, MAX_LLM_CHARS, label=name)
            text_hash = hashlib.sha256(effective_text.encode('utf-8')).hexdigest()
            if ENABLE_LLM and CACHE_ENABLED:
                cached_packet = load_cached_result(ticker, text_hash)
                if cached_packet and 'data' in cached_packet:
                    cached_data = cached_packet['data']
                    cached_data['ticker'] = ticker
                    cached_data['company_name'] = name
                    print(f"  -> [Cache] {name} ê²°ê³¼ ì¬ì‚¬ìš© (hash={text_hash[:8]})")
                    return cached_data
            if ENABLE_LLM:
                # 2. LLM êµ¬ì¡°í™”
                structured_data = extract_data_with_llm(effective_text)
                
                if structured_data:
                    structured_data['ticker'] = ticker
                    structured_data['company_name'] = name
                    if CACHE_ENABLED:
                        save_cached_result(ticker, text_hash, structured_data)
                    print(f"  -> âœ… {name} ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
                    return structured_data
            else:
                # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: LLM í˜¸ì¶œ ëŒ€ì‹  ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì •ë³´ë§Œ ì¶œë ¥
                original_len = len(report_text)
                filtered_len = len(effective_text)
                print(f"  -> [Test] LLM í˜¸ì¶œ ìƒëµ. ì›ë¬¸ {original_len}ì â†’ ì„ë² ë”© ì„ íƒ {filtered_len}ì")
                snippet = effective_text[:800]
                print(f"  -> [Test] ì„ íƒ í…ìŠ¤íŠ¸ ì•ë¶€ë¶„:\n{snippet}\n...")
                return None
        else:
            print(f"  -> â­ï¸ {name} ë°ì´í„° ì—†ìŒ, ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            return None
    except Exception as e:
        print(f"  -> [Error] {name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def chunk_companies(items, size):
    for idx in range(0, len(items), size):
        yield items[idx:idx + size], (idx // size) + 1

def save_results(data, suffix=None):
    if not data:
        return
    filename = "dart_analysis_result.json" if suffix is None else f"dart_analysis_result_{suffix}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    label = "ìµœì¢…" if suffix is None else suffix
    print(f"  -> [Save] {label} ê²°ê³¼ ì €ì¥ ({len(data)}ê°œ)")

def run_batch(batch_items, batch_index, total_batches):
    print(f"\n===== Batch {batch_index}/{total_batches} (ê¸°ì—… {len(batch_items)}ê°œ) =====")
    batch_results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_company = {
            executor.submit(process_company, ticker, name): (ticker, name)
            for ticker, name in batch_items
        }
        for future in as_completed(future_to_company):
            ticker, name = future_to_company[future]
            try:
                result = future.result()
                if result:
                    batch_results.append(result)
            except Exception as e:
                print(f"  -> [Error] {name} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            time.sleep(0.3)
    print(f"===== Batch {batch_index} ì™„ë£Œ: {len(batch_results)}ê°œ ì„±ê³µ =====")
    return batch_results

final_database = []

print("ğŸš€ [DART x LLM] ë°ì´í„° ëŒ êµ¬ì¶• ì‹œì‘ (Sample 10)")

company_items = list(TARGET_COMPANIES.items())
if not company_items:
    print("ì²˜ë¦¬í•  ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    total_batches = (len(company_items) + BATCH_SIZE - 1) // BATCH_SIZE
    for batch_items, batch_index in chunk_companies(company_items, BATCH_SIZE):
        batch_results = run_batch(batch_items, batch_index, total_batches)
        if batch_results:
            final_database.extend(batch_results)
            save_results(final_database, suffix=f"batch_{batch_index}")
        if batch_index < total_batches:
            print(f"  -> [Batch] ë‹¤ìŒ ë°°ì¹˜ê¹Œì§€ {BATCH_PAUSE_SECONDS}ì´ˆ ëŒ€ê¸°")
            time.sleep(BATCH_PAUSE_SECONDS)

    if final_database:
        save_results(final_database)
        print("\n" + "="*50)
        print(f"âœ… ì‘ì—… ì™„ë£Œ! ì´ {len(final_database)}ê°œ ê¸°ì—… ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
