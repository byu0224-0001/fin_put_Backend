# -*- coding: utf-8 -*-
"""
DBì—ì„œ ë§¤ì¶œ ë¹„ì¤‘ì´ ì—†ëŠ” ëª¨ë“  ê¸°ì—…ë“¤ì˜ ë§¤ì¶œ ë¹„ì¤‘ ì¬ìˆ˜ì§‘ ë° ì„±ê³µë¥  ëª¨ë‹ˆí„°ë§ (ê°œì„  ë²„ì „)

ê°œì„  ì‚¬í•­ (GPT/Gemini í”¼ë“œë°± ë°˜ì˜):
1. DRY RUN ê²°ê³¼ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ ê³„íš ë¦¬í¬íŠ¸ë¡œ ë³€ê²½
2. ì¬ìˆ˜ì§‘ "ì„±ê³µ" ì •ì˜ë¥¼ ë” ì—„ê²©í•˜ê²Œ
3. Top20 ì „ìš© ì‹¤íŒ¨ ì›ì¸ ë¶„í•´ ë¦¬í¬íŠ¸
4. ë‹¨ê³„ë³„ ì„±ê³µë¥  KPI ì¸¡ì •
"""
import sys
import os
import json
import time
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Tuple, Optional

# Windows í™˜ê²½ì—ì„œ UTF-8 ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')
    os.environ['PYTHONIOENCODING'] = 'utf-8'
else:
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

sys.path.insert(0, '.')

from app.db import SessionLocal
from app.models.company_detail import CompanyDetail
from app.models.stock import Stock
from app.services.dart_parser import DartParser
from app.services.llm_handler import LLMHandler
from app.services.embedding_filter import select_relevant_chunks
from dotenv import load_dotenv
import logging

load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

DART_API_KEY = os.getenv('DART_API_KEY')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
MAX_LLM_CHARS = 50000


def get_missing_revenue_companies(db, limit: Optional[int] = None):
    """ë§¤ì¶œ ë¹„ì¤‘ì´ ì—†ëŠ” ëª¨ë“  ê¸°ì—… ì¡°íšŒ"""
    query = db.query(CompanyDetail, Stock).join(
        Stock, CompanyDetail.ticker == Stock.ticker
    ).filter(
        # ë§¤ì¶œ ë¹„ì¤‘ì´ ì—†ëŠ” ê²½ìš°: None, {}, ë˜ëŠ” ë¹ˆ dict
        (
            (CompanyDetail.revenue_by_segment == None) |
            (CompanyDetail.revenue_by_segment == {}) |
            (CompanyDetail.revenue_by_segment == '{}')
        )
    ).order_by(Stock.market_cap.desc().nullslast())
    
    if limit:
        query = query.limit(limit)
    
    results = query.all()
    
    companies = []
    for detail, stock in results:
        # ğŸ†• CompanyDetail ì¡´ì¬, í‹°ì»¤ ìœ íš¨ì„± í™•ì¸
        if not detail or not stock:
            continue
        
        companies.append({
            'ticker': detail.ticker,
            'name': stock.stock_name if stock else detail.ticker,
            'market_cap': stock.market_cap if stock else 0,
            'has_company_detail': True,
            'ticker_valid': bool(detail.ticker and len(detail.ticker) >= 4)
        })
    
    return companies


def validate_revenue_data(revenue_data: Optional[Dict]) -> Tuple[bool, str]:
    """
    ğŸ†• ì¬ìˆ˜ì§‘ "ì„±ê³µ" ì •ì˜ë¥¼ ë” ì—„ê²©í•˜ê²Œ
    
    Returns:
        (is_valid, reason)
    """
    if not revenue_data:
        return False, "NO_REVENUE_DATA"
    
    if not isinstance(revenue_data, dict):
        return False, "INVALID_FORMAT"
    
    if len(revenue_data) == 0:
        return False, "EMPTY_DICT"
    
    # ì„¸ê·¸ë¨¼íŠ¸/ë¹„ì¤‘ì´ íŒŒì‹± ê°€ëŠ¥í•œì§€ í™•ì¸
    valid_segments = 0
    total_pct = 0.0
    
    for segment, pct in revenue_data.items():
        if not segment or not isinstance(segment, str):
            continue
        
        if isinstance(pct, (int, float)) and pct > 0:
            valid_segments += 1
            total_pct += pct
    
    if valid_segments == 0:
        return False, "NO_VALID_SEGMENTS"
    
    # ìµœì†Œ top1 ì„¸ê·¸ë¨¼íŠ¸ ë¹„ì¤‘ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if total_pct < 1.0:  # ìµœì†Œ 1% ì´ìƒ
        return False, "TOTAL_PCT_TOO_LOW"
    
    return True, "SUCCESS"


def fetch_and_update_revenue(
    db, 
    ticker: str, 
    stock_name: str, 
    dart_parser: DartParser, 
    llm_handler: LLMHandler, 
    year: int = 2024,
    track_steps: bool = True
) -> Tuple[bool, str, Optional[Dict], Dict]:
    """
    ë§¤ì¶œ ë°ì´í„° ì¬ìˆ˜ì§‘ (ë‹¨ê³„ë³„ ì„±ê³µë¥  ì¶”ì )
    
    Returns:
        (success, error_code, revenue_data, step_tracking)
        error_code: 'SUCCESS', 'NO_REPORT', 'NO_SECTION', 'LLM_FAIL', 'NO_REVENUE_DATA', 'VALIDATION_FAIL', 'NO_DETAIL', 'ERROR'
        step_tracking: {'dart_fetch': bool, 'section_extract': bool, 'llm_extract': bool, 'revenue_extract': bool, 'validation': bool}
    """
    step_tracking = {
        'dart_fetch': False,
        'section_extract': False,
        'llm_extract': False,
        'revenue_extract': False,
        'validation': False
    }
    
    try:
        # Step 1: DART APIë¡œ ì„¹ì…˜ ì¶”ì¶œ
        combined_text = dart_parser.extract_key_sections(ticker, year)
        step_tracking['dart_fetch'] = True
        
        if not combined_text:
            logger.warning(f"[{ticker}] {stock_name}: DART ì„¹ì…˜ ì¶”ì¶œ ì‹¤íŒ¨")
            return False, "NO_REPORT", None, step_tracking
        
        if len(combined_text) < 200:
            logger.warning(f"[{ticker}] {stock_name}: DART ì„¹ì…˜ ì¶”ì¶œ ì„±ê³µí–ˆì§€ë§Œ ë‚´ìš© ë¶€ì¡± ({len(combined_text)}ì)")
            return False, "NO_SECTION", None, step_tracking
        
        step_tracking['section_extract'] = True
        logger.info(f"[{ticker}] {stock_name}: DART ì„¹ì…˜ ì¶”ì¶œ ì„±ê³µ ({len(combined_text)}ì)")
        
        # Step 2: ì„ë² ë”© í•„í„°ë¡œ ê´€ë ¨ ì²­í¬ ì„ íƒ
        try:
            filtered_text = select_relevant_chunks(combined_text, ticker=ticker)
            effective_text = filtered_text if filtered_text and len(filtered_text) > 200 else combined_text
        except Exception as e:
            logger.warning(f"[{ticker}] ì„ë² ë”© í•„í„°ë§ ì‹¤íŒ¨, ì›ë¬¸ ì‚¬ìš©: {e}")
            effective_text = combined_text
        
        # ê¸¸ì´ ì œí•œ
        if len(effective_text) > MAX_LLM_CHARS:
            effective_text = effective_text[:MAX_LLM_CHARS]
        
        # Step 3: LLMìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
        structured_data = llm_handler.extract_structured_data(
            effective_text,
            ticker=ticker,
            company_name=stock_name
        )
        
        if not structured_data:
            logger.warning(f"[{ticker}] {stock_name}: LLM êµ¬ì¡°í™” ì‹¤íŒ¨")
            return False, "LLM_FAIL", None, step_tracking
        
        step_tracking['llm_extract'] = True
        
        # Step 4: revenue_by_segment ì¶”ì¶œ
        revenue_data = structured_data.get('revenue_by_segment', {})
        
        if not revenue_data or not isinstance(revenue_data, dict) or len(revenue_data) == 0:
            logger.warning(f"[{ticker}] {stock_name}: ë§¤ì¶œë¹„ì¤‘ ë°ì´í„° ì—†ìŒ")
            return False, "NO_REVENUE_DATA", None, step_tracking
        
        step_tracking['revenue_extract'] = True
        
        # Step 5: ğŸ†• ì—„ê²©í•œ ê²€ì¦
        is_valid, validation_reason = validate_revenue_data(revenue_data)
        
        if not is_valid:
            logger.warning(f"[{ticker}] {stock_name}: ë§¤ì¶œë¹„ì¤‘ ê²€ì¦ ì‹¤íŒ¨ - {validation_reason}")
            return False, f"VALIDATION_FAIL:{validation_reason}", None, step_tracking
        
        step_tracking['validation'] = True
        
        # Step 6: DB ì—…ë°ì´íŠ¸
        detail = db.query(CompanyDetail).filter(CompanyDetail.ticker == ticker).first()
        if detail:
            detail.revenue_by_segment = revenue_data
            detail.updated_at = datetime.utcnow()
            db.commit()
            logger.info(f"[{ticker}] {stock_name}: ë§¤ì¶œë¹„ì¤‘ ì—…ë°ì´íŠ¸ ì™„ë£Œ ({len(revenue_data)}ê°œ ì„¸ê·¸ë¨¼íŠ¸, ì´ {sum(revenue_data.values()):.1f}%)")
            return True, "SUCCESS", revenue_data, step_tracking
        else:
            logger.warning(f"[{ticker}] {stock_name}: CompanyDetail ì—†ìŒ")
            return False, "NO_DETAIL", None, step_tracking
            
    except Exception as e:
        logger.error(f"[{ticker}] {stock_name}: ì˜¤ë¥˜ - {e}")
        import traceback
        traceback.print_exc()
        return False, f"ERROR: {str(e)}", None, step_tracking


def analyze_failure_cause(error_code: str, step_tracking: Dict) -> str:
    """
    ğŸ†• ì‹¤íŒ¨ ì›ì¸ ë¶„í•´ (A/B/C)
    
    Returns:
        'A': DARTì—ì„œ í•´ë‹¹ ì„¹ì…˜/í‘œë¥¼ ëª» ì°¾ìŒ
        'B': í‘œ/í…ìŠ¤íŠ¸ëŠ” ìˆëŠ”ë° LLMì´ êµ¬ì¡°í™”ì— ì‹¤íŒ¨
        'C': ì •ê·œí™”/ë§¤í•‘ì—ì„œ ì‹¤íŒ¨ (ê°’ì€ ìˆëŠ”ë° ê²€ì¦ ì‹¤íŒ¨)
        'OTHER': ê¸°íƒ€
    """
    if error_code in ['NO_REPORT', 'NO_SECTION']:
        return 'A'  # DARTì—ì„œ í•´ë‹¹ ì„¹ì…˜/í‘œë¥¼ ëª» ì°¾ìŒ
    elif error_code == 'LLM_FAIL':
        return 'B'  # í‘œ/í…ìŠ¤íŠ¸ëŠ” ìˆëŠ”ë° LLMì´ êµ¬ì¡°í™”ì— ì‹¤íŒ¨
    elif error_code.startswith('VALIDATION_FAIL'):
        return 'C'  # ì •ê·œí™”/ë§¤í•‘ì—ì„œ ì‹¤íŒ¨
    elif error_code == 'NO_REVENUE_DATA':
        # step_trackingì„ ë³´ê³  íŒë‹¨
        if step_tracking.get('llm_extract'):
            return 'B'  # LLMì€ ì„±ê³µí–ˆì§€ë§Œ revenue_by_segmentê°€ ì—†ìŒ
        else:
            return 'A'  # LLM ìì²´ê°€ ì‹¤íŒ¨
    else:
        return 'OTHER'


def refetch_all_missing_revenue(
    dry_run: bool = True,
    limit: Optional[int] = None,
    batch_size: int = 50,
    year: int = 2024
) -> Dict:
    """
    ë§¤ì¶œ ë¹„ì¤‘ì´ ì—†ëŠ” ëª¨ë“  ê¸°ì—… ì¬ìˆ˜ì§‘ ë° ëª¨ë‹ˆí„°ë§ (ê°œì„  ë²„ì „)
    """
    db = SessionLocal()
    
    if not DART_API_KEY:
        print("\nâŒ DART_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", flush=True)
        return None
    
    if not OPENAI_API_KEY:
        print("\nâŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", flush=True)
        return None
    
    dart_parser = DartParser(DART_API_KEY)
    llm_handler = LLMHandler()
    
    try:
        print("=" * 80, flush=True)
        print("ë§¤ì¶œ ë¹„ì¤‘ ì—†ëŠ” ê¸°ì—… ì¬ìˆ˜ì§‘ ë° ì„±ê³µë¥  ëª¨ë‹ˆí„°ë§ (ê°œì„  ë²„ì „)", flush=True)
        print("=" * 80, flush=True)
        
        if dry_run:
            print("\nâš ï¸  DRY RUN ëª¨ë“œ (ì‹¤ì œ ìˆ˜ì •í•˜ì§€ ì•ŠìŒ)", flush=True)
        else:
            print("\nâœ… ì‹¤ì œ ìˆ˜ì • ëª¨ë“œ", flush=True)
        
        # ë§¤ì¶œ ë¹„ì¤‘ ì—†ëŠ” ê¸°ì—… ì¡°íšŒ
        print("\n[1ë‹¨ê³„] ë§¤ì¶œ ë¹„ì¤‘ ì—†ëŠ” ê¸°ì—… ì¡°íšŒ ì¤‘...", flush=True)
        missing_companies = get_missing_revenue_companies(db, limit=limit)
        
        print(f"\n[ëŒ€ìƒ ê¸°ì—…]", flush=True)
        print(f"  ë§¤ì¶œ ë¹„ì¤‘ ì—†ëŠ” ê¸°ì—…: {len(missing_companies)}ê°œ", flush=True)
        if limit:
            print(f"  ì²˜ë¦¬ ì œí•œ: {limit}ê°œ", flush=True)
        
        if not missing_companies:
            print("\nâœ… ë§¤ì¶œ ë¹„ì¤‘ì´ í•„ìš”í•œ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤.", flush=True)
            return None
        
        # ğŸ†• DRY RUN: ì‹¤í–‰ ê°€ëŠ¥í•œ ê³„íš ë¦¬í¬íŠ¸ ìƒì„±
        if dry_run:
            print("\n[DRY RUN ê³„íš ë¦¬í¬íŠ¸]", flush=True)
            
            # CompanyDetail ì¡´ì¬, í‹°ì»¤ ìœ íš¨ì„± í™•ì¸
            eligible_count = sum(1 for c in missing_companies if c.get('has_company_detail') and c.get('ticker_valid'))
            will_call_dart_count = eligible_count  # ëª¨ë“  eligible ê¸°ì—…ì— ëŒ€í•´ DART í˜¸ì¶œ ì˜ˆì •
            will_call_llm_count = eligible_count  # DART ì„±ê³µ ì‹œ LLM í˜¸ì¶œ ì˜ˆì •
            
            # ì˜ˆìƒ ë¹„ìš© ê³„ì‚° (ëŒ€ëµì )
            estimated_llm_calls = will_call_llm_count
            estimated_cost_usd = estimated_llm_calls * 0.01  # ëŒ€ëµ $0.01 per call
            
            print(f"  Eligible ê¸°ì—… (CompanyDetail ì¡´ì¬, í‹°ì»¤ ìœ íš¨): {eligible_count}ê°œ", flush=True)
            print(f"  ì˜ˆìƒ DART API í˜¸ì¶œ: {will_call_dart_count}íšŒ", flush=True)
            print(f"  ì˜ˆìƒ LLM API í˜¸ì¶œ: {will_call_llm_count}íšŒ", flush=True)
            print(f"  ì˜ˆìƒ ë¹„ìš©: ì•½ ${estimated_cost_usd:.2f}", flush=True)
            
            # ìš°ì„ ìˆœìœ„ í‘œì‹œ
            top20 = missing_companies[:20]
            print(f"\n[ìš°ì„  ì¬ìˆ˜ì§‘ ëŒ€ìƒ (ì‹œê°€ì´ì•¡ ìƒìœ„ 20ê°œ)]", flush=True)
            for i, company in enumerate(top20, 1):
                market_cap = company.get('market_cap', 0)
                if market_cap >= 1000000000000:
                    market_cap_str = f"{market_cap/1000000000000:.1f}ì¡°ì›"
                else:
                    market_cap_str = f"{market_cap/1000000000:.1f}ì–µì›"
                print(f"  {i}. {company['name']} ({company['ticker']}) - {market_cap_str}", flush=True)
            
            # DRY RUN ê²°ê³¼ ì €ì¥
            plan_report = {
                'generated_at': datetime.now().isoformat(),
                'dry_run': True,
                'year': year,
                'planning': {
                    'total_target': len(missing_companies),
                    'eligible_count': eligible_count,
                    'will_call_dart_count': will_call_dart_count,
                    'will_call_llm_count': will_call_llm_count,
                    'estimated_cost_usd': estimated_cost_usd,
                    'top20_priorities': top20
                }
            }
            
            os.makedirs('reports', exist_ok=True)
            output_file = 'reports/refetch_revenue_plan.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(plan_report, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"\nâœ… ê³„íš ë¦¬í¬íŠ¸ ì €ì¥: {output_file}", flush=True)
            print(f"\nâš ï¸  ì‹¤ì œ ì¬ìˆ˜ì§‘ì„ ì›í•˜ì‹œë©´ --apply í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:", flush=True)
            print(f"  python scripts/refetch_all_missing_revenue.py --apply --limit 20", flush=True)
            print("=" * 80, flush=True)
            
            return plan_report
        
        # ì‹¤ì œ ì¬ìˆ˜ì§‘ ì§„í–‰
        print("\n[2ë‹¨ê³„] ë§¤ì¶œ ë¹„ì¤‘ ì¬ìˆ˜ì§‘ ì§„í–‰ ì¤‘...", flush=True)
        
        results = []
        success_count = 0
        fail_count = 0
        error_stats = defaultdict(int)
        failure_cause_stats = defaultdict(int)  # A/B/C ë¶„í¬
        step_success_stats = defaultdict(int)  # ë‹¨ê³„ë³„ ì„±ê³µë¥ 
        
        # Top20 ì „ìš© í†µê³„
        top20_results = []
        top20_success_count = 0
        top20_fail_count = 0
        top20_failure_cause_stats = defaultdict(int)
        
        total = len(missing_companies)
        if limit:
            total = min(total, limit)
        
        for idx, company in enumerate(missing_companies[:limit] if limit else missing_companies, 1):
            ticker = company['ticker']
            name = company['name']
            market_cap = company.get('market_cap', 0)
            is_top20 = idx <= 20
            
            print(f"\n[{idx}/{total}] {name} ({ticker}) ì¬ìˆ˜ì§‘ ì¤‘...", flush=True)
            
            # ì‹¤ì œ ì¬ìˆ˜ì§‘
            success, error_code, revenue_data, step_tracking = fetch_and_update_revenue(
                db, ticker, name, dart_parser, llm_handler, year, track_steps=True
            )
            
            # ë‹¨ê³„ë³„ ì„±ê³µë¥  ì¶”ì 
            for step, step_success in step_tracking.items():
                if step_success:
                    step_success_stats[step] += 1
            
            if success:
                success_count += 1
                segment_count = len(revenue_data) if revenue_data else 0
                total_pct = sum(revenue_data.values()) if revenue_data else 0
                
                results.append({
                    'ticker': ticker,
                    'name': name,
                    'market_cap': market_cap,
                    'status': 'SUCCESS',
                    'error_code': error_code,
                    'revenue_data': revenue_data,
                    'segment_count': segment_count,
                    'total_pct': total_pct,
                    'step_tracking': step_tracking
                })
                print(f"  âœ… ì„±ê³µ: {segment_count}ê°œ ì„¸ê·¸ë¨¼íŠ¸, ì´ {total_pct:.1f}%", flush=True)
                
                if is_top20:
                    top20_success_count += 1
                    top20_results.append(results[-1])
            else:
                fail_count += 1
                error_stats[error_code] += 1
                
                # ì‹¤íŒ¨ ì›ì¸ ë¶„í•´
                failure_cause = analyze_failure_cause(error_code, step_tracking)
                failure_cause_stats[failure_cause] += 1
                
                results.append({
                    'ticker': ticker,
                    'name': name,
                    'market_cap': market_cap,
                    'status': 'FAIL',
                    'error_code': error_code,
                    'revenue_data': None,
                    'step_tracking': step_tracking,
                    'failure_cause': failure_cause
                })
                print(f"  âŒ ì‹¤íŒ¨: {error_code} (ì›ì¸: {failure_cause})", flush=True)
                
                if is_top20:
                    top20_fail_count += 1
                    top20_failure_cause_stats[failure_cause] += 1
                    top20_results.append(results[-1])
            
            # Rate limit ë°©ì§€
            if idx < total:
                time.sleep(1)
            
            # ì§„í–‰ë¥  ì¶œë ¥
            if idx % 10 == 0 or idx == total:
                progress = (idx / total * 100) if total > 0 else 0
                print(f"\n  ì§„í–‰: {idx}/{total} ({progress:.1f}%)", flush=True)
        
        # í†µê³„ ê³„ì‚°
        total_processed = len(results)
        success_rate = (success_count / total_processed * 100) if total_processed > 0 else 0
        
        # ë‹¨ê³„ë³„ ì„±ê³µë¥  ê³„ì‚°
        step_success_rates = {
            step: (count / total_processed * 100) if total_processed > 0 else 0
            for step, count in step_success_stats.items()
        }
        
        # Top20 ì„±ê³µë¥ 
        top20_success_rate = (top20_success_count / 20 * 100) if top20_success_count + top20_fail_count > 0 else 0
        
        # ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            'generated_at': datetime.now().isoformat(),
            'dry_run': False,
            'year': year,
            'statistics': {
                'total_target': len(missing_companies),
                'total_processed': total_processed,
                'success_count': success_count,
                'fail_count': fail_count,
                'success_rate': success_rate,
                'error_distribution': dict(error_stats),
                'failure_cause_distribution': dict(failure_cause_stats),  # ğŸ†• A/B/C ë¶„í¬
                'step_success_rates': step_success_rates  # ğŸ†• ë‹¨ê³„ë³„ ì„±ê³µë¥ 
            },
            'top20_analysis': {  # ğŸ†• Top20 ì „ìš© ë¶„ì„
                'success_count': top20_success_count,
                'fail_count': top20_fail_count,
                'success_rate': top20_success_rate,
                'failure_cause_distribution': dict(top20_failure_cause_stats),
                'results': top20_results
            },
            'results': results
        }
        
        # ë¦¬í¬íŠ¸ ì¶œë ¥
        print("\n" + "=" * 80, flush=True)
        print("ì¬ìˆ˜ì§‘ ê²°ê³¼", flush=True)
        print("=" * 80, flush=True)
        
        print(f"\n[ì²˜ë¦¬ í†µê³„]", flush=True)
        print(f"  ëŒ€ìƒ ê¸°ì—…: {len(missing_companies)}ê°œ", flush=True)
        print(f"  ì²˜ë¦¬ ì™„ë£Œ: {total_processed}ê°œ", flush=True)
        print(f"  ì„±ê³µ: {success_count}ê°œ", flush=True)
        print(f"  ì‹¤íŒ¨: {fail_count}ê°œ", flush=True)
        print(f"  ì„±ê³µë¥ : {success_rate:.1f}%", flush=True)
        
        print(f"\n[ë‹¨ê³„ë³„ ì„±ê³µë¥ ]", flush=True)
        for step, rate in step_success_rates.items():
            print(f"  {step}: {rate:.1f}%", flush=True)
        
        if error_stats:
            print(f"\n[ì‹¤íŒ¨ ì›ì¸ ë¶„ì„]", flush=True)
            for error_code, count in sorted(error_stats.items(), key=lambda x: x[1], reverse=True):
                print(f"  {error_code}: {count}ê°œ", flush=True)
        
        print(f"\n[ì‹¤íŒ¨ ì›ì¸ ë¶„í•´ (A/B/C)]", flush=True)
        print(f"  A (DARTì—ì„œ ì„¹ì…˜/í‘œ ëª» ì°¾ìŒ): {failure_cause_stats.get('A', 0)}ê°œ", flush=True)
        print(f"  B (LLM êµ¬ì¡°í™” ì‹¤íŒ¨): {failure_cause_stats.get('B', 0)}ê°œ", flush=True)
        print(f"  C (ê²€ì¦ ì‹¤íŒ¨): {failure_cause_stats.get('C', 0)}ê°œ", flush=True)
        print(f"  ê¸°íƒ€: {failure_cause_stats.get('OTHER', 0)}ê°œ", flush=True)
        
        print(f"\n[Top20 ë¶„ì„]", flush=True)
        print(f"  ì„±ê³µ: {top20_success_count}ê°œ", flush=True)
        print(f"  ì‹¤íŒ¨: {top20_fail_count}ê°œ", flush=True)
        print(f"  ì„±ê³µë¥ : {top20_success_rate:.1f}%", flush=True)
        print(f"  ì‹¤íŒ¨ ì›ì¸ ë¶„í•´:", flush=True)
        for cause, count in top20_failure_cause_stats.items():
            print(f"    {cause}: {count}ê°œ", flush=True)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        os.makedirs('reports', exist_ok=True)
        output_file = 'reports/refetch_revenue_monitoring.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_file}", flush=True)
        print("=" * 80, flush=True)
        
        return report
        
    except Exception as e:
        import traceback
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}", flush=True)
        traceback.print_exc()
        raise
    finally:
        db.close()


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ë§¤ì¶œ ë¹„ì¤‘ ì—†ëŠ” ê¸°ì—… ì¬ìˆ˜ì§‘ ë° ëª¨ë‹ˆí„°ë§ (ê°œì„  ë²„ì „)')
    parser.add_argument('--apply', action='store_true', help='ì‹¤ì œ ìˆ˜ì • ëª¨ë“œ (ê¸°ë³¸ê°’: DRY RUN)')
    parser.add_argument('--limit', type=int, default=None, help='ì²˜ë¦¬í•  ê¸°ì—… ìˆ˜ ì œí•œ')
    parser.add_argument('--batch-size', type=int, default=50, help='ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°')
    parser.add_argument('--year', type=int, default=2024, help='ëŒ€ìƒ ì—°ë„')
    
    args = parser.parse_args()
    
    refetch_all_missing_revenue(
        dry_run=not args.apply,
        limit=args.limit,
        batch_size=args.batch_size,
        year=args.year
    )


if __name__ == '__main__':
    main()

