"""
ì „ì²´ ê¸°ì—… ì„¹í„° ì¬ë¶„ë¥˜ ìŠ¤í¬ë¦½íŠ¸ (Ensemble ë°©ì‹) - ì™„ì „ ìµœì í™” ë²„ì „

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. âœ… ë°°ì¹˜ ì„ë² ë”© ì²˜ë¦¬ (32-64ê°œì”©)
2. âœ… DB ë°°ì¹˜ ì»¤ë°‹ (50ê°œë§ˆë‹¤)
3. âœ… GPT ë¹„ë™ê¸° í˜¸ì¶œ (concurrent.futures ì‚¬ìš©)
4. âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™” (ì„¹í„° ë¶„ë¥˜ë§Œ)
5. âœ… ë¬´í•œ ë¡œë”© ë°©ì§€ (ê°•í™”ëœ exit code ë° ìƒíƒœ íŒŒì¼ ê´€ë¦¬)

âš ï¸ ì£¼ì˜: ë°¸ë¥˜ì²´ì¸ ë¶„ë¥˜ ë° Edge ì—°ê²°ì€ ìˆœì°¨ ì²˜ë¦¬ í•„ìˆ˜
"""
import sys
import os
if sys.platform == 'win32':
    import codecs
    try:
        if hasattr(sys.stdout, 'encoding') and sys.stdout.encoding != 'utf-8':
            sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    except (AttributeError, TypeError):
        # stdoutì´ BufferedWriter ë“±ì¸ ê²½ìš° encoding ì†ì„±ì´ ì—†ì„ ìˆ˜ ìˆìŒ
        try:
            sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        except:
            pass
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    os.environ['PYTHONUNBUFFERED'] = '1'

from pathlib import Path
project_root = Path(__file__).parent.parent
os.chdir(project_root)
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
env_path = project_root / '.env'
if env_path.exists():
    load_dotenv(dotenv_path=env_path, override=True)
    print(f"âœ… .env íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {env_path}")
else:
    print(f"âš ï¸  .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {env_path}")

if 'OPEN_API_KEY' in os.environ and 'OPENAI_API_KEY' not in os.environ:
    os.environ['OPENAI_API_KEY'] = os.environ['OPEN_API_KEY']
    print(f"âš ï¸  í™˜ê²½ ë³€ìˆ˜ 'OPEN_API_KEY'ë¥¼ 'OPENAI_API_KEY'ë¡œ ë³µì‚¬í–ˆìŠµë‹ˆë‹¤.")
elif 'OPEN_API_KEY' in os.environ and 'OPENAI_API_KEY' in os.environ:
    print(f"âš ï¸  'OPEN_API_KEY'ì™€ 'OPENAI_API_KEY' ë‘˜ ë‹¤ ì¡´ì¬í•©ë‹ˆë‹¤. 'OPENAI_API_KEY'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

from app.db import SessionLocal
from app.models.company_detail import CompanyDetail
from app.models.investor_sector import InvestorSector
from app.models.stock import Stock
from app.services.sector_classifier_ensemble_won import classify_sector_ensemble_won
from app.services.gemini_handler import GeminiHandler
from app.config import settings
import logging
import argparse
import json
from datetime import datetime
from sqlalchemy.exc import IntegrityError
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Tuple, Optional
from collections import deque
import threading
import atexit
import signal

# ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
BATCH_EMBEDDING_SIZE = 32  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • (32-64)
BATCH_COMMIT_SIZE = 50  # DB ì»¤ë°‹ ë°°ì¹˜ í¬ê¸°
MAX_WORKERS = 4  # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (CPU ì½”ì–´ ìˆ˜ì— ë”°ë¼ ì¡°ì •)

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
log_dir = project_root / "logs"
log_dir.mkdir(exist_ok=True)

# ìƒíƒœ íŒŒì¼ ë””ë ‰í† ë¦¬ ìƒì„±
status_dir = project_root / "status"
status_dir.mkdir(exist_ok=True)

# ì „ì—­ ë³€ìˆ˜ (ì¢…ë£Œ ì‹ í˜¸ìš©)
_shutdown_flag = threading.Event()
_status_updated = False

def write_status(step_name, status, details=None):
    """ìƒíƒœ íŒŒì¼ ì‘ì„± (Auto Workflow ì—°ë™ìš©) - ê°•í™” ë²„ì „"""
    global _status_updated
    try:
        status_file = status_dir / f"{step_name}_status.json"
        data = {
            'step': step_name,
            'status': status,
            'timestamp': datetime.now().isoformat(),
            'details': details or {}
        }
        
        max_retries = 5
        for attempt in range(max_retries):
            try:
                # ì„ì‹œ íŒŒì¼ì— ë¨¼ì € ì‘ì„± í›„ ì›ìì  ì´ë™
                temp_file = status_file.with_suffix('.json.tmp')
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    f.flush()
                    os.fsync(f.fileno())
                
                # ì›ìì  ì´ë™
                if temp_file.exists():
                    temp_file.replace(status_file)
                    break
            except (IOError, OSError) as e:
                if attempt < max_retries - 1:
                    import time
                    time.sleep(0.2)
                else:
                    logger.error(f"ìƒíƒœ íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨ (ì‹œë„ {max_retries}íšŒ): {e}")
                    raise
        
        # ì™„ë£Œ í”Œë˜ê·¸ íŒŒì¼ ìƒì„±
        if status in ['completed', 'failed']:
            _status_updated = True
            flag_file = status_dir / f"{step_name}_{status}.flag"
            try:
                with open(flag_file, 'w', encoding='utf-8') as f:
                    f.write(datetime.now().isoformat())
                    f.flush()
                    os.fsync(f.fileno())
            except Exception as e:
                logger.warning(f"í”Œë˜ê·¸ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        
        logger.info(f"âœ… ìƒíƒœ íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {status_file} (status: {status})")
    except Exception as e:
        logger.error(f"âŒ ìƒíƒœ íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)

# ë¡œê·¸ íŒŒì¼ ì„¤ì • (ì „ì—­ logger ì„¤ì • ì „ì—)
log_file = log_dir / f"sector_reclassification_optimized_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# ìŠ¤ë ˆë“œ ì•ˆì „í•œ ì¹´ìš´í„°
class ThreadSafeCounter:
    def __init__(self):
        self._value = 0
        self._lock = threading.Lock()
    
    def increment(self, amount=1):
        with self._lock:
            self._value += amount
            return self._value
    
    @property
    def value(self):
        return self._value

def prepare_company_data_batch(
    db,
    tickers: List[str]
) -> List[Dict[str, Any]]:
    """ë°°ì¹˜ë¡œ ê¸°ì—… ë°ì´í„° ì¤€ë¹„ (DB ì¿¼ë¦¬ ìµœì í™”)"""
    companies = []
    
    # Stock ì •ë³´ ì¼ê´„ ì¡°íšŒ
    stocks = {s.ticker: s for s in db.query(Stock).filter(Stock.ticker.in_(tickers)).all()}
    
    # CompanyDetail ì¼ê´„ ì¡°íšŒ
    company_details = {
        cd.ticker: cd 
        for cd in db.query(CompanyDetail).filter(CompanyDetail.ticker.in_(tickers)).all()
    }
    
    for ticker in tickers:
        stock = stocks.get(ticker)
        company_detail = company_details.get(ticker)
        
        if not stock or not company_detail:
            continue
            
        companies.append({
            'ticker': ticker,
            'stock': stock,
            'company_detail': company_detail
        })
    
    return companies

def process_single_company_sector(
    ticker: str,
    stock: Stock,
    company_detail: CompanyDetail,
    gemini_handler: Optional[GeminiHandler],
    use_gpt: bool,
    idx: int,
    total: int,
    args,
    success_counter: ThreadSafeCounter,
    skip_counter: ThreadSafeCounter,
    fail_counter: ThreadSafeCounter,
    stats: Dict[str, int],
    stats_lock: threading.Lock
) -> Tuple[str, Optional[List[Dict[str, Any]]], Optional[Exception]]:
    """ë‹¨ì¼ ê¸°ì—… ì„¹í„° ë¶„ë¥˜ ì²˜ë¦¬ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
    if _shutdown_flag.is_set():
        return ticker, None, Exception("Shutdown requested")
    
    db = SessionLocal()
    try:
        # ê¸°ì¡´ ë¶„ë¥˜ í™•ì¸
        existing = db.query(InvestorSector).filter(
            InvestorSector.ticker == ticker
        ).first()
        
        if existing and args.skip_existing and not args.overwrite:
            logger.debug(f"[{idx}/{total}] [{ticker}] ì´ë¯¸ ë¶„ë¥˜ë¨ (ìŠ¤í‚µ)")
            skip_counter.increment()
            return ticker, None, None
        
        # ê¸°ì¡´ ë¶„ë¥˜ ì‚­ì œ
        if args.overwrite and existing:
            deleted = db.query(InvestorSector).filter(
                InvestorSector.ticker == ticker
            ).delete()
            if deleted > 0:
                logger.debug(f"[{idx}/{total}] [{ticker}] ê¸°ì¡´ ë¶„ë¥˜ {deleted}ê°œ ì‚­ì œ")
            db.commit()
            db.flush()
        
        # Ensemble ì„¹í„° ë¶„ë¥˜
        logger.info(f"[{idx}/{total}] [{ticker}] {stock.stock_name} ì„¹í„° ë¶„ë¥˜ ì¤‘...")
        
        results = classify_sector_ensemble_won(
            db=db,
            ticker=ticker,
            gemini_handler=gemini_handler if use_gpt else None,
            use_embedding=True,  # â­ GPU ì‚¬ìš© ë³µì›
            use_reranking=True,  # â­ GPU ì‚¬ìš© ë³µì›
            max_sectors=3,
            force_reclassify=args.overwrite
        )
        
        if not results:
            logger.warning(f"[{idx}/{total}] [{ticker}] ì„¹í„° ë¶„ë¥˜ ì‹¤íŒ¨")
            fail_counter.increment()
            return ticker, None, Exception("ë¶„ë¥˜ ê²°ê³¼ ì—†ìŒ")
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        with stats_lock:
            for result in results:
                method = result.get('method', 'ENSEMBLE')
                if method == 'RULE_BASED':
                    stats['rule_based'] += 1
                else:
                    stats['ensemble'] += 1
        
        return ticker, results, None
        
    except Exception as e:
        logger.error(f"[{idx}/{total}] [{ticker}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        db.rollback()
        fail_counter.increment()
        return ticker, None, e
    finally:
        db.close()

def save_results_batch(
    db: SessionLocal,
    results_batch: List[Tuple[str, Optional[List[Dict[str, Any]]]]],
    args,
    stats: Dict[str, int],
    stats_lock: threading.Lock
) -> Tuple[int, int]:
    """ë°°ì¹˜ë¡œ ê²°ê³¼ ì €ì¥ (DB ì»¤ë°‹ ìµœì í™”)"""
    success_count = 0
    fail_count = 0
    
    for ticker, results in results_batch:
        if not results:
            fail_count += 1
            continue
        
        try:
            for i, result in enumerate(results):
                # â­ NEW: ì €ì¥ ì „ ìµœì¢… ê²€ì¦
                if not result.get('sector_l1') and not result.get('major_sector'):
                    logger.warning(f"[{ticker}] ì €ì¥ ì „ NULL ì„¹í„° ê°ì§€, ê°•ì œ Fallback ì ìš©")
                    result['sector_l1'] = 'SEC_UNKNOWN'
                    result['major_sector'] = 'SEC_UNKNOWN'
                    result['fallback_used'] = 'TRUE'  # â­ VARCHARì— ë¬¸ìì—´ ì €ì¥
                    result['fallback_type'] = 'UNKNOWN'  # â­ íƒ€ì… ë¶„ë¦¬
                    result['confidence'] = 'VERY_LOW'
                    result['method'] = 'FALLBACK_UNKNOWN'
                    result['ensemble_score'] = 0.0
                    result['reasoning'] = 'NULL ì„¹í„° ê°ì§€, UNKNOWN í• ë‹¹'
                
                method = result.get('method', 'ENSEMBLE')
                sub_sector_str = result.get('sub_sector', '') or ''
                sector_id = f"{ticker}_{result['major_sector']}"
                if sub_sector_str:
                    sector_id += f"_{sub_sector_str}"
                if i > 0:
                    sector_id += f"_{i}"
                
                existing_sector = db.query(InvestorSector).filter(
                    InvestorSector.id == sector_id
                ).first()
                
                if existing_sector:
                    existing_sector.major_sector = result['major_sector']
                    existing_sector.sub_sector = result.get('sub_sector')
                    # âš ï¸ value_chain ê´€ë ¨ í•„ë“œëŠ” ì„¹í„° ì¬ë¶„ë¥˜ì—ì„œ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
                    # (ë°¸ë¥˜ì²´ì¸ ë¶„ë¥˜ëŠ” ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰)
                    # existing_sector.value_chain = result.get('value_chain')  # ì œê±°: ì„¹í„° ì¬ë¶„ë¥˜ëŠ” ë°¸ë¥˜ì²´ì¸ì„ ë®ì–´ì“°ì§€ ì•ŠìŒ
                    existing_sector.sector_weight = result.get('weight', 0.5)
                    existing_sector.is_primary = result.get('is_primary', (i == 0))
                    existing_sector.classification_method = method
                    existing_sector.confidence = result.get('confidence', 'MEDIUM')
                    existing_sector.fallback_used = result.get('fallback_used') or 'FALSE'  # â­ Fallback ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: 'FALSE')
                    existing_sector.fallback_type = result.get('fallback_type')  # â­ Fallback íƒ€ì…
                    existing_sector.rule_score = result.get('rule_score')
                    existing_sector.embedding_score = result.get('embedding_score')
                    existing_sector.bge_score = result.get('bge_score')
                    existing_sector.gpt_score = result.get('gpt_score')
                    existing_sector.ensemble_score = result.get('ensemble_score')
                    existing_sector.classification_reasoning = result.get('reasoning')
                    # â­ ë ˆë²¨ 2: ì¸ê³¼ êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ì €ì¥
                    existing_sector.causal_structure = result.get('causal_structure')
                    existing_sector.investment_insights = result.get('investment_insights')
                    # â­ Rule-based ë©”íƒ€ë°ì´í„° (í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ìš©)
                    existing_sector.rule_version = result.get('rule_version')
                    existing_sector.rule_confidence = result.get('rule_confidence')
                    existing_sector.training_label = result.get('training_label', False)
                    # â­ ìƒˆë¡œìš´ ê³„ì¸µ êµ¬ì¡° í•„ë“œ
                    existing_sector.sector_l1 = result.get('sector_l1') or result.get('major_sector')
                    existing_sector.sector_l2 = result.get('sector_l2') or result.get('sub_sector')
                    existing_sector.sector_l3_tags = result.get('sector_l3_tags') or result.get('causal_structure', {}).get('sector_l3_tags', [])
                    # â­ Boosting ë¡œê·¸ ì €ì¥
                    if result.get('boosting_log'):
                        existing_sector.boosting_log = result.get('boosting_log')
                    # âš ï¸ Phase 2: 5ë‹¨ê³„ ë°¸ë¥˜ì²´ì¸ í•„ë“œëŠ” ì„¹í„° ì¬ë¶„ë¥˜ì—ì„œ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
                    # (ë°¸ë¥˜ì²´ì¸ ë¶„ë¥˜ëŠ” classify_value_chain_final.pyë¡œ ë³„ë„ ì‹¤í–‰)
                else:
                    investor_sector = InvestorSector(
                        id=sector_id,
                        ticker=ticker,
                        major_sector=result['major_sector'],
                        sub_sector=result.get('sub_sector'),
                        # âš ï¸ value_chain ê´€ë ¨ í•„ë“œëŠ” ì„¹í„° ì¬ë¶„ë¥˜ì—ì„œ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
                        # (ë°¸ë¥˜ì²´ì¸ ë¶„ë¥˜ëŠ” ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰)
                        # value_chain=result.get('value_chain'),  # ì œê±°: ì„¹í„° ì¬ë¶„ë¥˜ëŠ” ë°¸ë¥˜ì²´ì¸ì„ ì„¤ì •í•˜ì§€ ì•ŠìŒ
                        sector_weight=result.get('weight', 0.5),
                        is_primary=result.get('is_primary', (i == 0)),
                        classification_method=method,
                        confidence=result.get('confidence', 'MEDIUM'),
                        fallback_used=result.get('fallback_used') or 'FALSE',  # â­ Fallback ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: 'FALSE')
                        fallback_type=result.get('fallback_type'),  # â­ Fallback íƒ€ì…
                        rule_score=result.get('rule_score'),
                        embedding_score=result.get('embedding_score'),
                        bge_score=result.get('bge_score'),
                        gpt_score=result.get('gpt_score'),
                        ensemble_score=result.get('ensemble_score'),
                        classification_reasoning=result.get('reasoning'),
                        # â­ ë ˆë²¨ 2: ì¸ê³¼ êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ì €ì¥
                        causal_structure=result.get('causal_structure'),
                        investment_insights=result.get('investment_insights'),
                        # â­ Rule-based ë©”íƒ€ë°ì´í„° (í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ìš©)
                        rule_version=result.get('rule_version'),
                        rule_confidence=result.get('rule_confidence'),
                        training_label=result.get('training_label', False),
                        # â­ ìƒˆë¡œìš´ ê³„ì¸µ êµ¬ì¡° í•„ë“œ
                        # âš ï¸ Phase 2: 5ë‹¨ê³„ ë°¸ë¥˜ì²´ì¸ í•„ë“œëŠ” ì„¹í„° ì¬ë¶„ë¥˜ì—ì„œ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
                        # (ë°¸ë¥˜ì²´ì¸ ë¶„ë¥˜ëŠ” classify_value_chain_final.pyë¡œ ë³„ë„ ì‹¤í–‰)
                        sector_l1=result.get('sector_l1') or result.get('major_sector'),
                        sector_l2=result.get('sector_l2') or result.get('sub_sector'),
                        sector_l3_tags=result.get('sector_l3_tags') or result.get('causal_structure', {}).get('sector_l3_tags', []),
                        # â­ Boosting ë¡œê·¸ ì €ì¥
                        boosting_log=result.get('boosting_log')
                    )
                    db.add(investor_sector)
            
            success_count += 1
        except IntegrityError as ie:
            logger.error(f"[{ticker}] IntegrityError ë°œìƒ: {ie}")
            db.rollback()
            fail_count += 1
        except Exception as e:
            logger.error(f"[{ticker}] ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}", exc_info=True)
            db.rollback()
            fail_count += 1
    
    return success_count, fail_count

def signal_handler(signum, frame):
    """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ (ë¬´í•œ ë¡œë”© ë°©ì§€)"""
    global _shutdown_flag
    logger.warning(f"ì‹œê·¸ë„ {signum} ìˆ˜ì‹ . ì¢…ë£Œ ì¤‘...")
    _shutdown_flag.set()
    sys.exit(1)

# ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
if sys.platform != 'win32':
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)

def cleanup_and_exit(exit_code, step_name):
    """ì •ë¦¬ ë° ì¢…ë£Œ (ë¬´í•œ ë¡œë”© ë°©ì§€) - ê°•í™” ë²„ì „"""
    global _status_updated
    
    max_cleanup_attempts = 3
    cleanup_success = False
    
    for attempt in range(max_cleanup_attempts):
        try:
            # ëª¨ë“  ì¶œë ¥ ë²„í¼ ê°•ì œ flush
            try:
                sys.stdout.flush()
                sys.stderr.flush()
            except:
                pass
            
            # ë¡œê±° í•¸ë“¤ëŸ¬ flush
            try:
                for handler in logger.handlers:
                    if hasattr(handler, 'flush'):
                        handler.flush()
            except:
                pass
            
            # ìƒíƒœ íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ê°•ì œ ì—…ë°ì´íŠ¸
            if not _status_updated:
                status_file = status_dir / f"{step_name}_status.json"
                if status_file.exists():
                    try:
                        with open(status_file, 'r', encoding='utf-8') as f:
                            current_status = json.load(f)
                            if current_status.get('status') == 'running':
                                final_status = 'completed' if exit_code == 0 else 'failed'
                                final_details = {
                                    'note': 'cleanup_and_exitì—ì„œ ê°•ì œ ì—…ë°ì´íŠ¸',
                                    'exit_code': exit_code
                                } if exit_code == 0 else {
                                    'error': f'exit code {exit_code}ë¡œ ì¢…ë£Œ'
                                }
                                write_status(step_name, final_status, final_details)
                                _status_updated = True
                    except Exception as e:
                        if attempt == max_cleanup_attempts - 1:
                            logger.error(f"ìƒíƒœ íŒŒì¼ í™•ì¸ ì‹¤íŒ¨: {e}")
                else:
                    # ìƒíƒœ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
                    final_status = 'completed' if exit_code == 0 else 'failed'
                    write_status(step_name, final_status, {'exit_code': exit_code})
                    _status_updated = True
            
            # ì™„ë£Œ í”Œë˜ê·¸ íŒŒì¼ ê°•ì œ ìƒì„±
            try:
                flag_status = 'completed' if exit_code == 0 else 'failed'
                flag_file = status_dir / f"{step_name}_{flag_status}.flag"
                with open(flag_file, 'w', encoding='utf-8') as f:
                    f.write(datetime.now().isoformat())
                    f.write(f"\nexit_code={exit_code}\n")
                    f.flush()
                    os.fsync(f.fileno())
            except:
                pass
            
            cleanup_success = True
            break
            
        except Exception as e:
            if attempt == max_cleanup_attempts - 1:
                logger.error(f"ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ìµœì¢… ì‹œë„): {e}", exc_info=True)
            else:
                import time
                time.sleep(0.5)
    
    # ìµœì¢… ë¡œê·¸
    try:
        logger.info(f"âœ… ì •ë¦¬ ì™„ë£Œ. ì¢…ë£Œ (exit code: {exit_code}, cleanup_success: {cleanup_success})")
    except:
        pass
    
    # ê°•ì œ ì¢…ë£Œ ë³´ì¥ (ë¬´í•œ ë¡œë”© ë°©ì§€)
    try:
        os._exit(exit_code)
    except:
        sys.exit(exit_code)

def main(step_name=None):
    global _status_updated
    if step_name is None:
        step_name = 'sector_reclassification_optimized'
    
    parser = argparse.ArgumentParser(description='ì „ì²´ ê¸°ì—… ì„¹í„° ì¬ë¶„ë¥˜ (Ensemble) - ìµœì í™” ë²„ì „')
    parser.add_argument('--limit', type=int, help='ì²˜ë¦¬í•  ê¸°ì—… ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)')
    parser.add_argument('--overwrite', action='store_true', help='ê¸°ì¡´ ë¶„ë¥˜ ë®ì–´ì“°ê¸°')
    parser.add_argument('--skip-existing', action='store_true', help='ê¸°ì¡´ ë¶„ë¥˜ ìŠ¤í‚µ')
    parser.add_argument('--batch-size', type=int, default=BATCH_EMBEDDING_SIZE, 
                       help=f'ì„ë² ë”© ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: {BATCH_EMBEDDING_SIZE})')
    parser.add_argument('--commit-size', type=int, default=BATCH_COMMIT_SIZE,
                       help=f'DB ì»¤ë°‹ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: {BATCH_COMMIT_SIZE})')
    parser.add_argument('--workers', type=int, default=MAX_WORKERS,
                       help=f'ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: {MAX_WORKERS})')
    args = parser.parse_args()
    
    # step_nameì´ ì „ë‹¬ë˜ì§€ ì•Šì•˜ê±°ë‚˜, limitì´ ìˆìœ¼ë©´ step_name ì¬ì„¤ì •
    if step_name == 'sector_reclassification_optimized' and args.limit:
        step_name = f'sector_reclassification_optimized_test_{args.limit}'
    
    # ì‹œì‘ ìƒíƒœ ê¸°ë¡
    write_status(step_name, 'running', {
        'limit': args.limit,
        'overwrite': args.overwrite,
        'batch_size': args.batch_size,
        'commit_size': args.commit_size,
        'workers': args.workers,
        'start_time': datetime.now().isoformat()
    })
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
    atexit.register(lambda: cleanup_and_exit(0, step_name))
    
    db = None
    try:
        # OpenAI API í‚¤ ê²€ì¦
        api_key = settings.OPENAI_API_KEY
        if not api_key:
            try:
                from dotenv import dotenv_values
                env_vars = dotenv_values(env_path)
                api_key = env_vars.get('OPENAI_API_KEY', '')
                if api_key:
                    os.environ['OPENAI_API_KEY'] = api_key
                    logger.info("âœ… .env íŒŒì¼ì—ì„œ OPENAI_API_KEY ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                logger.warning(f".env íŒŒì¼ ì§ì ‘ ì½ê¸° ì‹¤íŒ¨: {e}")
        
        use_gpt = bool(api_key and api_key.startswith('sk-'))
        if use_gpt:
            logger.info(f"âœ… OpenAI API í‚¤ í™•ì¸ ì™„ë£Œ (ì‹œì‘: {api_key[:15]}...)")
        else:
            logger.warning("âš ï¸  OpenAI API í‚¤ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. GPT ê²€ì¦ ë‹¨ê³„ê°€ ìŠ¤í‚µë©ë‹ˆë‹¤.")
        
        # ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™”
        import app.services.sector_classifier_embedding as sce
        sce._embedding_model = None
        sce._sector_reference_embeddings = None
        logger.info("âœ… ì„¹í„° ë¶„ë¥˜ìš© ì„ë² ë”© ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        db = SessionLocal()
        gemini_handler = GeminiHandler(api_key=api_key) if use_gpt else None
        
        # ì²˜ë¦¬í•  í‹°ì»¤ ëª©ë¡
        query = db.query(CompanyDetail.ticker).distinct()
        if args.limit:
            query = query.limit(args.limit)
        
        details = query.all()
        tickers = [t[0] for t in details]
        
        logger.info("=" * 80)
        logger.info(f"ğŸš€ ì„¹í„° ì¬ë¶„ë¥˜ ì‹œì‘ (ì™„ì „ ìµœì í™” ë²„ì „)")
        logger.info(f"ğŸ“Š ì´ ê¸°ì—… ìˆ˜: {len(tickers)}")
        logger.info(f"âš™ï¸  ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
        logger.info(f"âš™ï¸  ì»¤ë°‹ í¬ê¸°: {args.commit_size}")
        logger.info(f"âš™ï¸  ì›Œì»¤ ìˆ˜: {args.workers}")
        logger.info(f"âš™ï¸  GPT ì‚¬ìš©: {'ì˜ˆ' if use_gpt else 'ì•„ë‹ˆì˜¤'}")
        logger.info("=" * 80)
        
        # ì¹´ìš´í„° ë° í†µê³„
        success_counter = ThreadSafeCounter()
        skip_counter = ThreadSafeCounter()
        fail_counter = ThreadSafeCounter()
        stats = {'rule_based': 0, 'ensemble': 0}
        stats_lock = threading.Lock()
        
        # ë°°ì¹˜ ì»¤ë°‹ í
        batch_commit_queue = deque()
        
        # ë°°ì¹˜ ì²˜ë¦¬ ë£¨í”„
        total_batches = (len(tickers) + args.batch_size - 1) // args.batch_size
        
        for batch_idx in range(total_batches):
            if _shutdown_flag.is_set():
                logger.warning("ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ . ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨.")
                break
            
            batch_start = batch_idx * args.batch_size
            batch_end = min(batch_start + args.batch_size, len(tickers))
            batch_tickers = tickers[batch_start:batch_end]
            
            logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_idx+1}/{total_batches} ì²˜ë¦¬: {batch_start+1}-{batch_end}/{len(tickers)}")
            
            # ë°°ì¹˜ë¡œ ê¸°ì—… ë°ì´í„° ì¤€ë¹„
            companies_batch = prepare_company_data_batch(db, batch_tickers)
            
            if not companies_batch:
                logger.warning(f"ë°°ì¹˜ {batch_idx+1}ì— ì²˜ë¦¬í•  ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ë³‘ë ¬ë¡œ ì„¹í„° ë¶„ë¥˜ ì²˜ë¦¬
            futures = {}
            with ThreadPoolExecutor(max_workers=args.workers) as executor:
                for idx, company_data in enumerate(companies_batch):
                    if _shutdown_flag.is_set():
                        break
                    
                    ticker = company_data['ticker']
                    stock = company_data['stock']
                    company_detail = company_data['company_detail']
                    global_idx = batch_start + idx + 1
                    
                    future = executor.submit(
                        process_single_company_sector,
                        ticker, stock, company_detail,
                        gemini_handler, use_gpt,
                        global_idx, len(tickers),
                        args,
                        success_counter, skip_counter, fail_counter,
                        stats, stats_lock
                    )
                    futures[future] = ticker
                
                # ê²°ê³¼ ìˆ˜ì§‘ (íƒ€ì„ì•„ì›ƒ ì—†ì´ ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°)
                batch_results = []
                for future in as_completed(futures):
                    if _shutdown_flag.is_set():
                        break
                    
                    ticker = futures[future]
                    try:
                        ticker_result, results, error = future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                        if error is None and results:
                            batch_results.append((ticker_result, results))
                    except Exception as e:
                        logger.error(f"[{ticker}] Future ì²˜ë¦¬ ì˜¤ë¥˜: {e}", exc_info=True)
            
            # ê²°ê³¼ë¥¼ ì»¤ë°‹ íì— ì¶”ê°€
            batch_commit_queue.extend(batch_results)
            
            # ë°°ì¹˜ ì»¤ë°‹
            while len(batch_commit_queue) >= args.commit_size:
                commit_batch = [
                    batch_commit_queue.popleft() 
                    for _ in range(min(args.commit_size, len(batch_commit_queue)))
                ]
                
                commit_db = SessionLocal()
                try:
                    success, fail = save_results_batch(commit_db, commit_batch, args, stats, stats_lock)
                    commit_db.commit()
                    logger.info(f"âœ… ë°°ì¹˜ ì»¤ë°‹ ì™„ë£Œ: {len(commit_batch)}ê°œ (ì„±ê³µ: {success}, ì‹¤íŒ¨: {fail})")
                except Exception as e:
                    logger.error(f"âŒ ë°°ì¹˜ ì»¤ë°‹ ì‹¤íŒ¨: {e}", exc_info=True)
                    commit_db.rollback()
                finally:
                    commit_db.close()
            
            # ì§„í–‰ ìƒí™© ë¡œê·¸
            processed = success_counter.value + skip_counter.value + fail_counter.value
            if processed > 0 and processed % 100 == 0:
                logger.info(
                    f"ğŸ“Š ì§„í–‰ ìƒí™©: {processed}/{len(tickers)} "
                    f"(ì„±ê³µ: {success_counter.value}, ìŠ¤í‚µ: {skip_counter.value}, ì‹¤íŒ¨: {fail_counter.value})"
                )
        
        # ë‚¨ì€ ê²°ê³¼ ì»¤ë°‹
        if batch_commit_queue:
            commit_db = SessionLocal()
            try:
                success, fail = save_results_batch(commit_db, list(batch_commit_queue), args, stats, stats_lock)
                commit_db.commit()
                logger.info(f"âœ… ìµœì¢… ì»¤ë°‹ ì™„ë£Œ: {len(batch_commit_queue)}ê°œ (ì„±ê³µ: {success}, ì‹¤íŒ¨: {fail})")
            except Exception as e:
                logger.error(f"âŒ ìµœì¢… ì»¤ë°‹ ì‹¤íŒ¨: {e}", exc_info=True)
                commit_db.rollback()
            finally:
                commit_db.close()
        
        # ìµœì¢… ê²°ê³¼
        logger.info("=" * 80)
        logger.info("âœ… ì„¹í„° ì¬ë¶„ë¥˜ ì™„ë£Œ")
        logger.info(f"ğŸ“Š ì´ ê¸°ì—… ìˆ˜: {len(tickers)}")
        logger.info(f"âœ… ì„±ê³µ: {success_counter.value}")
        logger.info(f"â­ï¸  ìŠ¤í‚µ: {skip_counter.value}")
        logger.info(f"âŒ ì‹¤íŒ¨: {fail_counter.value}")
        if len(tickers) > 0:
            logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {success_counter.value / len(tickers) * 100:.1f}%")
        logger.info(f"ğŸ“Š Rule-based: {stats['rule_based']}")
        logger.info(f"ğŸ“Š Ensemble: {stats['ensemble']}")
        logger.info("=" * 80)
        
        # ìƒíƒœ íŒŒì¼ ì™„ë£Œ ê¸°ë¡
        write_status(step_name, 'completed', {
            'total': len(tickers),
            'success': success_counter.value,
            'skip': skip_counter.value,
            'failed': fail_counter.value,
            'success_rate': success_counter.value / len(tickers) * 100 if len(tickers) > 0 else 0,
            'rule_based': stats['rule_based'],
            'ensemble': stats['ensemble'],
            'end_time': datetime.now().isoformat()
        })
        
        return 0
        
    except KeyboardInterrupt:
        logger.warning("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        write_status(step_name, 'failed', {'error': 'ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨'})
        return 1
    except Exception as e:
        logger.error(f"ì „ì²´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        if db:
            db.rollback()
        write_status(step_name, 'failed', {'error': str(e)})
        return 1
    finally:
        if db:
            db.close()

if __name__ == "__main__":
    step_name = 'sector_reclassification_optimized'
    
    # íŒŒë¼ë¯¸í„° íŒŒì‹± (step_name ê²°ì •ìš©)
    parser = argparse.ArgumentParser(description='ì „ì²´ ê¸°ì—… ì„¹í„° ì¬ë¶„ë¥˜ (Ensemble) - ìµœì í™” ë²„ì „')
    parser.add_argument('--limit', type=int, help='ì²˜ë¦¬í•  ê¸°ì—… ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)')
    args, _ = parser.parse_known_args()
    
    if args.limit:
        step_name = f'sector_reclassification_optimized_test_{args.limit}'
    
    exit_code = 1  # ê¸°ë³¸ê°’ì€ ì‹¤íŒ¨
    try:
        exit_code = main(step_name=step_name)
        logger.info(f"âœ… ìŠ¤í¬ë¦½íŠ¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ (exit code: {exit_code})")
    except Exception as e:
        logger.error(f"ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)
        exit_code = 1
        write_status(step_name, 'failed', {'error': str(e)})
    finally:
        # ì •ë¦¬ ë° ê°•ì œ ì¢…ë£Œ (ë¬´í•œ ë¡œë”© ë°©ì§€)
        cleanup_and_exit(exit_code, step_name)
        # ëª…í™•í•œ ì¢…ë£Œ ì‹ í˜¸ ì¶œë ¥ (Cursor ì™„ë£Œ ê°ì§€ìš©)
        print("\n" + "="*80)
        print(f"SCRIPT_EXIT_CODE:{exit_code}")
        print("="*80 + "\n")
        sys.stdout.flush()
        sys.stderr.flush()
        sys.exit(exit_code)

