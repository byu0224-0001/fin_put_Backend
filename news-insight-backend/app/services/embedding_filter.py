"""
Embedding Filter Service

BGE-M3 ìž„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‹œë§¨í‹± í•„í„°ë§ ë° ì²­í¬ ì„ íƒ
"""
import logging
import re
import numpy as np
import torch
from typing import Optional, List, Dict, Any
from functools import lru_cache
from threading import Lock
from collections import defaultdict

try:
    from FlagEmbedding import BGEM3FlagModel
except ImportError:
    BGEM3FlagModel = None

try:
    from langchain_text_splitters import MarkdownHeaderTextSplitter
except ImportError:
    MarkdownHeaderTextSplitter = None

logger = logging.getLogger(__name__)

# ì„¤ì •
USE_EMBEDDING_FILTER = True
EMBEDDING_MODEL_NAME = 'BAAI/bge-m3'
EMBEDDING_BATCH_SIZE = 24
EMBEDDING_TOP_K = 6
EMBEDDING_MIN_SIM = 0.28
HEADER_MIN_SIM = 0.45
HEADER_TOP_K = 8  # 6 â†’ 8ë¡œ ì¦ê°€ (2ì°¨/3ì°¨ í—¤ë” ì²­í¬ ë” ë§Žì´ í¬í•¨)

# ì „ì—­ ë³€ìˆ˜
_embedding_model = None
_topic_vectors = None
_header_vectors = None
_embedding_lock = Lock()

# ì£¼ì œ ì„¤ëª… ë° í—¤ë” íƒ€ê²Ÿ (test_file.pyì—ì„œ ê°€ì ¸ì˜´)
TOPIC_DESCRIPTIONS = {
    'products_services': [
        "ì´ íšŒì‚¬ëŠ” ì–´ë–¤ ì œí’ˆì´ë‚˜ ì„œë¹„ìŠ¤ë¥¼ ì–´ë–¤ ê³ ê°ì—ê²Œ ì œê³µí•˜ë©° ê²½ìŸ ìš°ìœ„ë¥¼ ì–´ë–»ê²Œ ì„¤ëª…í•˜ëŠ”ì§€ ì–¸ê¸‰í•œ ë¬¸ë‹¨",
        "ì£¼ìš” ì‚¬ì—…ë¶€ë³„ ì œí’ˆ ë¼ì¸ì—…ê³¼ íŒë§¤ ì±„ë„, ëŒ€í‘œ ê³ ê° ì‚¬ë¡€ë¥¼ ì„œìˆ í•œ ë¬¸ë‹¨",
        "ë§¤ì¶œ ë¹„ì¤‘, í’ˆëª©ë³„ ë§¤ì¶œ, ë‚´ìˆ˜ ë° ìˆ˜ì¶œ êµ¬ì„±, ì£¼ìš” ìƒí‘œ, ìƒí’ˆ ë° ìš©ì—­ ë‚´ì—­ì„ ë‚˜ì—´í•œ ë¬¸ë‹¨",
    ],
    'supply_chain': [
        "í•µì‹¬ ì›ìž¬ë£Œë‚˜ ë¶€í’ˆ, ì¡°ë‹¬ì²˜, ê³µê¸‰ ë¦¬ìŠ¤í¬, ìƒì‚°ì„¤ë¹„ í˜„í™©ì„ ì„¤ëª…í•œ ë¬¸ë‹¨",
        "ì›ìž¬ë£Œ ë§¤ìž…ì•¡, ê°€ê²© ë³€ë™ ì¶”ì´, ì£¼ìš” ë§¤ìž…ì²˜, ê³µê¸‰ ê³„ì•½, ê°€ë™ë¥ , ìƒì‚°ëŠ¥ë ¥, CAPEXë¥¼ ë‹¤ë£¬ ë¬¸ë‹¨",
    ],
    'sales_orders': [
        "ë§¤ì¶œ êµ¬ì„±, ìˆ˜ì£¼ ìž”ê³ , ì§€ì—­ ë° ê³ ê°ë³„ ë§¤ì¶œ ë¹„ì¤‘ ë³€í™”ë¥¼ ì„¤ëª…í•œ ë¬¸ë‹¨",
        "ìˆ˜ì£¼ ìž”ê³ , ìˆ˜ì£¼ ìƒí™©, íŒë§¤ ê²½ë¡œ, íŒë§¤ ë°©ë²• ë° ì¡°ê±´, ì£¼ìš” ë§¤ì¶œì²˜ ë¹„ì¤‘ì„ ë‚˜ì—´í•œ ë¬¸ë‹¨",
    ],
    'strategy_outlook': [
        "ê²½ì˜ì§„ì´ ì œì‹œí•œ ì¤‘ìž¥ê¸° ì „ëžµ, ì‹ ì‚¬ì—… ê³„íš, íˆ¬ìž ê³„íš, ì„±ìž¥ ë™ë ¥ì„ ë‹¤ë£¬ ë¬¸ë‹¨",
        "ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨ ë° ë¶„ì„ì˜ê²¬, ìž¬ë¬´ìƒíƒœ ë° ì˜ì—…ì‹¤ì  ë¶„ì„, ì‹œìž¥ ì „ë§, ì™¸ë¶€ ë³€ìˆ˜ ëŒ€ì‘ì„ ì„¤ëª…í•œ ë¬¸ë‹¨",
    ],
    'financial_summary': [
        "ì‚¬ì—…ë¶€ë¬¸ë³„ ì‹¤ì  ìš”ì•½, ì†ìµ ë° ìžì‚°ì§€í‘œ, ìžë³¸/ìœ ë™ì„± ê´€ë¦¬ ë‚´ìš©ì„ ì •ë¦¬í•œ ë¬¸ë‹¨",
        "EBITDA, ì˜ì—…ì´ìµë¥ , CAPEX, ë°°ë‹¹ ì •ì±…, í˜„ê¸ˆíë¦„, ë ˆë²„ë¦¬ì§€ ë¹„ìœ¨ ë“± ìž¬ë¬´ ìš”ì•½ì„ ê¸°ìˆ í•œ ë¬¸ë‹¨"
    ],
    'risk_management': [
        "ìœ„í—˜ ê´€ë¦¬ ì •ì±…, íŒŒìƒìƒí’ˆÂ·í—·ì§€ ì „ëžµ, í™˜ìœ¨ ë° ê¸ˆë¦¬ ë¯¼ê°ë„ì— ëŒ€í•´ ì„¤ëª…í•œ ë¬¸ë‹¨",
        "ì‹œìž¥ìœ„í—˜ê´€ë¦¬, ì‹ ìš©ìœ„í—˜, ìœ ë™ì„±ìœ„í—˜, ìžë³¸ê´€ë¦¬, í—·ì§€ ë¹„ìœ¨, ì¶©ë‹¹ê¸ˆ ì •ì±…ì„ ë‚˜ì—´í•œ ë¬¸ë‹¨",
    ],
    'revenue_segment': [
        "ì‚¬ì—…ë¶€ë¬¸ë³„ ë§¤ì¶œ ë¹„ì¤‘, ë§¤ì¶œ êµ¬ì„±, ë¶€ë¬¸ë³„ ì‹¤ì ì„ ë‚˜ì—´í•œ ë¬¸ë‹¨",
        "ê° ì‚¬ì—…ë¶€ì˜ ë§¤ì¶œì•¡ê³¼ ë¹„ì¤‘ì„ ì„¤ëª…í•œ í‘œ",
        "ë§¤ì¶œ ë° ìˆ˜ì£¼ìƒí™©, ì£¼ìš” ì œí’ˆ ë§¤ì¶œ, ì œí’ˆë³„ ë§¤ì¶œì‹¤ì  í‘œ",
        "ì œí’ˆêµ°ë³„ ë˜ëŠ” ì‚¬ì—…ë¶€ë¬¸ë³„ ë§¤ì¶œì•¡ ë° ë§¤ì¶œ ë¹„ì¤‘",
        # ðŸ†• ê¸ˆìœµì‚¬ ì „ìš© í‚¤ì›Œë“œ ì¶”ê°€
        "ì˜ì—…ì˜ í˜„í™©, ì˜ì—…ì˜ ì¢…ë¥˜, ì‚¬ì—…ë¶€ë¬¸ë³„, ì˜ì—…ì¢…ë¥˜ë³„",
        "ë¶€ë¬¸ì •ë³´, ì˜ì—…ë¶€ë¬¸, ë¶€ë¬¸ë³„ ì†ìµ, ë‹¹ê¸°ì†ìµ, ì˜ì—…ì´ìµ, ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´, ì—°ê²°ë¶€ë¬¸",
        "ì´ìžì´ìµ, ìˆ˜ìˆ˜ë£Œì´ìµ, ë³´í—˜ì†ìµ, íˆ¬ìžì†ìµ",
        # ðŸ†• ë³´í—˜ì‚¬ ì „ìš© í‚¤ì›Œë“œ ì¶”ê°€
        "ë³´í—˜ë£Œìˆ˜ìµ, ë³´í—˜ìˆ˜ìµ, ë³´í—˜ì„œë¹„ìŠ¤ìˆ˜ìµ, ë³´í—˜ê¸ˆ, ì†í•´ìœ¨, ì‚¬ì—…ë¹„, ìƒí’ˆë³„, ì¢…ëª©ë³„",
        "ìž¥ê¸°ë³´í—˜, ìžë™ì°¨ë³´í—˜, ì¼ë°˜ë³´í—˜, ìƒëª…ë³´í—˜, ì†í•´ë³´í—˜",
    ],
}

HEADER_TARGETS = {
    'products_services': [
        "ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤, ì˜ì—…ì˜ í˜„í™©, ë§¤ì¶œ ë¹„ì¤‘, ìƒí’ˆ, ìš©ì—­, ìˆ˜ìˆ˜ë£Œ ìˆ˜ìµ",
        "ì„œë¹„ìŠ¤ ë°ì´í„°, í”Œëž«í¼ ì§€í‘œ, ì‚¬ì—…ë¶€ë³„ ë§¤ì¶œ êµ¬ì„±",
    ],
    'supply_chain': [
        "ì›ìž¬ë£Œ ë° ìƒì‚°ì„¤ë¹„, ë§¤ìž…, ì¡°ë‹¬, ìžê¸ˆì¡°ë‹¬ ë° ìš´ìš©, ë¹„ìš© êµ¬ì¡°",
        "í›„íŒ ê°€ê²©, ì›¨ì´í¼, ë¦¬íŠ¬, ì–‘ê·¹ìž¬ ë“± ê³µê¸‰ë§ ê´€ë ¨ í˜„í™©",
    ],
    'sales_orders': [
        "ë§¤ì¶œ ë° ìˆ˜ì£¼ìƒí™©, íŒë§¤ ê²½ë¡œ, ìˆ˜ì£¼ ìž”ê³ , ì˜ì—…ì‹¤ì ",
        "íŒë§¤ ê³„íš, ì¸ë„ ì¼ì •, ê³ ê°ë³„ ë§¤ì¶œ ë¹„ì¤‘",
    ],
    'strategy_outlook': [
        "ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨ ë° ë¶„ì„ì˜ê²¬, ì‚¬ì—…ì˜ ê°œìš”, ì¤‘ì  ì¶”ì§„ ì „ëžµ, ì‹ ê·œ ì‚¬ì—… ê³„íš",
        "ê²½ì˜ì§„ì´ ì–¸ê¸‰í•œ ì‹œìž¥ ì „ë§, ì‚°ì—… í™˜ê²½, ì„±ìž¥ ì „ëžµ"
    ],
    'risk_management': [
        "ìœ„í—˜ê´€ë¦¬, ì‹œìž¥ìœ„í—˜, íŒŒìƒìƒí’ˆ ê±°ëž˜ í˜„í™©, ìš°ë°œì±„ë¬´, ì œìž¬ ë° ê¸°íƒ€ ìœ„í—˜",
        "í™˜ìœ¨, ìœ ê°€, ê¸ˆë¦¬ ë¯¼ê°ë„ ë° í—·ì§€ ì •ì±…"
    ],
    'financial_summary': [
        "ìž¬ë¬´ì— ê´€í•œ ì‚¬í•­, ìžê¸ˆ ì¡°ë‹¬ ë° ìš´ìš©, ìž¬ë¬´ìƒíƒœ ìš”ì•½, ìžë³¸ ê´€ë¦¬",
        "ë¶€ë¬¸ë³„ ì‹¤ì  ìš”ì•½, ì†ìµ ë° ìžì‚° ì§€í‘œ"
    ],
    'revenue_segment': [
        "ë§¤ì¶œ ë° ìˆ˜ì£¼ìƒí™©, ë§¤ì¶œì‹¤ì , ë¶€ë¬¸ë³„ ë§¤ì¶œ, ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤",
        "ì œí’ˆë³„ ë§¤ì¶œ, ì‚¬ì—…ë¶€ë¬¸ë³„ ì‹¤ì , ë§¤ì¶œ êµ¬ì„±, ì˜ì—… í˜„í™©",
        # ðŸ†• ê¸ˆìœµì‚¬ ì „ìš© í‚¤ì›Œë“œ ì¶”ê°€
        "ì˜ì—…ì˜ í˜„í™©, ì˜ì—…ì˜ ì¢…ë¥˜, ë¶€ë¬¸ì •ë³´, ì˜ì—…ë¶€ë¬¸, ë¶€ë¬¸ë³„ ì†ìµ, ë‹¹ê¸°ì†ìµ",
        "ì´ìžì´ìµ, ìˆ˜ìˆ˜ë£Œì´ìµ, ë³´í—˜ì†ìµ, íˆ¬ìžì†ìµ, ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´, ì—°ê²°ë¶€ë¬¸",
        # ðŸ†• ë³´í—˜ì‚¬ ì „ìš© í‚¤ì›Œë“œ ì¶”ê°€
        "ë³´í—˜ë£Œìˆ˜ìµ, ë³´í—˜ìˆ˜ìµ, ë³´í—˜ì„œë¹„ìŠ¤ìˆ˜ìµ, ë³´í—˜ê¸ˆ, ì†í•´ìœ¨, ì‚¬ì—…ë¹„",
        "ìƒí’ˆë³„, ì¢…ëª©ë³„, ìž¥ê¸°ë³´í—˜, ìžë™ì°¨ë³´í—˜, ì¼ë°˜ë³´í—˜",
    ],
}


def ensure_embedding_model() -> bool:
    """ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì „ì—­ ì‹±ê¸€í†¤)"""
    global _embedding_model, _topic_vectors, _header_vectors
    
    if not USE_EMBEDDING_FILTER:
        return False
    
    if BGEM3FlagModel is None:
        logger.warning("FlagEmbedding ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False
    
    if _embedding_model is None:
        with _embedding_lock:
            if _embedding_model is None:
                logger.info("ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
                device = "cuda:0" if torch.cuda.is_available() else "cpu"
                use_fp16 = device != "cpu"
                try:
                    _embedding_model = BGEM3FlagModel(
                        EMBEDDING_MODEL_NAME,
                        use_fp16=use_fp16,
                        devices=device
                    )
                    logger.info(f"{device} ìž¥ì¹˜ì— ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (fp16={use_fp16})")
                except Exception as exc:
                    logger.error(f"ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {exc}")
                    return False
                
                # ë²¡í„° ë¹Œë“œ
                def build_vectors(source_dict):
                    entries = []
                    for key, sentences in source_dict.items():
                        for sentence in sentences:
                            entries.append((key, sentence))
                    if not entries:
                        return []
                    texts = [entry[1] for entry in entries]
                    encoded = _embedding_model.encode(texts, batch_size=len(texts), max_length=8192)
                    dense = encoded['dense_vecs']
                    vectors = []
                    for idx, vec in enumerate(dense):
                        norm = np.linalg.norm(vec)
                        normalized = vec if norm == 0 else vec / norm
                        vectors.append({'topic': entries[idx][0], 'vector': normalized})
                    return vectors
                
                _topic_vectors = build_vectors(TOPIC_DESCRIPTIONS)
                _header_vectors = build_vectors(HEADER_TARGETS)
                logger.info("ìž„ë² ë”© ëª¨ë¸ ë° ì£¼ì œ ë²¡í„° ì¤€ë¹„ ì™„ë£Œ")
    
    return True


def embed_texts(texts: List[str]) -> Optional[np.ndarray]:
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìž„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
    if not ensure_embedding_model():
        return None
    
    if not texts:
        return None
    
    encoded = _embedding_model.encode(
        texts,
        batch_size=min(EMBEDDING_BATCH_SIZE, len(texts)),
        max_length=8192
    )
    dense = encoded['dense_vecs']
    dense = np.array(dense)
    norms = np.linalg.norm(dense, axis=1, keepdims=True)
    norms[norms == 0] = 1e-9
    return dense / norms


def split_markdown_into_chunks(markdown_text: str) -> List[Dict[str, Any]]:
    """
    í—¤ë” ê¸°ë°˜ìœ¼ë¡œ ë§ˆí¬ë‹¤ìš´ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ê³„ì¸µ êµ¬ì¡° ë³´ì¡´)
    
    LangChain MarkdownHeaderTextSplitter ì‚¬ìš©:
    - ê³„ì¸µ êµ¬ì¡° ë©”íƒ€ë°ì´í„° ë³´ì¡´ (Header_1 > Header_2 > Header_3)
    - DART ë³´ê³ ì„œì˜ ë³µìž¡í•œ êµ¬ì¡° ì²˜ë¦¬
    - ê²€ì¦ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
    """
    if not markdown_text or not markdown_text.strip():
        return []
    
    # Fallback: LangChainì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
    if MarkdownHeaderTextSplitter is None:
        logger.warning("MarkdownHeaderTextSplitterê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ íŒŒì‹± ì‚¬ìš©.")
        return _split_markdown_simple(markdown_text)
    
    try:
        # DART ë³´ê³ ì„œ êµ¬ì¡°ì— ë§žì¶˜ í—¤ë” ë ˆë²¨ ì„¤ì •
        headers_to_split_on = [
            ("#", "Header_1"),      # ëŒ€ë¶„ë¥˜ (ì˜ˆ: II. ì‚¬ì—…ì˜ ë‚´ìš©)
            ("##", "Header_2"),     # ì¤‘ë¶„ë¥˜ (ì˜ˆ: 2. ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤)
            ("###", "Header_3"),    # ì†Œë¶„ë¥˜ (ì˜ˆ: ê°€. ì£¼ìš” ì œí’ˆ í˜„í™©)
        ]
        
        splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on,
            strip_headers=False  # í—¤ë” í…ìŠ¤íŠ¸ë¥¼ ë³¸ë¬¸ì—ë„ ìœ ì§€ (ë¬¸ë§¥ ë³´ì¡´)
        )
        
        # ë§ˆí¬ë‹¤ìš´ ë¶„ë¦¬
        docs = splitter.split_text(markdown_text)
        
        chunks = []
        for doc in docs:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ê³„ì¸µ ê²½ë¡œ ìƒì„±
            metadata = doc.metadata
            header_path_parts = []
            
            # Header_1 > Header_2 > Header_3 ìˆœì„œë¡œ ê²½ë¡œ êµ¬ì„±
            for level in ["Header_1", "Header_2", "Header_3"]:
                if level in metadata and metadata[level]:
                    header_path_parts.append(metadata[level].strip())
            
            # ê²½ë¡œ ë¬¸ìžì—´ ìƒì„± (ì˜ˆ: "II. ì‚¬ì—…ì˜ ë‚´ìš© > 2. ì£¼ìš” ì œí’ˆ")
            header_path = " > ".join(header_path_parts) if header_path_parts else "ê°œìš”"
            
            # ìµœìƒìœ„ í—¤ë”ë§Œ ì¶”ì¶œ (ê¸°ì¡´ 'heading' í•„ë“œ í˜¸í™˜ì„±)
            top_heading = header_path_parts[0] if header_path_parts else "ê°œìš”"
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            content = doc.page_content.strip()
            if not content:
                continue
            
            chunks.append({
                'heading': top_heading,          # ê¸°ì¡´ í˜¸í™˜ì„± (ìµœìƒìœ„ í—¤ë”ë§Œ)
                'header_path': header_path,      # ê³„ì¸µ ê²½ë¡œ (New!)
                'text': content,                 # ì²­í¬ ë‚´ìš©
                'metadata': metadata,            # ì „ì²´ ë©”íƒ€ë°ì´í„° (New!)
                'full_text': f"[{header_path}]\n{content}"  # ìž„ë² ë”©ìš© (ë¬¸ë§¥ í¬í•¨)
            })
        
        logger.debug(f"ë§ˆí¬ë‹¤ìš´ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        return chunks
        
    except Exception as e:
        logger.warning(f"MarkdownHeaderTextSplitter ì‹¤íŒ¨, ê¸°ë³¸ íŒŒì‹± ì‚¬ìš©: {e}")
        return _split_markdown_simple(markdown_text)


def _split_markdown_simple(markdown_text: str) -> List[Dict[str, Any]]:
    """ê¸°ë³¸ ë§ˆí¬ë‹¤ìš´ íŒŒì‹± (Fallback)"""
    lines = markdown_text.split('\n')
    chunks = []
    current_heading = "ê°œìš”"
    buffer = []
    
    for line in lines:
        stripped = line.strip()
        if stripped.startswith('#'):
            if buffer:
                chunk_text = '\n'.join(buffer).strip()
                if chunk_text:
                    chunks.append({
                        'heading': current_heading,
                        'header_path': current_heading,
                        'text': chunk_text,
                        'metadata': {'Header_1': current_heading},
                        'full_text': f"[{current_heading}]\n{chunk_text}"
                    })
                buffer = []
            current_heading = stripped
            buffer.append(line)
        else:
            buffer.append(line)
    
    if buffer:
        chunk_text = '\n'.join(buffer).strip()
        if chunk_text:
            chunks.append({
                'heading': current_heading,
                'header_path': current_heading,
                'text': chunk_text,
                'metadata': {'Header_1': current_heading},
                'full_text': f"[{current_heading}]\n{chunk_text}"
            })
    
    return chunks


def clean_heading_label(heading_line: str) -> str:
    """í—¤ë”© ë¼ë²¨ ì •ë¦¬"""
    if not heading_line:
        return "ê°œìš”"
    label = heading_line.replace('#', '').strip()
    label = re.sub(r'^\d+[\.\)]\s*', '', label)
    label = re.sub(r'^[IVXLCDM]+\.\s*', '', label, flags=re.IGNORECASE)
    label = re.sub(r'^[ê°€-íž£]+\.\s*', '', label)
    return label.strip() or "ê°œìš”"


def semantic_select_sections(markdown_text: str, ticker: Optional[str] = None) -> Optional[str]:
    """
    í—¤ë” ìž„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ì„¹ì…˜ ì„ ë³„ (ê³„ì¸µ êµ¬ì¡° í™œìš©)
    
    ê°œì„ ì‚¬í•­:
    - header_pathë¥¼ í™œìš©í•˜ì—¬ ë” ì •í™•í•œ ë¬¸ë§¥ ì¸ì‹
    - ê³„ì¸µ êµ¬ì¡° ë©”íƒ€ë°ì´í„°ë¡œ ë¶€ëª¨-ìžì‹ ê´€ê³„ íŒŒì•…
    """
    if not USE_EMBEDDING_FILTER or not markdown_text.strip():
        return None
    
    if not ensure_embedding_model() or not _header_vectors:
        return None
    
    chunks = split_markdown_into_chunks(markdown_text)
    if not chunks:
        return None
    
    # header_pathë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš© (ê³„ì¸µ êµ¬ì¡° ì •ë³´ í¬í•¨)
    heading_texts = []
    valid_chunks = []
    
    for chunk in chunks:
        # header_pathê°€ ìžˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ heading ì‚¬ìš©
        heading = chunk.get('header_path') or chunk.get('heading', '')
        if heading:
            heading_texts.append(heading)
            valid_chunks.append(chunk)
    
    if not heading_texts:
        return None
    
    # ê³„ì¸µ ê²½ë¡œ ì •ë³´ë¥¼ í¬í•¨í•œ í—¤ë” í…ìŠ¤íŠ¸ë¥¼ ìž„ë² ë”©
    embeddings = embed_texts(heading_texts)
    if embeddings is None:
        return None
    
    scored = []
    for idx, emb in enumerate(embeddings):
        scores = [(entry['topic'], float(np.dot(emb, entry['vector']))) for entry in _header_vectors]
        if not scores:
            continue
        best_topic, best_score = max(scores, key=lambda x: x[1])
        if best_score >= HEADER_MIN_SIM:
            chunk = valid_chunks[idx]
            scored.append({
                'topic': best_topic,
                'score': best_score,
                'heading': chunk.get('header_path') or chunk.get('heading', ''),
                'text': chunk.get('text', ''),
                'metadata': chunk.get('metadata', {})
            })
    
    if not scored:
        logger.debug("í—¤ë” ì‹œë§¨í‹± ë¼ìš°íŒ… ê²°ê³¼ ì—†ìŒ, ë°±ì—… ê²½ë¡œ ì‚¬ìš©")
        return None
    
    scored.sort(key=lambda x: x['score'], reverse=True)
    selected = scored[:HEADER_TOP_K]
    
    logger.info("í—¤ë” ê¸°ë°˜ ì„ íƒ ê²°ê³¼ (ê³„ì¸µ êµ¬ì¡° í™œìš©):")
    for item in selected:
        preview = item['heading'][:80]
        logger.info(f"  â€¢ {preview} -> {item['topic']} (score={item['score']:.3f})")
    
    combined = '\n\n'.join(item['text'] for item in selected if item['text'].strip())
    return combined if combined.strip() else None


def select_relevant_chunks(markdown_text: str, ticker: Optional[str] = None) -> str:
    """
    ìž„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ì²­í¬ ì„ ë³„ (ê³„ì¸µ êµ¬ì¡° í™œìš©)
    
    ê°œì„ ì‚¬í•­:
    - full_text (ë¬¸ë§¥ í¬í•¨)ë¥¼ ìž„ë² ë”©í•˜ì—¬ ë” ì •í™•í•œ ì˜ë¯¸ ë§¤ì¹­
    - header_path ë©”íƒ€ë°ì´í„°ë¡œ ì¶œì²˜ ì¶”ì  ê°€ëŠ¥
    """
    if not markdown_text or not markdown_text.strip():
        return ""
    
    # í—¤ë” ê¸°ë°˜ ì‹œë§¨í‹± ì„ íƒ ì‹œë„ (ê³„ì¸µ êµ¬ì¡° í™œìš©)
    semantic_text = None
    if USE_EMBEDDING_FILTER:
        semantic_text = semantic_select_sections(markdown_text, ticker=ticker)
        if semantic_text and len(semantic_text) > 200:
            return semantic_text
    
    # ì²­í¬ ê¸°ë°˜ ë°±ì—… ì„ íƒ
    chunks = split_markdown_into_chunks(markdown_text)
    if not chunks:
        return markdown_text
    
    # full_text ì‚¬ìš© (ë¬¸ë§¥ í¬í•¨): "[II. ì‚¬ì—…ì˜ ë‚´ìš© > 2. ì£¼ìš” ì œí’ˆ]\në‚´ìš©..."
    # ì´ë ‡ê²Œ í•˜ë©´ ìž„ë² ë”© ì‹œ ë¬¸ë§¥ ì •ë³´ê°€ í•¨ê»˜ ê³ ë ¤ë¨
    chunk_texts_for_embedding = [
        chunk.get('full_text', chunk.get('text', '')) for chunk in chunks
    ]
    embeddings = embed_texts(chunk_texts_for_embedding) if USE_EMBEDDING_FILTER else None
    
    if embeddings is None:
        return markdown_text
    
    scored_chunks = []
    topic_buckets = defaultdict(list)
    
    for idx, emb in enumerate(embeddings):
        topic_scores = [(entry['topic'], float(np.dot(emb, entry['vector']))) for entry in _topic_vectors]
        best_topic, best_score = max(topic_scores, key=lambda x: x[1])
        chunk = chunks[idx]
        chunk_info = {
            'heading': chunk.get('header_path') or chunk.get('heading', ''),
            'text': chunk.get('text', ''),
            'score': best_score,
            'topic': best_topic,
            'metadata': chunk.get('metadata', {})
        }
        scored_chunks.append(chunk_info)
        topic_buckets[best_topic].append(chunk_info)
    
    if not scored_chunks:
        return markdown_text
    
    scored_chunks.sort(key=lambda x: x['score'], reverse=True)
    filtered = [chunk for chunk in scored_chunks if chunk['score'] >= EMBEDDING_MIN_SIM]
    
    if not filtered:
        filtered = scored_chunks[:EMBEDDING_TOP_K]
    else:
        filtered = filtered[:EMBEDDING_TOP_K]
    
    selected_text = '\n\n'.join(chunk['text'] for chunk in filtered)
    return selected_text if selected_text.strip() else markdown_text

