"""
ì§ì ‘ BGE-M3 ëª¨ë¸ - FlagEmbedding ì™„ì „ ìš°íšŒ
Meta Tensor ì˜¤ë¥˜ë¥¼ ì™„ì „íˆ íšŒí”¼í•˜ê¸° ìœ„í•´ AutoModel + CLS pooling ì§ì ‘ êµ¬í˜„
"""
import logging
import os
import numpy as np
from typing import List, Union, Optional, Dict
import torch
from transformers import AutoModel, AutoTokenizer

# â­ ëª¨ë“ˆ ë¡œë“œ ì‹œì ì— accelerate ë¹„í™œì„±í™” (ê°€ì¥ ë¹ ë¥¸ ì‹œì ì— ì„¤ì •)
os.environ['TRANSFORMERS_NO_ACCELERATE'] = '1'
os.environ['ACCELERATE_USE_CPU'] = '1'

logger = logging.getLogger(__name__)


class DirectBGEM3Model:
    """
    FlagEmbeddingì„ ì™„ì „íˆ ìš°íšŒí•˜ëŠ” ì§ì ‘ BGE-M3 ëª¨ë¸
    AutoModel + CLS token poolingìœ¼ë¡œ ì„ë² ë”© ìƒì„±
    """
    
    def __init__(self, model_name: str = "BAAI/bge-m3", device: str = None, use_fp16: bool = True):
        """
        Args:
            model_name: HuggingFace ëª¨ë¸ ì´ë¦„
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', None=ìë™)
            use_fp16: FP16 ì‚¬ìš© ì—¬ë¶€ (GPUì—ì„œ ê¶Œì¥)
        """
        self.model_name = model_name
        self._model = None
        self._tokenizer = None
        self._device = None
        self.use_fp16 = use_fp16
        
        # ë””ë°”ì´ìŠ¤ ê²°ì •
        if device is None:
            if torch.cuda.is_available():
                device = "cuda"
            else:
                device = "cpu"
        
        self._load_model(device)
    
    def _load_model(self, target_device: str):
        """ëª¨ë¸ ë¡œë“œ (meta tensor ë¬¸ì œ ì™„ì „ íšŒí”¼, GPU ìš°ì„  ì‚¬ìš©)"""
        import time
        load_start = time.time()
        logger.info(f"ğŸ”„ Loading BGE-M3 model: {self.model_name} (Target: {target_device})")
        
        try:
            # 1. Tokenizer ë¡œë“œ
            self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            logger.info(f"âœ… BGE-M3 Tokenizer loaded: {self.model_name}")
            
            # 2. ëª¨ë¸ ë¡œë“œ (CPU ë¡œë“œ í›„ GPU ì´ë™)
            model_start = time.time()
            
            # CPUì— ì§ì ‘ ë¡œë“œ (meta tensor ë¬¸ì œ ì™„ì „ íšŒí”¼)
            # torch_dtypeì„ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ dtypeìœ¼ë¡œ ì‹¤ì œ ë©”ëª¨ë¦¬ì— ë¡œë“œë¨
            # í•˜ì§€ë§Œ ì—¬ì „íˆ meta tensor ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ëª…ì‹œì ìœ¼ë¡œ ì‹¤ì œ ë””ë°”ì´ìŠ¤ì— ë¡œë“œ
            import torch.nn as nn
            
            # ëª¨ë¸ì„ ë¡œë“œí•˜ë˜, ì‹¤ì œ ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ë„ë¡ ê°•ì œ
            self._model = AutoModel.from_pretrained(
                self.model_name,
                device_map=None,  # â­ device_map ì‚¬ìš© ì•ˆ í•¨
                low_cpu_mem_usage=False,
                torch_dtype=None  # â­ ëª…ì‹œì ìœ¼ë¡œ None ì§€ì •
            )
            
            # â­ ë”ë¯¸ forward passë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì‹¤ì œ ë©”ëª¨ë¦¬ì— ë¡œë“œ
            # ì´ë ‡ê²Œ í•˜ë©´ meta tensorê°€ ì‹¤ì œ tensorë¡œ ë³€í™˜ëœ í›„ .to('cpu') í˜¸ì¶œ ê°€ëŠ¥
            try:
                dummy_input = self._tokenizer("test", return_tensors="pt", padding=True, truncation=True)
                # ëª¨ë¸ì´ ì•„ì§ meta deviceì— ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì…ë ¥ë„ meta deviceì— ë§ì¶¤
                with torch.no_grad():
                    _ = self._model(**dummy_input)
                logger.info("âœ… [BGE-M3] ë”ë¯¸ forward passë¡œ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì‹¤ì œ ë©”ëª¨ë¦¬ì— ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ [BGE-M3] ë”ë¯¸ forward pass ì‹¤íŒ¨, ì§ì ‘ CPU ì´ë™ ì‹œë„: {e}")
            
            # ì´ì œ ëª¨ë¸ì„ CPUë¡œ ì´ë™ (ë”ë¯¸ forward pass í›„ì—ëŠ” meta tensorê°€ ì‹¤ì œ tensorë¡œ ë³€í™˜ë¨)
            try:
                self._model = self._model.to('cpu')
            except (NotImplementedError, RuntimeError) as e:
                logger.error(f"âŒ [BGE-M3] CPU ì´ë™ ì‹¤íŒ¨: {e}")
                raise
            
            self._device = "cpu"
            
            # GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)
            if target_device == "cuda" and torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                    # FP16 ì‚¬ìš© ì‹œ GPUë¡œ ì´ë™í•˜ë©´ì„œ dtype ë³€ê²½
                    if self.use_fp16:
                        self._model = self._model.to("cuda").half()
                    else:
                        self._model = self._model.to("cuda")
                    self._device = "cuda"
                    dtype_str = "float16" if self.use_fp16 else "float32"
                    logger.info(f"âœ… BGE-M3 ëª¨ë¸ GPU ì´ë™ ì™„ë£Œ (dtype={dtype_str})")
                except Exception as e:
                    logger.warning(f"âš ï¸ BGE-M3 GPU ì´ë™ ì‹¤íŒ¨, CPU ì‚¬ìš©: {e}")
                    self._device = "cpu"
            
            model_time = time.time() - model_start
            logger.info(f"âœ… BGE-M3 Model loaded ({model_time:.2f}ì´ˆ, device: {self._device})")
            
            # 3. í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self._model.eval()
                
        except Exception as e:
            logger.error(f"âŒ Failed to load BGE-M3 model: {e}")
            raise
        
        total_time = time.time() - load_start
        logger.info(f"âœ… BGE-M3 ì „ì²´ ë¡œë”© ì™„ë£Œ (ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ)")
    
    @property
    def device(self) -> str:
        """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self._device
    
    def _cls_pooling(self, model_output, attention_mask=None):
        """CLS token pooling - BGE-M3ì˜ ê¸°ë³¸ pooling ë°©ì‹"""
        # [CLS] í† í°ì˜ ì¶œë ¥ ì‚¬ìš© (ì²« ë²ˆì§¸ í† í°)
        return model_output.last_hidden_state[:, 0]
    
    def _mean_pooling(self, model_output, attention_mask):
        """Mean pooling (ëŒ€ì•ˆ)"""
        token_embeddings = model_output.last_hidden_state
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    def encode(
        self, 
        sentences: Union[str, List[str]], 
        batch_size: int = 12,
        max_length: int = 8192,
        convert_to_numpy: bool = True,
        normalize_embeddings: bool = True,
        show_progress_bar: bool = False,
        pooling_method: str = "cls"
    ) -> Union[np.ndarray, torch.Tensor, Dict]:
        """
        ë¬¸ì¥ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        
        Args:
            sentences: ë‹¨ì¼ ë¬¸ì¥ ë˜ëŠ” ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            max_length: ìµœëŒ€ í† í° ê¸¸ì´ (BGE-M3ëŠ” 8192ê¹Œì§€ ì§€ì›)
            convert_to_numpy: numpy ë°°ì—´ë¡œ ë³€í™˜ ì—¬ë¶€
            normalize_embeddings: ì„ë² ë”© ì •ê·œí™” ì—¬ë¶€
            show_progress_bar: ì§„í–‰ í‘œì‹œì¤„ í‘œì‹œ ì—¬ë¶€ (í˜„ì¬ ë¯¸êµ¬í˜„)
            pooling_method: í’€ë§ ë°©ë²• ('cls' ë˜ëŠ” 'mean')
        
        Returns:
            ì„ë² ë”© ë²¡í„° (numpy ë°°ì—´ ë˜ëŠ” torch í…ì„œ)
            ë˜ëŠ” FlagEmbedding í˜¸í™˜ í˜•ì‹: {'dense_vecs': embeddings}
        """
        # ë‹¨ì¼ ë¬¸ì¥ ì²˜ë¦¬
        if isinstance(sentences, str):
            sentences = [sentences]
        
        all_embeddings = []
        
        with torch.no_grad():
            for i in range(0, len(sentences), batch_size):
                batch = sentences[i:i + batch_size]
                
                # í† í¬ë‚˜ì´ì§•
                encoded_input = self._tokenizer(
                    batch,
                    padding=True,
                    truncation=True,
                    max_length=max_length,
                    return_tensors='pt'
                )
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                encoded_input = {k: v.to(self._device) for k, v in encoded_input.items()}
                
                # ëª¨ë¸ ì¶”ë¡ 
                model_output = self._model(**encoded_input)
                
                # Pooling
                if pooling_method == "cls":
                    embeddings = self._cls_pooling(model_output)
                else:
                    embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])
                
                # ì •ê·œí™”
                if normalize_embeddings:
                    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
                
                # Meta tensor ë¬¸ì œ íšŒí”¼: clone í›„ CPUë¡œ ì´ë™
                # clone()ì€ ì‹¤ì œ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ë¯€ë¡œ meta tensor ë¬¸ì œ íšŒí”¼
                try:
                    embeddings_cpu = embeddings.clone().detach().cpu()
                    all_embeddings.append(embeddings_cpu)
                except (NotImplementedError, RuntimeError) as e:
                    # clone()ë„ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš° (ë§¤ìš° ë“œë¬¾): numpy ê²½ìœ 
                    logger.warning(f"âš ï¸ [BGE-M3] clone() ì‹¤íŒ¨, numpy ê²½ìœ : {e}")
                    if convert_to_numpy:
                        # numpyë¡œ ì§ì ‘ ë³€í™˜
                        with torch.no_grad():
                            embeddings_np = embeddings.cpu().numpy()
                        all_embeddings.append(torch.from_numpy(embeddings_np))
                    else:
                        raise e
        
        # ê²°ê³¼ í•©ì¹˜ê¸°
        all_embeddings = torch.cat(all_embeddings, dim=0)
        
        if convert_to_numpy:
            return all_embeddings.numpy()
        
        return all_embeddings
    
    def encode_queries(
        self,
        queries: Union[str, List[str]],
        batch_size: int = 12,
        max_length: int = 8192,
        **kwargs
    ) -> Dict[str, np.ndarray]:
        """
        ì¿¼ë¦¬ ì¸ì½”ë”© (FlagEmbedding í˜¸í™˜)
        
        Returns:
            {'dense_vecs': embeddings}
        """
        embeddings = self.encode(
            queries,
            batch_size=batch_size,
            max_length=max_length,
            convert_to_numpy=True,
            **kwargs
        )
        return {'dense_vecs': embeddings}
    
    def encode_corpus(
        self,
        corpus: Union[str, List[str]],
        batch_size: int = 12,
        max_length: int = 8192,
        **kwargs
    ) -> Dict[str, np.ndarray]:
        """
        ì½”í¼ìŠ¤ ì¸ì½”ë”© (FlagEmbedding í˜¸í™˜)
        
        Returns:
            {'dense_vecs': embeddings}
        """
        embeddings = self.encode(
            corpus,
            batch_size=batch_size,
            max_length=max_length,
            convert_to_numpy=True,
            **kwargs
        )
        return {'dense_vecs': embeddings}
    
    def to(self, device: str):
        """ë””ë°”ì´ìŠ¤ ì´ë™"""
        if device == "cuda" and torch.cuda.is_available():
            self._model = self._model.to("cuda")
            self._device = "cuda"
        else:
            self._model = self._model.to("cpu")
            self._device = "cpu"
        return self
    
    def get_sentence_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self._model.config.hidden_size


# ì „ì—­ ëª¨ë¸ ìºì‹œ
_direct_bge_model = None


def get_direct_bge_model(
    model_name: str = "BAAI/bge-m3",
    device: str = None,
    use_fp16: bool = True
) -> DirectBGEM3Model:
    """
    ì§ì ‘ BGE-M3 ëª¨ë¸ ë¡œë“œ (ìºì‹±)
    
    Args:
        model_name: HuggingFace ëª¨ë¸ ì´ë¦„
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', None=ìë™)
        use_fp16: FP16 ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        DirectBGEM3Model ì¸ìŠ¤í„´ìŠ¤
    """
    global _direct_bge_model
    
    if _direct_bge_model is None:
        _direct_bge_model = DirectBGEM3Model(
            model_name=model_name,
            device=device,
            use_fp16=use_fp16
        )
    
    return _direct_bge_model
