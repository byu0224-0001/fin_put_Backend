"""
DART Parser Service

DART API í˜¸ì¶œ, HTML íŒŒì‹±, ë§ˆí¬ë‹¤ìš´ ë³€í™˜, í•µì‹¬ ì„¹ì…˜ ì¶”ì¶œ
ì´ì¤‘ ë§¤í•‘ ì „ëµ: íšŒì‚¬ëª… â†’ ê³ ìœ ë²ˆí˜¸ë¡œ ìë™ ì¬ì‹œë„
"""
import logging
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from typing import Optional, Dict, Any
import OpenDartReader
from app.services.retry_handler import retry_dart_api, dart_rate_limiter
from app.services.dart_corp_code_mapper import DartCorpCodeMapper

logger = logging.getLogger(__name__)


class DartParser:
    """DART API íŒŒì„œ (ì´ì¤‘ ë§¤í•‘ ì „ëµ ì§€ì›)"""
    
    def __init__(self, api_key: str):
        """
        Args:
            api_key: DART API Key
        """
        self.api_key = api_key  # ì§ì ‘ API í˜¸ì¶œì„ ìœ„í•´ ì €ì¥
        self.dart = OpenDartReader(api_key)
        self.corp_code_mapper = DartCorpCodeMapper(api_key)  # ê³ ìœ ë²ˆí˜¸ ë§¤í•‘ ì„œë¹„ìŠ¤
    
    @retry_dart_api
    @dart_rate_limiter
    def list_reports(
        self,
        ticker: str,
        start_date: str,
        end_date: str,
        kind: str = 'A',
        final: bool = True,
        business_report_only: bool = False
    ) -> Optional[pd.DataFrame]:
        """
        DART APIë¡œ ë³´ê³ ì„œ ëª©ë¡ ì¡°íšŒ (ì´ì¤‘ ì‹œë„: íšŒì‚¬ëª… â†’ ê³ ìœ ë²ˆí˜¸)
        
        Args:
            ticker: ì¢…ëª©ì½”ë“œ
            start_date: ì‹œì‘ì¼ (YYYY-MM-DD)
            end_date: ì¢…ë£Œì¼ (YYYY-MM-DD)
            kind: ê³µì‹œ ì¢…ë¥˜ ('A': ì •ê¸°ê³µì‹œ)
            final: ìµœì¢…ë³´ê³ ì„œë§Œ ì¡°íšŒ ì—¬ë¶€
            business_report_only: ì‚¬ì—…ë³´ê³ ì„œë§Œ ì¡°íšŒ ì—¬ë¶€ (Trueì´ë©´ pblntf_detail_ty='A001' ì‚¬ìš©)
        
        Returns:
            ë³´ê³ ì„œ ëª©ë¡ DataFrame ë˜ëŠ” None
        """
        # ì‚¬ì—…ë³´ê³ ì„œë§Œ ì¡°íšŒí•˜ëŠ” ê²½ìš°, ê³ ìœ ë²ˆí˜¸ ê¸°ë°˜ ì§ì ‘ í˜¸ì¶œ
        if business_report_only:
            return self._list_business_reports_direct(ticker, start_date, end_date, final)
        
        # 1ì°¨ ì‹œë„: ê¸°ì¡´ ë°©ì‹ (OpenDartReader - íšŒì‚¬ëª… ê¸°ë°˜)
        try:
            reports = self.dart.list(ticker, start=start_date, end=end_date, kind=kind, final=final)
            if reports is not None and len(reports) > 0:
                logger.debug(f"{ticker}: íšŒì‚¬ëª… ê¸°ë°˜ ê²€ìƒ‰ ì„±ê³µ ({len(reports)}ê°œ ë³´ê³ ì„œ)")
                return reports
        except Exception as e:
            logger.warning(f"{ticker}: íšŒì‚¬ëª… ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # 2ì°¨ ì‹œë„: ê³ ìœ ë²ˆí˜¸ ê¸°ë°˜ ì§ì ‘ í˜¸ì¶œ
        logger.info(f"{ticker}: íšŒì‚¬ëª… ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨, ê³ ìœ ë²ˆí˜¸ ê¸°ë°˜ ì¬ì‹œë„")
        return self._list_reports_by_corp_code(ticker, start_date, end_date, kind, final)
    
    def _list_business_reports_direct(
        self,
        ticker: str,
        start_date: str,
        end_date: str,
        final: bool = True
    ) -> Optional[pd.DataFrame]:
        """
        DART API ì§ì ‘ í˜¸ì¶œë¡œ ì‚¬ì—…ë³´ê³ ì„œë§Œ ì¡°íšŒ (ê³ ìœ ë²ˆí˜¸ ê¸°ë°˜)
        
        Args:
            ticker: ì¢…ëª©ì½”ë“œ
            start_date: ì‹œì‘ì¼ (YYYY-MM-DD)
            end_date: ì¢…ë£Œì¼ (YYYY-MM-DD)
            final: ìµœì¢…ë³´ê³ ì„œë§Œ ì¡°íšŒ ì—¬ë¶€
        
        Returns:
            ì‚¬ì—…ë³´ê³ ì„œ ëª©ë¡ DataFrame ë˜ëŠ” None
        """
        # ê³ ìœ ë²ˆí˜¸ ì¡°íšŒ
        corp_code = self.corp_code_mapper.get_corp_code(ticker)
        
        if not corp_code:
            logger.error(f"{ticker}: ê³ ìœ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        return self._list_reports_by_corp_code(
            ticker, start_date, end_date, 
            kind='A', final=final, 
            pblntf_detail_ty='A001'  # ì‚¬ì—…ë³´ê³ ì„œë§Œ
        )
    
    def _list_reports_by_corp_code(
        self,
        ticker: str,
        start_date: str,
        end_date: str,
        kind: str = 'A',
        final: bool = True,
        pblntf_detail_ty: Optional[str] = None
    ) -> Optional[pd.DataFrame]:
        """
        ê³ ìœ ë²ˆí˜¸ ê¸°ë°˜ìœ¼ë¡œ ë³´ê³ ì„œ ëª©ë¡ ì¡°íšŒ
        
        Args:
            ticker: ì¢…ëª©ì½”ë“œ
            start_date: ì‹œì‘ì¼ (YYYY-MM-DD)
            end_date: ì¢…ë£Œì¼ (YYYY-MM-DD)
            kind: ê³µì‹œ ì¢…ë¥˜ ('A': ì •ê¸°ê³µì‹œ)
            final: ìµœì¢…ë³´ê³ ì„œë§Œ ì¡°íšŒ ì—¬ë¶€
            pblntf_detail_ty: ê³µì‹œ ìƒì„¸ ìœ í˜• (ì˜ˆ: 'A001' = ì‚¬ì—…ë³´ê³ ì„œ)
        
        Returns:
            ë³´ê³ ì„œ ëª©ë¡ DataFrame ë˜ëŠ” None
        """
        try:
            # ê³ ìœ ë²ˆí˜¸ ì¡°íšŒ
            corp_code = self.corp_code_mapper.get_corp_code(ticker)
            
            if not corp_code:
                logger.error(f"{ticker}: ê³ ìœ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (YYYY-MM-DD -> YYYYMMDD)
            start_date_formatted = start_date.replace('-', '')
            end_date_formatted = end_date.replace('-', '')
            
            # DART API ì§ì ‘ í˜¸ì¶œ
            url = "https://opendart.fss.or.kr/api/list.json"
            params = {
                'crtfc_key': self.api_key,
                'corp_code': corp_code,  # ê³ ìœ ë²ˆí˜¸ ì‚¬ìš©
                'bgn_de': start_date_formatted,
                'end_de': end_date_formatted,
                'pblntf_ty': kind,  # 'A': ì •ê¸°ê³µì‹œ
                'page_no': '1',
                'page_count': '100',
                'last_reprt_at': 'Y' if final else 'N',
                'sort': 'date',
                'sort_mth': 'desc'
            }
            
            # ì‚¬ì—…ë³´ê³ ì„œë§Œ ì¡°íšŒí•˜ëŠ” ê²½ìš°
            if pblntf_detail_ty:
                params['pblntf_detail_ty'] = pblntf_detail_ty
            
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if data.get('status') != '000':
                error_msg = data.get('message', 'Unknown error')
                logger.error(f"DART API ì˜¤ë¥˜ ({ticker}): {error_msg}")
                return None
            
            reports_list = data.get('list', [])
            if not reports_list:
                logger.debug(f"{ticker}: ê³ ìœ ë²ˆí˜¸ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return None
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            df = pd.DataFrame(reports_list)
            logger.info(f"{ticker}: ê³ ìœ ë²ˆí˜¸ ê¸°ë°˜ ê²€ìƒ‰ ì„±ê³µ ({len(df)}ê°œ ë³´ê³ ì„œ)")
            return df
            
        except requests.RequestException as e:
            logger.error(f"ê³ ìœ ë²ˆí˜¸ ê¸°ë°˜ ë³´ê³ ì„œ ì¡°íšŒ ì‹¤íŒ¨ ({ticker}): ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ - {e}")
            return None
        except Exception as e:
            logger.error(f"ê³ ìœ ë²ˆí˜¸ ê¸°ë°˜ ë³´ê³ ì„œ ì¡°íšŒ ì‹¤íŒ¨ ({ticker}): {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return None
    
    @retry_dart_api
    @dart_rate_limiter
    def get_sub_docs(self, rcept_no: str) -> Optional[pd.DataFrame]:
        """
        ë³´ê³ ì„œì˜ í•˜ìœ„ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
        
        Args:
            rcept_no: ì ‘ìˆ˜ë²ˆí˜¸
        
        Returns:
            í•˜ìœ„ ë¬¸ì„œ ëª©ë¡ DataFrame ë˜ëŠ” None
        """
        try:
            sub_docs = self.dart.sub_docs(rcept_no)
            return sub_docs
        except Exception as e:
            logger.error(f"DART API í•˜ìœ„ ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨ ({rcept_no}): {e}")
            return None
    
    def find_business_report(
        self,
        ticker: str,
        target_year: int = None
    ) -> Optional[Dict[str, Any]]:
        """
        ì‚¬ì—…ë³´ê³ ì„œ ì°¾ê¸° (ë‚ ì§œ ì œí•œ ì—†ì´ ìµœì‹  ë³´ê³ ì„œ ê°€ì ¸ì˜¤ê¸°)
        
        Args:
            ticker: ì¢…ëª©ì½”ë“œ
            target_year: ëŒ€ìƒ ì—°ë„ (Noneì´ë©´ ìµœì‹  ë³´ê³ ì„œ ê°€ì ¸ì˜¤ê¸°)
        
        Returns:
            {
                'rcept_no': ì ‘ìˆ˜ë²ˆí˜¸,
                'report_nm': ë³´ê³ ì„œëª…,
                'rcept_dt': ì ‘ìˆ˜ì¼ì (YYYYMMDD í˜•ì‹),
                'report': ë³´ê³ ì„œ ì •ë³´ DataFrame
            } ë˜ëŠ” None
        """
        from datetime import datetime
        import re
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •: ìµœê·¼ 3ë…„ì¹˜ ê²€ìƒ‰ (ì‚¬ì—…ë³´ê³ ì„œëŠ” ë³´í†µ ë‹¤ìŒ í•´ì— ì œì¶œ)
        current_year = datetime.now().year
        if target_year:
            # íŠ¹ì • ì—°ë„ ì§€ì • ì‹œ: í•´ë‹¹ ì—°ë„ë¶€í„° í˜„ì¬ê¹Œì§€ ê²€ìƒ‰
            start_date = f'{target_year}-01-01'
        else:
            # ì—°ë„ ë¯¸ì§€ì • ì‹œ: ìµœê·¼ 3ë…„ì¹˜ ê²€ìƒ‰
            start_date = f'{current_year - 3}-01-01'
        
        # ì¢…ë£Œì¼ì€ í˜„ì¬ ë‚ ì§œë¡œ ì„¤ì • (ì œí•œ ì—†ìŒ)
        end_date = datetime.now().strftime('%Y-%m-%d')
        
        # ë¨¼ì € ì‚¬ì—…ë³´ê³ ì„œë§Œ ì¡°íšŒ ì‹œë„ (pblntf_detail_ty='A001' ì‚¬ìš©)
        business_reports_only = self.list_reports(ticker, start_date, end_date, kind='A', final=True, business_report_only=True)
        business_reports = None
        
        if business_reports_only is not None and len(business_reports_only) > 0:
            business_reports = business_reports_only
            logger.info(f"{ticker}: ì‚¬ì—…ë³´ê³ ì„œ {len(business_reports)}ê°œ ë°œê²¬")
        else:
            # ì‚¬ì—…ë³´ê³ ì„œê°€ ì—†ìœ¼ë©´ ì •ê¸° ê³µì‹œ ì „ì²´ ì¡°íšŒ (A001, A002, A003)
            logger.info(f"{ticker}: ì‚¬ì—…ë³´ê³ ì„œ ì—†ìŒ. ì •ê¸° ê³µì‹œ ì „ì²´ ì¡°íšŒ ì‹œë„...")
            all_regular = self.list_reports(ticker, start_date, end_date, kind='A', final=True, business_report_only=False)
            if all_regular is not None and len(all_regular) > 0:
                # ì •ê¸° ê³µì‹œë§Œ í•„í„°ë§ (A001: ì‚¬ì—…ë³´ê³ ì„œ, A002: ë°˜ê¸°ë³´ê³ ì„œ, A003: ë¶„ê¸°ë³´ê³ ì„œ)
                regular_types = ['A001', 'A002', 'A003']
                if 'pblntf_detail_ty' in all_regular.columns:
                    filtered_regular = all_regular[all_regular['pblntf_detail_ty'].isin(regular_types)]
                    if len(filtered_regular) > 0:
                        business_reports = filtered_regular
                        logger.info(f"{ticker}: ì •ê¸° ê³µì‹œ {len(business_reports)}ê°œ ë°œê²¬")
                    else:
                        logger.warning(f"{ticker}: ì •ê¸° ê³µì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        return None
                else:
                    # pblntf_detail_ty ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ëª¨ë‘ ì‚¬ìš©
                    business_reports = all_regular
                    logger.info(f"{ticker}: ì •ê¸° ê³µì‹œ {len(business_reports)}ê°œ ë°œê²¬ (íƒ€ì… í•„í„°ë§ ì—†ìŒ)")
            else:
                logger.warning(f"{ticker}: ì •ê¸° ê³µì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
        
        if business_reports is None or len(business_reports) == 0:
            logger.warning(f"{ticker}: ë³´ê³ ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None
        
        # ë³´ê³ ì„œ ì œëª©ì—ì„œ ì—°ë„ ì¶”ì¶œ í•¨ìˆ˜
        def extract_year_from_title(title: str) -> Optional[int]:
            """ë³´ê³ ì„œ ì œëª©ì—ì„œ ì—°ë„ ì¶”ì¶œ (ì˜ˆ: 'ì‚¬ì—…ë³´ê³ ì„œ (2024.12)' -> 2024)"""
            if not title:
                return None
            # íŒ¨í„´: (YYYY.MM) ë˜ëŠ” (YYYY.MM.DD) ë˜ëŠ” YYYY.12 ë“±
            patterns = [
                r'\((\d{4})\.\d{1,2}',  # (2024.12)
                r'\((\d{4})\.\d{1,2}\.\d{1,2}',  # (2024.12.31)
                r'(\d{4})\.\d{1,2}',  # 2024.12
                r'(\d{4})\s*ë…„\s*ì‚¬ì—…ë³´ê³ ì„œ',  # 2024ë…„ ì‚¬ì—…ë³´ê³ ì„œ
                r'(\d{4})\s*ì‚¬ì—…ë³´ê³ ì„œ',  # 2024 ì‚¬ì—…ë³´ê³ ì„œ
            ]
            for pattern in patterns:
                match = re.search(pattern, str(title))
                if match:
                    try:
                        return int(match.group(1))
                    except:
                        continue
            return None
        
        # target_yearê°€ ì§€ì •ëœ ê²½ìš°, ë³´ê³ ì„œ ì œëª©ì—ì„œ ì—°ë„ ì¶”ì¶œí•˜ì—¬ í•„í„°ë§
        if target_year:
            # ëª¨ë“  ë³´ê³ ì„œì— ëŒ€í•´ ì—°ë„ ì¶”ì¶œ
            business_reports['report_year'] = business_reports['report_nm'].apply(extract_year_from_title)
            
            # target_yearì™€ ì¼ì¹˜í•˜ëŠ” ë³´ê³ ì„œë§Œ í•„í„°ë§
            filtered_reports = business_reports[business_reports['report_year'] == target_year]
            
            if len(filtered_reports) > 0:
                # í•„í„°ë§ëœ ë³´ê³ ì„œ ì¤‘ ì ‘ìˆ˜ì¼ì ê¸°ì¤€ ìµœì‹  ê²ƒ ì„ íƒ
                business_reports = filtered_reports
                logger.info(f"{ticker}: {target_year}ë…„ ì‚¬ì—…ë³´ê³ ì„œ {len(business_reports)}ê°œ ë°œê²¬")
            else:
                # target_yearì™€ ì¼ì¹˜í•˜ëŠ” ë³´ê³ ì„œê°€ ì—†ìœ¼ë©´ ìµœì‹  ì •ê¸° ê³µì‹œ(ë¶„ê¸°/ë°˜ê¸°/ì‚¬ì—…ë³´ê³ ì„œ) ì‚¬ìš© (Fallback)
                available_years = business_reports['report_year'].dropna().unique().tolist()
                logger.warning(f"{ticker}: {target_year}ë…„ ì‚¬ì—…ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                logger.warning(f"  ì‚¬ìš© ê°€ëŠ¥í•œ ì—°ë„: {sorted(available_years)}")
                logger.warning(f"  ì‚¬ìš© ê°€ëŠ¥í•œ ë³´ê³ ì„œ: {business_reports['report_nm'].tolist()[:5]}")
                logger.info(f"{ticker}: ìµœì‹  ì •ê¸° ê³µì‹œ(ë¶„ê¸°/ë°˜ê¸°/ì‚¬ì—…ë³´ê³ ì„œ)ë¥¼ ì°¾ìŠµë‹ˆë‹¤ (Fallback)")
                
                # ì •ê¸° ê³µì‹œ ì „ì²´ ì¡°íšŒ (A001: ì‚¬ì—…ë³´ê³ ì„œ, A002: ë°˜ê¸°ë³´ê³ ì„œ, A003: ë¶„ê¸°ë³´ê³ ì„œ)
                all_regular_reports = self.list_reports(ticker, start_date, end_date, kind='A', final=True, business_report_only=False)
                if all_regular_reports is not None and len(all_regular_reports) > 0:
                    # ì •ê¸° ê³µì‹œë§Œ í•„í„°ë§ (A001, A002, A003)
                    regular_types = ['A001', 'A002', 'A003']
                    if 'pblntf_detail_ty' in all_regular_reports.columns:
                        filtered_regular = all_regular_reports[all_regular_reports['pblntf_detail_ty'].isin(regular_types)]
                        if len(filtered_regular) > 0:
                            # ì ‘ìˆ˜ì¼ì ê¸°ì¤€ ì •ë ¬ (ìµœì‹ ìˆœ)
                            if 'rcept_dt' in filtered_regular.columns:
                                filtered_regular = filtered_regular.sort_values('rcept_dt', ascending=False)
                            business_reports = filtered_regular
                            logger.info(f"{ticker}: ìµœì‹  ì •ê¸° ê³µì‹œ ë°œê²¬: {business_reports.iloc[0]['report_nm']}")
                        else:
                            logger.warning(f"{ticker}: ì •ê¸° ê³µì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ ì‚¬ì—…ë³´ê³ ì„œ ëª©ë¡ ì‚¬ìš©")
                    else:
                        logger.warning(f"{ticker}: pblntf_detail_ty ì»¬ëŸ¼ ì—†ìŒ. ê¸°ì¡´ ì‚¬ì—…ë³´ê³ ì„œ ëª©ë¡ ì‚¬ìš©")
                else:
                    logger.warning(f"{ticker}: ì •ê¸° ê³µì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ ì‚¬ì—…ë³´ê³ ì„œ ëª©ë¡ ì‚¬ìš©")
        
        # ì ‘ìˆ˜ì¼ì ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ) - ê°€ì¥ ìµœì‹  ë³´ê³ ì„œ ì„ íƒ
        if 'rcept_dt' in business_reports.columns:
            business_reports = business_reports.sort_values('rcept_dt', ascending=False)
        
        # ìµœì‹  ì‚¬ì—…ë³´ê³ ì„œ ì„ íƒ
        target_report = business_reports.iloc[0]
        
        return {
            'rcept_no': target_report['rcept_no'],
            'report_nm': target_report['report_nm'],
            'rcept_dt': target_report.get('rcept_dt'),  # ì ‘ìˆ˜ì¼ì (YYYYMMDD í˜•ì‹)
            'report': target_report
        }
    
    def get_raw_html_for_revenue_sections(self, ticker: str, year: Optional[int] = None, is_financial: bool = False) -> Optional[str]:
        """
        ë§¤ì¶œ ë¹„ì¤‘ ì¶”ì¶œì„ ìœ„í•œ ì›ë¬¸ HTML ê°€ì ¸ì˜¤ê¸°
        
        Args:
            ticker: ì¢…ëª©ì½”ë“œ
            year: ì—°ë„
            is_financial: ê¸ˆìœµì‚¬ ì—¬ë¶€ (ê¸ˆìœµì‚¬ëŠ” "ë¶€ë¬¸ë³„ ì˜ì—…ìˆ˜ì§€" ê°™ì€ íŠ¹ì • ì„¹ì…˜ ìš°ì„ )
        
        Returns:
            HTML ë¬¸ìì—´ ë˜ëŠ” None
        """
        try:
            report_info = self.find_business_report(ticker, year)
            if not report_info:
                return None
            
            rcept_no = report_info['rcept_no']
            sub_docs = self.get_sub_docs(rcept_no)
            if sub_docs is None or len(sub_docs) == 0:
                return None
            
            # ê¸ˆìœµì‚¬ ì „ìš©: "ë¶€ë¬¸ë³„ ì˜ì—…ìˆ˜ì§€" ê°™ì€ íŠ¹ì • ì„¹ì…˜ ìš°ì„  ê²€ìƒ‰
            if is_financial:
                financial_specific_keywords = ['ë¶€ë¬¸ë³„ ì˜ì—…ìˆ˜ì§€', 'ë¶€ë¬¸ë³„ ì˜ì—…ì‹¤ì ', 'ì´ì ë¶€ë¬¸', 'ìˆ˜ìˆ˜ë£Œ ë¶€ë¬¸', 'ì˜ì—…ìˆ˜ì§€']
                for idx, row in sub_docs.iterrows():
                    title = str(row.get('title', ''))
                    url = row.get('url', '')
                    
                    if any(keyword in title for keyword in financial_specific_keywords):
                        html = self.fetch_section_content(url)
                        if html:
                            logger.info(f"{ticker}: ê¸ˆìœµì‚¬ ì „ìš© ì„¹ì…˜ HTML ì¶”ì¶œ ì„±ê³µ: {title}")
                            return html
            
            # ğŸ†• P0-4: í•˜ìœ„ ì„¹ì…˜ íƒ€ê²ŸíŒ… ê°œì„  (ìŠ¤ì½”ì–´ë§ ê¸°ë°˜ ì„ íƒ)
            # "ì˜ì—…ì˜ í˜„í™©" ë©”ì¸ ì„¹ì…˜ì—ì„œ í•˜ìœ„ ì„¹ì…˜ íƒìƒ‰í•˜ë©° íƒ€ê²Ÿ í‚¤ì›Œë“œê°€ ìˆëŠ” ì„¹ì…˜ë§Œ ìˆ˜ì§‘
            target_keywords = ['ì˜ì—…ì˜ í˜„í™©', 'ì˜ì—…ì˜ ì¢…ë¥˜', 'ì£¼ìš” ì œí’ˆ', 'ë§¤ì¶œ', 'ì‚¬ì—…ë¶€ë¬¸', 'ì˜ì—…ê°œí™©']
            
            # íƒ€ê²Ÿ ì„¹ì…˜ í‚¤ì›Œë“œ (ë¶€ë¬¸ë³„ ì˜ì—…ì‹¤ì  ë“±)
            if is_financial:
                target_section_keywords = ['ë¶€ë¬¸ë³„ ì˜ì—…ì‹¤ì ', 'ë¶€ë¬¸ë³„ ì˜ì—…ìˆ˜ì§€', 'ì‚¬ì—…ë¶€ë¬¸ë³„', 'ì˜ì—…ì˜ ì¢…ë¥˜']
            else:
                target_section_keywords = ['ë¶€ë¬¸ë³„ ì˜ì—…ì‹¤ì ', 'ë¶€ë¬¸ë³„ ë§¤ì¶œ', 'ì‚¬ì—…ë¶€ë¬¸ë³„', 'ë§¤ì¶œ ë¹„ì¤‘']
            
            # Step 1: "ì˜ì—…ì˜ í˜„í™©" ë©”ì¸ ì„¹ì…˜ ì°¾ê¸°
            main_section_idx = None
            for idx, row in sub_docs.iterrows():
                title = str(row.get('title', ''))
                if any(keyword in title for keyword in target_keywords):
                    main_section_idx = idx
                    logger.info(f"{ticker}: 'ì˜ì—…ì˜ í˜„í™©' ë©”ì¸ ì„¹ì…˜ ë°œê²¬: {title}")
                    break
            
            if main_section_idx is None:
                # ë©”ì¸ ì„¹ì…˜ ì—†ìœ¼ë©´ ì¼ë°˜ ê²€ìƒ‰
                for idx, row in sub_docs.iterrows():
                    title = str(row.get('title', ''))
                    url = row.get('url', '')
                    
                    if any(keyword in title for keyword in target_section_keywords):
                        html = self.fetch_section_content(url)
                        if html:
                            logger.info(f"{ticker}: íƒ€ê²Ÿ ì„¹ì…˜ ì§ì ‘ ë°œê²¬: {title}")
                            return html
                # ë©”ì¸ ì„¹ì…˜ë„ íƒ€ê²Ÿ ì„¹ì…˜ë„ ì—†ìœ¼ë©´ None ë°˜í™˜
                return None
            
            # Step 2: ë©”ì¸ ì„¹ì…˜ ë‹¤ìŒì˜ í•˜ìœ„ ì„¹ì…˜ë“¤ì„ íƒìƒ‰í•˜ë©° íƒ€ê²Ÿ í‚¤ì›Œë“œ ì°¾ê¸°
            target_sections = []
            
            # ë©”ì¸ ì„¹ì…˜ë„ í¬í•¨
            main_row = sub_docs.iloc[main_section_idx]
            main_url = main_row.get('url', '')
            if main_url:
                main_html = self.fetch_section_content(main_url)
                if main_html:
                    # ë©”ì¸ ì„¹ì…˜ HTMLì—ì„œ íƒ€ê²Ÿ í‚¤ì›Œë“œ í™•ì¸
                    if any(kw in main_html for kw in target_section_keywords):
                        target_sections.append((main_row.get('title', ''), main_html))
                        logger.info(f"{ticker}: ë©”ì¸ ì„¹ì…˜ì— íƒ€ê²Ÿ í‚¤ì›Œë“œ ë°œê²¬")
            
            # í•˜ìœ„ ì„¹ì…˜ íƒìƒ‰
            for idx in range(main_section_idx + 1, len(sub_docs)):
                row = sub_docs.iloc[idx]
                title = str(row.get('title', ''))
                url = row.get('url', '')
                
                # ë‹¤ìŒ í° ì„¹ì…˜ì´ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
                if any(major in title for major in ['ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨', 'ì¬ë¬´ìƒíƒœ', 'ì†ìµê³„ì‚°ì„œ', 'í˜„ê¸ˆíë¦„']):
                    logger.debug(f"{ticker}: ë‹¤ìŒ í° ì„¹ì…˜ ë°œê²¬, íƒìƒ‰ ì¤‘ë‹¨: {title}")
                    break
                
                # íƒ€ê²Ÿ í‚¤ì›Œë“œê°€ ìˆëŠ” í•˜ìœ„ ì„¹ì…˜ ì°¾ê¸°
                if any(keyword in title for keyword in target_section_keywords):
                    html = self.fetch_section_content(url)
                    if html:
                        target_sections.append((title, html))
                        logger.info(f"{ticker}: íƒ€ê²Ÿ í•˜ìœ„ ì„¹ì…˜ ë°œê²¬: {title}")
                        # íƒ€ê²Ÿ ì„¹ì…˜ì„ ì°¾ìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜ (ê°€ì¥ ì •í™•í•œ ì„¹ì…˜)
                        return html
                
                # í•˜ìœ„ ì„¹ì…˜ íŒ¨í„´ì´ì§€ë§Œ í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš°, HTML ë‚´ìš©ì—ì„œë„ í™•ì¸
                if re.search(r'[ê°€-ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜]\.|\([0-9]\)|\([ê°€-í£]\)', title):
                    html = self.fetch_section_content(url)
                    if html:
                        # HTML ë‚´ìš©ì—ì„œ íƒ€ê²Ÿ í‚¤ì›Œë“œ í™•ì¸
                        if any(keyword in html for keyword in target_section_keywords):
                            target_sections.append((title, html))
                            logger.info(f"{ticker}: HTML ë‚´ìš©ì—ì„œ íƒ€ê²Ÿ í‚¤ì›Œë“œ ë°œê²¬: {title}")
                            return html
            
            # Step 3: íƒ€ê²Ÿ ì„¹ì…˜ì„ ì°¾ì•˜ìœ¼ë©´ ë°˜í™˜, ì—†ìœ¼ë©´ ë©”ì¸ ì„¹ì…˜ ë°˜í™˜ (Fallback)
            if target_sections:
                # ê°€ì¥ ì²« ë²ˆì§¸ íƒ€ê²Ÿ ì„¹ì…˜ ë°˜í™˜
                return target_sections[0][1]
            
            # íƒ€ê²Ÿ ì„¹ì…˜ì„ ëª» ì°¾ìœ¼ë©´ ë©”ì¸ ì„¹ì…˜ ë°˜í™˜
            if main_url:
                main_html = self.fetch_section_content(main_url)
                if main_html:
                    logger.info(f"{ticker}: íƒ€ê²Ÿ ì„¹ì…˜ ë¯¸ë°œê²¬, ë©”ì¸ ì„¹ì…˜ ë°˜í™˜")
                    return main_html
            
            # ê´€ë ¨ ì„¹ì…˜ì„ ëª» ì°¾ìœ¼ë©´ "ì‚¬ì—…ì˜ ë‚´ìš©" ì„¹ì…˜ ì‚¬ìš©
            for idx, row in sub_docs.iterrows():
                title = str(row.get('title', ''))
                url = row.get('url', '')
                
                if 'ì‚¬ì—…ì˜ ë‚´ìš©' in title or 'ì‚¬ì—…ì˜ ê°œìš”' in title:
                    html = self.fetch_section_content(url)
                    if html:
                        logger.info(f"{ticker}: ì‚¬ì—…ì˜ ë‚´ìš© ì„¹ì…˜ HTML ì¶”ì¶œ ì„±ê³µ: {title}")
                        return html
            
            return None
        except Exception as e:
            logger.error(f"{ticker}: ì›ë¬¸ HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    @retry_dart_api
    @dart_rate_limiter
    def fetch_section_content(self, url: str, timeout: int = 30) -> Optional[str]:
        """
        ì„¹ì…˜ URLì—ì„œ HTML ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        
        SSL ì˜¤ë¥˜ ë° ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ì— ëŒ€í•œ ì¬ì‹œë„ ë¡œì§ í¬í•¨
        
        Args:
            url: ì„¹ì…˜ URL
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        
        Returns:
            HTML ë‚´ìš© ë˜ëŠ” None
        """
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        # Sessionì„ ì‚¬ìš©í•˜ì—¬ ì—°ê²° ì¬ì‚¬ìš© ë° ì¬ì‹œë„ ì„¤ì •
        session = requests.Session()
        
        # ì¬ì‹œë„ ì „ëµ ì„¤ì •
        retry_strategy = Retry(
            total=3,  # ì´ 3ë²ˆ ì¬ì‹œë„
            backoff_factor=1,  # 1ì´ˆ, 2ì´ˆ, 4ì´ˆ ê°„ê²©ìœ¼ë¡œ ì¬ì‹œë„
            status_forcelist=[429, 500, 502, 503, 504],  # HTTP ìƒíƒœ ì½”ë“œë³„ ì¬ì‹œë„
            allowed_methods=["GET", "POST"]  # GET, POSTë§Œ ì¬ì‹œë„
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        try:
            # SSL ê²€ì¦ì€ ìœ ì§€í•˜ë˜, ì—°ê²° ì¬ì‹œë„ ê°•í™”
            response = session.get(
                url, 
                timeout=timeout,
                verify=True,  # SSL ê²€ì¦ ìœ ì§€
                headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
            )
            response.raise_for_status()
            return response.text
        except requests.exceptions.SSLError as ssl_err:
            logger.warning(f"SSL ì˜¤ë¥˜ ë°œìƒ ({url[:50]}...): {ssl_err}")
            # SSL ì˜¤ë¥˜ ì‹œ í•œ ë²ˆ ë” ì‹œë„ (ê²€ì¦ ìš°íšŒ ì˜µì…˜)
            try:
                logger.info(f"SSL ê²€ì¦ ìš°íšŒí•˜ì—¬ ì¬ì‹œë„: {url[:50]}...")
                response = session.get(
                    url,
                    timeout=timeout,
                    verify=False,  # SSL ê²€ì¦ ìš°íšŒ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
                    headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
                )
                response.raise_for_status()
                logger.info(f"SSL ê²€ì¦ ìš°íšŒ í›„ ì„±ê³µ: {url[:50]}...")
                return response.text
            except Exception as e2:
                logger.error(f"SSL ê²€ì¦ ìš°íšŒ í›„ì—ë„ ì‹¤íŒ¨ ({url[:50]}...): {e2}")
                return None
        except Exception as e:
            logger.warning(f"ì„¹ì…˜ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({url[:50]}...): {e}")
            return None
        finally:
            session.close()
    
    def clean_html_to_markdown(self, html_content: str) -> str:
        """
        HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
        
        Args:
            html_content: HTML ë‚´ìš©
        
        Returns:
            ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
        """
        if not html_content:
            return ""
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'img', 'svg', 'path']):
            tag.decompose()
        
        cleaned_html = str(soup)
        text = md(cleaned_html, heading_style="ATX", strip=['a'], newline_style="BACKSLASH")
        
        # ì •ê·œí™”
        text = re.sub(r'\n\s+\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        text = re.sub(r'\n{3,}', '\n\n', text)  # ì—°ì†ëœ ë¹ˆ ì¤„ ì••ì¶•
        text = re.sub(r'\|\s*\|\s*\|', '|', text)  # í‘œì˜ ë¹ˆ ì…€ ì œê±°
        
        # ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ì œê±°
        text = self._filter_boilerplate_references(text)
        
        return text
    
    def _filter_boilerplate_references(self, text: str) -> str:
        """'~ì°¸ì¡° ë°”ëë‹ˆë‹¤' ë“± ë„¤ë¹„ê²Œì´ì…˜ ë¬¸ì¥ ì œê±°"""
        if not text:
            return ""
        
        nav_patterns = [
            r"ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤",
            r"ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤",
            r"ì°¸ì¡° ë°”ëë‹ˆë‹¤",
            r"ì°¸ì¡°ë°”ëë‹ˆë‹¤",
            r"ì°¸ê³  ë°”ëë‹ˆë‹¤",
            r"ì°¸ê³ ë°”ëë‹ˆë‹¤",
            r"ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤",
            r"í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤",
            r"ê¸°ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤",
            r"ê¸°ì¬ë˜ì–´ìˆìŠµë‹ˆë‹¤"
        ]
        value_keywords = [
            "í†µí™”ì„ ë„", "ìŠ¤ì™‘", "ìŠ¤ì™€í”„", "ì„ ë¬¼", "ì˜µì…˜", "íŒŒìƒ", "í—·ì§€", "í—¤ì§€",
            "ìœ„í—˜íšŒí”¼", "ë§¤ë§¤", "ê³„ì•½", "ì²´ê²°", "í‰ê°€", "ì†ìµ", "ì”ì•¡",
            "%", "ì›", "ë‹¬ëŸ¬", "ì–µì›", "ë°°ëŸ´", "í†¤"
        ]
        ref_patterns = [
            r"['\"ã€Œ].+['\"ã€]\s*(ì„|ë¥¼)?\s*ì°¸(ì¡°|ê³ )",
            r"ìƒì„¸ ë‚´ìš©ì€\s*['\"ã€Œ].+['\"ã€]"
        ]
        
        lines = text.split('\n')
        filtered_lines = []
        for line in lines:
            clean_line = line.strip()
            if len(clean_line) < 2:
                continue
            is_nav = any(re.search(pat, clean_line) for pat in nav_patterns)
            has_value = any(keyword in clean_line for keyword in value_keywords)
            ref_hit = any(re.search(pat, clean_line) for pat in ref_patterns)
            if (is_nav or ref_hit):
                if has_value:
                    ref_positions = [idx for idx in (clean_line.find("ì°¸ê³ "), clean_line.find("ì°¸ì¡°")) if idx != -1]
                    if ref_positions:
                        cut_idx = min(ref_positions)
                        trimmed = clean_line[:cut_idx].rstrip(" ,.-")
                        if trimmed:
                            filtered_lines.append(trimmed)
                        continue
                else:
                    continue
            filtered_lines.append(line)
        
        cleaned = '\n'.join(filtered_lines).strip()
        return cleaned or ""
    
    def extract_key_sections(
        self,
        ticker: str,
        target_year: Optional[int] = None
    ) -> Optional[str]:
        """
        DARTì—ì„œ í•µì‹¬ ì„¹ì…˜ ì¶”ì¶œ (ì‚¬ì—…ì˜ ë‚´ìš© + ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨)
        
        3ë‹¨ê³„ Fallback ì „ëµ:
        - Strategy A: ìƒìœ„ ëª©ì°¨ + URL ìˆìŒ â†’ ë°”ë¡œ ì‚¬ìš©
        - Strategy B: ìƒìœ„ ëª©ì°¨ ìˆì§€ë§Œ URL ì—†ìŒ â†’ í•˜ìœ„ ëª©ì°¨ í†µí•© ìˆ˜ì§‘
        - Strategy C: ìƒìœ„ ëª©ì°¨ ì—†ìŒ â†’ í‚¤ì›Œë“œ ê¸°ë°˜ ì§ì ‘ ìˆ˜ì§‘
        
        Args:
            ticker: ì¢…ëª©ì½”ë“œ
            target_year: ëŒ€ìƒ ì—°ë„
        
        Returns:
            ê²°í•©ëœ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        # ì‚¬ì—…ë³´ê³ ì„œ ì°¾ê¸°
        report_info = self.find_business_report(ticker, target_year)
        if report_info is None:
            # ë³´ê³ ì„œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œë¡œ í´ë°±
            logger.warning(f"{ticker}: ì‚¬ì—…ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œë¡œ í´ë°± ì‹œë„...")
            return self._fallback_to_quarterly_or_semi_annual(ticker, target_year)
        
        rcept_no = report_info['rcept_no']
        report_title = report_info['report_nm']
        logger.info(f"{ticker}: ëŒ€ìƒ ë³´ê³ ì„œ - {report_title} (No: {rcept_no})")
        
        # í•˜ìœ„ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
        sub_docs = self.get_sub_docs(rcept_no)
        if sub_docs is None or len(sub_docs) == 0:
            logger.warning(f"{ticker}: í•˜ìœ„ ë¬¸ì„œ ì—†ìŒ. ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œë¡œ í´ë°± ì‹œë„...")
            return self._fallback_to_quarterly_or_semi_annual(ticker, target_year)
        
        if 'title' not in sub_docs.columns or 'url' not in sub_docs.columns:
            logger.error(f"{ticker}: sub_docsì— 'title' ë˜ëŠ” 'url' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œë¡œ í´ë°± ì‹œë„...")
            return self._fallback_to_quarterly_or_semi_annual(ticker, target_year)
        
        # íŠ¹ìˆ˜ ë³´ê³ ì„œ í˜•ì‹ ê°ì§€ ë° ì²˜ë¦¬ ì‹œë„
        if self._is_special_report_format(sub_docs):
            logger.info(f"{ticker}: íŠ¹ìˆ˜ ë³´ê³ ì„œ í˜•ì‹ ê°ì§€. íŠ¹ìˆ˜ í˜•ì‹ì—ì„œ ì¶”ì¶œ ì‹œë„...")
            special_result = self._try_extract_from_special_format(sub_docs, ticker)
            if special_result:
                return special_result
            # íŠ¹ìˆ˜ í˜•ì‹ì—ì„œ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ë¶„ê¸°/ë°˜ê¸° í´ë°±
            logger.info(f"{ticker}: íŠ¹ìˆ˜ í˜•ì‹ì—ì„œ ì¶”ì¶œ ì‹¤íŒ¨. ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œë¡œ í´ë°± ì‹œë„...")
            return self._fallback_to_quarterly_or_semi_annual(ticker, target_year)
        
        combined_text = ""
        found_count = 0
        business_section_texts = []  # í•˜ìœ„ ì„¹ì…˜ ìˆ˜ì§‘ìš©
        business_section_idx = None
        business_section_has_url = False
        
        # Strategy A & B: ìƒìœ„ ëª©ì°¨ "ì‚¬ì—…ì˜ ë‚´ìš©" ì°¾ê¸°
        for idx, row in sub_docs.iterrows():
            title_str = str(row['title']).strip()
            clean_title = title_str.replace(" ", "").replace(".", "").strip()
            
            # "ì‚¬ì—…ì˜ ë‚´ìš©" ìƒìœ„ ì„¹ì…˜ ë§¤ì¹­
            if self._matches_business_section(clean_title, title_str):
                business_section_idx = idx
                url = row.get('url')
                
                # Strategy A: URLì´ ìˆê³  ë‚´ìš©ì´ ì¶©ë¶„í•œ ê²½ìš°
                if pd.notna(url) and url and len(str(url)) > 5:
                    html = self.fetch_section_content(url)
                    if html:
                        md_text = self.clean_html_to_markdown(html)
                        md_text = self._extract_business_subsections(md_text)
                        
                        # ë‚´ìš©ì´ ì¶©ë¶„í•œ ê²½ìš°ë§Œ ì„±ê³µìœ¼ë¡œ ê°„ì£¼ (ìµœì†Œ 500ì)
                        md_length = len(md_text.strip())
                        logger.debug(f"{ticker}: 'ì‚¬ì—…ì˜ ë‚´ìš©' ì¶”ì¶œ ê²°ê³¼: {md_length}ì")
                        if md_length > 500:
                            combined_text += f"# 1. ì‚¬ì—…ì˜ ë‚´ìš©\n{md_text}\n\n"
                            found_count += 1
                            logger.info(f"{ticker}: 'ì‚¬ì—…ì˜ ë‚´ìš©' ì¶”ì¶œ ì„±ê³µ (ìƒìœ„ ì„¹ì…˜, {md_length}ì)")
                            break  # Strategy A ì„±ê³µ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
                        else:
                            logger.warning(f"{ticker}: 'ì‚¬ì—…ì˜ ë‚´ìš©' ìƒìœ„ ì„¹ì…˜ URL ìˆìœ¼ë‚˜ ë‚´ìš© ë¶€ì¡± ({md_length}ì). í•˜ìœ„ ì„¹ì…˜ íƒìƒ‰...")
                            business_section_has_url = True
                else:
                    # Strategy B: URLì´ ì—†ëŠ” ê²½ìš°
                    logger.info(f"{ticker}: 'ì‚¬ì—…ì˜ ë‚´ìš©' ìƒìœ„ ì„¹ì…˜ ë°œê²¬í–ˆìœ¼ë‚˜ URL ì—†ìŒ. í•˜ìœ„ ì„¹ì…˜ íƒìƒ‰...")
                    business_section_has_url = False
                break
        
        # Strategy B: ìƒìœ„ ëª©ì°¨ëŠ” ìˆì§€ë§Œ URLì´ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ë¶€ì¡±í•œ ê²½ìš° â†’ í•˜ìœ„ ëª©ì°¨ ìˆ˜ì§‘
        if business_section_idx is not None and found_count == 0:
            logger.info(f"{ticker}: í•˜ìœ„ ì„¹ì…˜ ìˆ˜ì§‘ ì‹œì‘ (ì¸ë±ìŠ¤ {business_section_idx}ë¶€í„°)")
            
            # í˜„ì¬ ì¸ë±ìŠ¤ ë‹¤ìŒë¶€í„° íƒìƒ‰ ì‹œì‘
            for sub_idx in range(business_section_idx + 1, len(sub_docs)):
                sub_row = sub_docs.iloc[sub_idx]
                sub_title = str(sub_row['title']).strip()
                sub_clean_title = sub_title.replace(" ", "").replace(".", "").strip()
                sub_url = sub_row.get('url')
                
                # ë‹¤ìŒ í° ì„¹ì…˜ì´ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
                if self._is_next_major_section(sub_title, sub_clean_title):
                    logger.info(f"{ticker}: ë‹¤ìŒ ëŒ€ì„¹ì…˜ '{sub_title}' ë°œê²¬. í•˜ìœ„ ì„¹ì…˜ ìˆ˜ì§‘ ì¢…ë£Œ")
                    break
                
                # í•˜ìœ„ ì„¹ì…˜ ìµœëŒ€ 7ê°œ ì œí•œ (ì„±ëŠ¥ ê³ ë ¤)
                if len(business_section_texts) >= 7:
                    logger.warning(f"{ticker}: í•˜ìœ„ ì„¹ì…˜ ìˆ˜ì§‘ ì œí•œ ë„ë‹¬ (7ê°œ). ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
                
                # í•˜ìœ„ ì„¹ì…˜ íŒ¨í„´ í™•ì¸
                if self._is_business_subsection(sub_clean_title, sub_title):
                    if pd.notna(sub_url) and sub_url and len(str(sub_url)) > 5:
                        html = self.fetch_section_content(sub_url)
                        if html:
                            md_text = self.clean_html_to_markdown(html)
                            md_text = self._extract_structured_content(md_text, html)
                            
                            if len(md_text.strip()) > 100:  # ìµœì†Œ 100ì ì´ìƒ
                                business_section_texts.append({
                                    'title': sub_title,
                                    'content': md_text
                                })
                                logger.info(f"{ticker}: í•˜ìœ„ ì„¹ì…˜ ìˆ˜ì§‘ - {sub_title} ({len(md_text)}ì)")
            
            # í•˜ìœ„ ì„¹ì…˜ë“¤ì„ í†µí•©
            if len(business_section_texts) > 0:
                combined_business = "# 1. ì‚¬ì—…ì˜ ë‚´ìš©\n\n"
                for section in business_section_texts:
                    combined_business += f"## {section['title']}\n{section['content']}\n\n"
                combined_text = combined_business + combined_text
                found_count += 1
                logger.info(f"{ticker}: Strategy B - í•˜ìœ„ ì„¹ì…˜ {len(business_section_texts)}ê°œ í†µí•© ì™„ë£Œ (ì´ {len(combined_text)}ì)")
        
        # Strategy C: ìƒìœ„ ëª©ì°¨ì¡°ì°¨ ì—†ëŠ” ê²½ìš° â†’ í‚¤ì›Œë“œ ê¸°ë°˜ ì§ì ‘ ìˆ˜ì§‘
        if found_count == 0:
            logger.warning(f"{ticker}: 'ì‚¬ì—…ì˜ ë‚´ìš©' ìƒìœ„ ëª©ì°¨ ì—†ìŒ. í‚¤ì›Œë“œ ê¸°ë°˜ í•˜ìœ„ ì„¹ì…˜ ì§ì ‘ ìˆ˜ì§‘...")
            
            target_keywords = [
                "ì‚¬ì—…ì˜ ê°œìš”", "ì£¼ìš” ì œí’ˆ", "ì£¼ìš” ì„œë¹„ìŠ¤", "ì›ì¬ë£Œ",
                "ìƒì‚°", "ë§¤ì¶œ", "ìˆ˜ì£¼", "íŒë§¤", "ê³ ê°", "ìœ„í—˜ê´€ë¦¬"
            ]
            
            for idx, row in sub_docs.iterrows():
                title_str = str(row['title']).strip()
                url = row.get('url')
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ (í•˜ì§€ë§Œ ì¬ë¬´ ê´€ë ¨ì€ ì œì™¸)
                clean_title = title_str.replace(" ", "").replace(".", "").strip()
                if 'ì¬ë¬´' in clean_title and 'ì‚¬ì—…' not in clean_title:
                    continue  # ì¬ë¬´ ì„¹ì…˜ì€ ì œì™¸
                
                if any(kw in title_str for kw in target_keywords):
                    if pd.notna(url) and url and len(str(url)) > 5:
                        html = self.fetch_section_content(url)
                        if html:
                            md_text = self.clean_html_to_markdown(html)
                            md_text = self._extract_structured_content(md_text, html)
                            
                            if len(md_text.strip()) > 100:
                                business_section_texts.append({
                                    'title': title_str,
                                    'content': md_text
                                })
                                logger.info(f"{ticker}: í‚¤ì›Œë“œ ë§¤ì¹­ ì„¹ì…˜ ìˆ˜ì§‘ - {title_str} ({len(md_text)}ì)")
                                
                                # ìµœëŒ€ 7ê°œ ì œí•œ
                                if len(business_section_texts) >= 7:
                                    break
            
            if len(business_section_texts) > 0:
                combined_business = "# 1. ì‚¬ì—…ì˜ ë‚´ìš©\n\n"
                for section in business_section_texts:
                    combined_business += f"## {section['title']}\n{section['content']}\n\n"
                combined_text = combined_business + combined_text
                found_count += 1
                logger.info(f"{ticker}: Strategy C - í‚¤ì›Œë“œ ê¸°ë°˜ ì„¹ì…˜ {len(business_section_texts)}ê°œ í†µí•© ì™„ë£Œ (ì´ {len(combined_text)}ì)")
        
        # ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨ ì²˜ë¦¬
        for idx, row in sub_docs.iterrows():
            title_str = str(row['title']).strip()
            clean_title = title_str.replace(" ", "").replace(".", "").strip()
            
            if 'ì´ì‚¬ì˜ê²½ì˜ì§„ë‹¨' in clean_title or 'ê²½ì˜ì§„ë‹¨' in clean_title or 'ë¶„ì„ì˜ê²¬' in clean_title:
                if pd.notna(row['url']) and row['url']:
                    html = self.fetch_section_content(row['url'])
                    if html:
                        md_text = self.clean_html_to_markdown(html)
                        md_text = self._extract_mda_subsections(md_text)
                        md_length = len(md_text.strip())
                        combined_text += f"# 2. ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨\n{md_text}\n\n"
                        found_count += 1
                        combined_length = len(combined_text.strip())
                        logger.info(f"{ticker}: 'ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨' ì¶”ì¶œ ì„±ê³µ ({md_length}ì, ì „ì²´: {combined_length}ì)")
        
        # ìµœì¢… ì‹¤íŒ¨ ì²˜ë¦¬
        if found_count == 0:
            logger.warning(f"{ticker}: íƒ€ê²Ÿ ëª©ì°¨ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            # ë””ë²„ê¹…: ì „ì²´ ì„¹ì…˜ ëª©ë¡ì„ WARNING ë ˆë²¨ë¡œ ì¶œë ¥
            if 'title' in sub_docs.columns:
                all_titles = sub_docs['title'].tolist()
                logger.warning(f"ì „ì²´ ì„¹ì…˜ ëª©ë¡ ({len(all_titles)}ê°œ):")
                for i, title in enumerate(all_titles[:30], 1):
                    url_exists = pd.notna(sub_docs.iloc[i-1].get('url')) and sub_docs.iloc[i-1].get('url')
                    url_info = "âœ“" if url_exists else "âœ—"
                    logger.warning(f"  {i}. [{url_info}] {title}")
                if len(all_titles) > 30:
                    logger.warning(f"  ... (ì´ {len(all_titles)}ê°œ, ì²˜ìŒ 30ê°œë§Œ í‘œì‹œ)")
            
            # ì‚¬ì—…ë³´ê³ ì„œë¥¼ ì°¾ì•˜ì§€ë§Œ ëª©ì°¨ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œë¡œ ëŒ€ì²´ ì‹œë„
            logger.info(f"{ticker}: ì‚¬ì—…ë³´ê³ ì„œ ëª©ì°¨ ë¯¸ë°œê²¬. ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œë¡œ ëŒ€ì²´ ì‹œë„...")
            return self._fallback_to_quarterly_or_semi_annual(ticker, target_year)
        
        # ìµœì¢… ê²€ì¦: combined_textê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ í´ë°± ì‹œë„
        combined_length = len(combined_text.strip()) if combined_text else 0
        logger.info(f"{ticker}: ìµœì¢… combined_text ê¸¸ì´: {combined_length}ì")
        
        if combined_length < 100:
            logger.warning(f"{ticker}: combined_textê°€ ë„ˆë¬´ ì§§ìŒ ({combined_length}ì). ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œë¡œ í´ë°± ì‹œë„...")
            return self._fallback_to_quarterly_or_semi_annual(ticker, target_year)
        
        # combined_text ë‚´ìš© ìš”ì•½ ë¡œê·¸ (ì²˜ìŒ 200ì)
        preview = combined_text[:200].replace('\n', ' ') if combined_text else ""
        logger.info(f"{ticker}: combined_text ë¯¸ë¦¬ë³´ê¸°: {preview}...")
        
        return combined_text
    
    def _fallback_to_quarterly_or_semi_annual(
        self,
        ticker: str,
        target_year: int = 2024
    ) -> Optional[str]:
        """
        ì‚¬ì—…ë³´ê³ ì„œ ëª©ì°¨ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œ ì¤‘ ìµœì‹  ê²ƒì„ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            ticker: ì¢…ëª©ì½”ë“œ
            target_year: ëŒ€ìƒ ì—°ë„
        
        Returns:
            ê²°í•©ëœ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ (ì‚¬ì—…ì˜ ë‚´ìš©ë§Œ, ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨ ì œì™¸) ë˜ëŠ” None
        """
        from datetime import datetime
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        current_year = datetime.now().year
        if target_year:
            start_date = f'{target_year}-01-01'
        else:
            start_date = f'{current_year - 3}-01-01'
        
        end_date = datetime.now().strftime('%Y-%m-%d')
        
        # ì •ê¸° ê³µì‹œ ì „ì²´ ì¡°íšŒ (A002: ë°˜ê¸°ë³´ê³ ì„œ, A003: ë¶„ê¸°ë³´ê³ ì„œ)
        all_regular = self.list_reports(ticker, start_date, end_date, kind='A', final=True, business_report_only=False)
        if all_regular is None or len(all_regular) == 0:
            logger.warning(f"{ticker}: ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œë§Œ í•„í„°ë§ (A002: ë°˜ê¸°ë³´ê³ ì„œ, A003: ë¶„ê¸°ë³´ê³ ì„œ)
        quarterly_types = ['A002', 'A003']
        if 'pblntf_detail_ty' in all_regular.columns:
            filtered_reports = all_regular[all_regular['pblntf_detail_ty'].isin(quarterly_types)]
            if len(filtered_reports) == 0:
                logger.warning(f"{ticker}: ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
        else:
            logger.warning(f"{ticker}: pblntf_detail_ty ì»¬ëŸ¼ ì—†ìŒ. ì „ì²´ ë³´ê³ ì„œ ì‚¬ìš©")
            filtered_reports = all_regular
        
        # ì ‘ìˆ˜ì¼ì ê¸°ì¤€ ì •ë ¬ (ìµœì‹ ìˆœ)
        if 'rcept_dt' in filtered_reports.columns:
            filtered_reports = filtered_reports.sort_values('rcept_dt', ascending=False)
        
        # ìµœì‹  ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œ ì„ íƒ
        latest_report = filtered_reports.iloc[0]
        rcept_no = latest_report['rcept_no']
        report_title = latest_report['report_nm']
        report_type = latest_report.get('pblntf_detail_ty', 'UNKNOWN')
        
        logger.info(f"{ticker}: ìµœì‹  ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œ ë°œê²¬ - {report_title} (No: {rcept_no}, Type: {report_type})")
        
        # í•˜ìœ„ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
        sub_docs = self.get_sub_docs(rcept_no)
        if sub_docs is None or len(sub_docs) == 0:
            logger.warning(f"{ticker}: í•˜ìœ„ ë¬¸ì„œ ì—†ìŒ")
            return None
        
        if 'title' not in sub_docs.columns or 'url' not in sub_docs.columns:
            logger.error(f"{ticker}: sub_docsì— 'title' ë˜ëŠ” 'url' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        combined_text = ""
        found_count = 0
        business_section_texts = []  # í•˜ìœ„ ì„¹ì…˜ ìˆ˜ì§‘ìš©
        business_section_idx = None
        
        # Strategy A & B: ìƒìœ„ ëª©ì°¨ "ì‚¬ì—…ì˜ ë‚´ìš©" ì°¾ê¸°
        for idx, row in sub_docs.iterrows():
            title_str = str(row['title']).strip()
            clean_title = title_str.replace(" ", "").replace(".", "").strip()
            
            # "ì‚¬ì—…ì˜ ë‚´ìš©" ìƒìœ„ ì„¹ì…˜ ë§¤ì¹­
            if self._matches_business_section(clean_title, title_str):
                business_section_idx = idx
                url = row.get('url')
                
                # Strategy A: URLì´ ìˆê³  ë‚´ìš©ì´ ì¶©ë¶„í•œ ê²½ìš°
                if pd.notna(url) and url and len(str(url)) > 5:
                    html = self.fetch_section_content(url)
                    if html:
                        md_text = self.clean_html_to_markdown(html)
                        md_text = self._extract_business_subsections(md_text)
                        
                        # ë‚´ìš©ì´ ì¶©ë¶„í•œ ê²½ìš°ë§Œ ì„±ê³µìœ¼ë¡œ ê°„ì£¼ (ìµœì†Œ 500ì)
                        if len(md_text.strip()) > 500:
                            combined_text += f"# 1. ì‚¬ì—…ì˜ ë‚´ìš©\n{md_text}\n\n"
                            found_count += 1
                            logger.info(f"{ticker}: 'ì‚¬ì—…ì˜ ë‚´ìš©' ì¶”ì¶œ ì„±ê³µ (ìƒìœ„ ì„¹ì…˜, {len(md_text)}ì)")
                            break
                        else:
                            logger.warning(f"{ticker}: 'ì‚¬ì—…ì˜ ë‚´ìš©' ìƒìœ„ ì„¹ì…˜ URL ìˆìœ¼ë‚˜ ë‚´ìš© ë¶€ì¡± ({len(md_text)}ì). í•˜ìœ„ ì„¹ì…˜ íƒìƒ‰...")
                else:
                    # Strategy B: URLì´ ì—†ëŠ” ê²½ìš°
                    logger.info(f"{ticker}: 'ì‚¬ì—…ì˜ ë‚´ìš©' ìƒìœ„ ì„¹ì…˜ ë°œê²¬í–ˆìœ¼ë‚˜ URL ì—†ìŒ. í•˜ìœ„ ì„¹ì…˜ íƒìƒ‰...")
                break
        
        # Strategy B: í•˜ìœ„ ëª©ì°¨ ìˆ˜ì§‘
        if business_section_idx is not None and found_count == 0:
            logger.info(f"{ticker}: í•˜ìœ„ ì„¹ì…˜ ìˆ˜ì§‘ ì‹œì‘ (ì¸ë±ìŠ¤ {business_section_idx}ë¶€í„°)")
            
            for sub_idx in range(business_section_idx + 1, len(sub_docs)):
                sub_row = sub_docs.iloc[sub_idx]
                sub_title = str(sub_row['title']).strip()
                sub_clean_title = sub_title.replace(" ", "").replace(".", "").strip()
                sub_url = sub_row.get('url')
                
                # ë‹¤ìŒ í° ì„¹ì…˜ì´ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
                if self._is_next_major_section(sub_title, sub_clean_title):
                    logger.info(f"{ticker}: ë‹¤ìŒ ëŒ€ì„¹ì…˜ '{sub_title}' ë°œê²¬. í•˜ìœ„ ì„¹ì…˜ ìˆ˜ì§‘ ì¢…ë£Œ")
                    break
                
                # í•˜ìœ„ ì„¹ì…˜ ìµœëŒ€ 7ê°œ ì œí•œ
                if len(business_section_texts) >= 7:
                    break
                
                # í•˜ìœ„ ì„¹ì…˜ íŒ¨í„´ í™•ì¸
                if self._is_business_subsection(sub_clean_title, sub_title):
                    if pd.notna(sub_url) and sub_url and len(str(sub_url)) > 5:
                        html = self.fetch_section_content(sub_url)
                        if html:
                            md_text = self.clean_html_to_markdown(html)
                            md_text = self._extract_structured_content(md_text, html)
                            
                            if len(md_text.strip()) > 100:
                                business_section_texts.append({
                                    'title': sub_title,
                                    'content': md_text
                                })
                                logger.info(f"{ticker}: í•˜ìœ„ ì„¹ì…˜ ìˆ˜ì§‘ - {sub_title} ({len(md_text)}ì)")
            
            # í•˜ìœ„ ì„¹ì…˜ë“¤ì„ í†µí•©
            if len(business_section_texts) > 0:
                logger.info(f"{ticker}: í•˜ìœ„ ì„¹ì…˜ {len(business_section_texts)}ê°œ í†µí•©")
                combined_business = "# 1. ì‚¬ì—…ì˜ ë‚´ìš©\n\n"
                for section in business_section_texts:
                    combined_business += f"## {section['title']}\n{section['content']}\n\n"
                combined_text = combined_business + combined_text
                found_count += 1
        
        # Strategy C: í‚¤ì›Œë“œ ê¸°ë°˜ ì§ì ‘ ìˆ˜ì§‘
        if found_count == 0:
            logger.warning(f"{ticker}: 'ì‚¬ì—…ì˜ ë‚´ìš©' ìƒìœ„ ëª©ì°¨ ì—†ìŒ. í‚¤ì›Œë“œ ê¸°ë°˜ í•˜ìœ„ ì„¹ì…˜ ì§ì ‘ ìˆ˜ì§‘...")
            
            target_keywords = [
                "ì‚¬ì—…ì˜ ê°œìš”", "ì£¼ìš” ì œí’ˆ", "ì£¼ìš” ì„œë¹„ìŠ¤", "ì›ì¬ë£Œ",
                "ìƒì‚°", "ë§¤ì¶œ", "ìˆ˜ì£¼", "íŒë§¤", "ê³ ê°"
            ]
            
            for idx, row in sub_docs.iterrows():
                title_str = str(row['title']).strip()
                url = row.get('url')
                clean_title = title_str.replace(" ", "").replace(".", "").strip()
                
                # ì¬ë¬´ ê´€ë ¨ì€ ì œì™¸
                if 'ì¬ë¬´' in clean_title and 'ì‚¬ì—…' not in clean_title:
                    continue
                
                if any(kw in title_str for kw in target_keywords):
                    if pd.notna(url) and url and len(str(url)) > 5:
                        html = self.fetch_section_content(url)
                        if html:
                            md_text = self.clean_html_to_markdown(html)
                            md_text = self._extract_structured_content(md_text, html)
                            
                            if len(md_text.strip()) > 100:
                                business_section_texts.append({
                                    'title': title_str,
                                    'content': md_text
                                })
                                logger.info(f"{ticker}: í‚¤ì›Œë“œ ë§¤ì¹­ ì„¹ì…˜ ìˆ˜ì§‘ - {title_str} ({len(md_text)}ì)")
                                
                                if len(business_section_texts) >= 7:
                                    break
            
            if len(business_section_texts) > 0:
                combined_business = "# 1. ì‚¬ì—…ì˜ ë‚´ìš©\n\n"
                for section in business_section_texts:
                    combined_business += f"## {section['title']}\n{section['content']}\n\n"
                combined_text = combined_business + combined_text
                found_count += 1
        
        if found_count == 0:
            logger.warning(f"{ticker}: ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œì—ì„œë„ 'ì‚¬ì—…ì˜ ë‚´ìš©' ëª©ì°¨ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
            if 'title' in sub_docs.columns:
                all_titles = sub_docs['title'].tolist()
                logger.warning(f"ì „ì²´ ì„¹ì…˜ ëª©ë¡ ({len(all_titles)}ê°œ):")
                for i, title in enumerate(all_titles[:20], 1):
                    url_exists = pd.notna(sub_docs.iloc[i-1].get('url')) and sub_docs.iloc[i-1].get('url')
                    url_info = "âœ“" if url_exists else "âœ—"
                    logger.warning(f"  {i}. [{url_info}] {title}")
            return None
        
        logger.info(f"{ticker}: ë¶„ê¸°/ë°˜ê¸° ë³´ê³ ì„œì—ì„œ 'ì‚¬ì—…ì˜ ë‚´ìš©' ì¶”ì¶œ ì™„ë£Œ (ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨ ì œì™¸)")
        return combined_text
    
    def _matches_business_section(self, clean_title: str, original_title: str) -> bool:
        """
        'ì‚¬ì—…ì˜ ë‚´ìš©' ì„¹ì…˜ì¸ì§€ íŒë‹¨ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
        
        ì§€ì› í˜•ì‹:
        - "II. ì‚¬ì—…ì˜ ë‚´ìš©" (ë¡œë§ˆìˆ«ì)
        - "2. ì‚¬ì—…ì˜ ë‚´ìš©" (ì•„ë¼ë¹„ì•„ìˆ«ì)
        - "IIì‚¬ì—…ì˜ë‚´ìš©" (ì /ê³µë°± ì—†ìŒ)
        - "ì‚¬ì—…ì˜ ë‚´ìš©" (ìˆ«ì ì—†ìŒ)
        """
        # ê¸°ë³¸ íŒ¨í„´
        if 'ì‚¬ì—…ì˜ë‚´ìš©' in clean_title or 'ì‚¬ì—…ë‚´ìš©' in clean_title:
            return True
        
        # ë¡œë§ˆìˆ«ì + ì‚¬ì—…ì˜ ë‚´ìš© íŒ¨í„´
        if re.search(r'[IVX]+.*?ì‚¬ì—….*?ë‚´ìš©', clean_title):
            return True
        
        # ì•„ë¼ë¹„ì•„ìˆ«ì + ì‚¬ì—…ì˜ ë‚´ìš© íŒ¨í„´
        if re.search(r'^\d+.*?ì‚¬ì—….*?ë‚´ìš©', clean_title):
            return True
        
        # ì›ë³¸ ì œëª©ì—ì„œ ì§ì ‘ í™•ì¸ (ì , ê³µë°± í¬í•¨)
        if re.search(r'[IVX\d]*\s*\.?\s*ì‚¬ì—…\s*ì˜\s*ë‚´ìš©', original_title):
            return True
        
        return False
    
    def _is_next_major_section(self, title: str, clean_title: str) -> bool:
        """
        ë‹¤ìŒ ëŒ€ì„¹ì…˜ì¸ì§€ íŒë‹¨ (ì‚¬ì—… ê´€ë ¨ ì œì™¸)
        
        í•µì‹¬ í‚¤ì›Œë“œ ê¸°ë°˜ ì¢…ë£Œ ì¡°ê±´:
        - "ì¬ë¬´" í‚¤ì›Œë“œê°€ ìˆê³  "ì‚¬ì—…" í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        - ë¡œë§ˆìˆ«ì/ì•„ë¼ë¹„ì•„ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ëŒ€ì„¹ì…˜
        """
        # ë¡œë§ˆìˆ«ìë¡œ ì‹œì‘
        if re.match(r'^[IVX]+\.', title):
            # "ì‚¬ì—…" í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì„¹ì…˜ìœ¼ë¡œ ê°„ì£¼
            if 'ì‚¬ì—…' not in clean_title:
                return True
        
        # ì•„ë¼ë¹„ì•„ìˆ«ìë¡œ ì‹œì‘ (3 ì´ìƒì€ ë³´í†µ ë‹¤ë¥¸ ëŒ€ì„¹ì…˜)
        if re.match(r'^[3-9]\.', title):
            if 'ì‚¬ì—…' not in clean_title:
                return True
        
        # í•µì‹¬ ì¢…ë£Œ í‚¤ì›Œë“œ: "ì¬ë¬´"
        if 'ì¬ë¬´' in clean_title and 'ì‚¬ì—…' not in clean_title:
            return True
        
        return False
    
    def _is_business_subsection(self, clean_title: str, original_title: str) -> bool:
        """
        'ì‚¬ì—…ì˜ ë‚´ìš©'ì˜ í•˜ìœ„ ì„¹ì…˜ì¸ì§€ íŒë‹¨
        
        í•˜ìœ„ ì„¹ì…˜ íŒ¨í„´:
        - ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” í•˜ìœ„ ì„¹ì…˜
        - ì‚¬ì—… ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨
        - ì¬ë¬´ ê´€ë ¨ í‚¤ì›Œë“œëŠ” ì œì™¸
        """
        # ë¡œë§ˆìˆ«ì ë˜ëŠ” í° ì•„ë¼ë¹„ì•„ìˆ«ìë¡œ ì‹œì‘í•˜ë©´ ì œì™¸ (ëŒ€ì„¹ì…˜)
        if re.match(r'^[IVX]+\.|^[3-9]\.', original_title):
            return False
        
        # ì¬ë¬´ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì œì™¸
        if 'ì¬ë¬´' in clean_title:
            return False
        
        # í•˜ìœ„ ì„¹ì…˜ íŒ¨í„´ í™•ì¸
        subsection_patterns = [
            r'^\d+.*?ì‚¬ì—….*?ê°œìš”',      # "1. ì‚¬ì—…ì˜ ê°œìš”"
            r'^\d+.*?ì£¼ìš”.*?ì œí’ˆ',      # "2. ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤"
            r'^\d+.*?ì£¼ìš”.*?ì„œë¹„ìŠ¤',    # "2. ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤"
            r'^\d+.*?ì›ì¬ë£Œ',           # "3. ì›ì¬ë£Œ ë° ìƒì‚°ì‹¤ë¹„"
            r'^\d+.*?ìƒì‚°',             # "3. ì›ì¬ë£Œ ë° ìƒì‚°ì‹¤ë¹„"
            r'^\d+.*?ë§¤ì¶œ',             # "4. ë§¤ì¶œ ë° ìˆ˜ì£¼ìƒí™©"
            r'^\d+.*?ìˆ˜ì£¼',             # "4. ë§¤ì¶œ ë° ìˆ˜ì£¼ìƒí™©"
            r'^\d+.*?íŒë§¤',             # íŒë§¤ ê´€ë ¨
            r'^\d+.*?ê³ ê°',             # ê³ ê° ê´€ë ¨
            r'^\d+.*?ìœ„í—˜ê´€ë¦¬',         # ìœ„í—˜ê´€ë¦¬
            r'^\d+.*?ì—°êµ¬ê°œë°œ',         # ì—°êµ¬ê°œë°œ
        ]
        
        for pattern in subsection_patterns:
            if re.search(pattern, clean_title):
                return True
        
        return False
    
    def _extract_structured_content(self, markdown_text: str, html_content: str) -> str:
        """
        í‘œ í˜•íƒœ ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ
        
        í‘œê°€ ì£¼ìš” ë‚´ìš©ì¸ ê²½ìš°, í‘œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ì¶œ
        ìµœëŒ€ 20í–‰ë§Œ ì¶”ì¶œí•˜ì—¬ í† í° ì ˆì•½
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        tables = soup.find_all('table')
        
        if len(tables) == 0:
            # í‘œê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ ë°˜í™˜
            return markdown_text
        
        table_texts = []
        for table in tables:
            # í‘œ í—¤ë” ì¶”ì¶œ
            headers = []
            header_row = table.find('tr')
            if header_row:
                for th in header_row.find_all(['th', 'td']):
                    headers.append(th.get_text(strip=True))
            
            # í‘œ ë°ì´í„° ì¶”ì¶œ
            rows = []
            for tr in table.find_all('tr')[1:]:  # í—¤ë” ì œì™¸
                cells = [td.get_text(strip=True) for td in tr.find_all('td')]
                if cells and any(cell for cell in cells):  # ë¹ˆ í–‰ ì œì™¸
                    rows.append(cells)
            
            # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ìµœëŒ€ 20í–‰)
            if headers and rows:
                table_text = f"í‘œ í—¤ë”: {', '.join(headers)}\n"
                for i, row in enumerate(rows[:20], 1):
                    row_text = ', '.join([cell for cell in row if cell])
                    if row_text:
                        table_text += f"í–‰ {i}: {row_text}\n"
                table_texts.append(table_text)
        
        # í‘œ ë‚´ìš©ì´ ìˆìœ¼ë©´ í‘œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€
        if table_texts:
            table_summary = "\n\n".join(table_texts)
            # ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì™€ í‘œ í…ìŠ¤íŠ¸ ê²°í•©
            return f"{markdown_text}\n\n### í‘œ ë°ì´í„°\n{table_summary}"
        
        return markdown_text
    
    def _extract_business_subsections(self, markdown_text: str) -> str:
        """ì‚¬ì—…ì˜ ë‚´ìš© ì„¹ì…˜ì—ì„œ í•µì‹¬ í•˜ìœ„ ì„¹ì…˜ë§Œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§
        target_keywords = [
            'ì£¼ìš” ì œí’ˆ', 'ì£¼ìš” ì„œë¹„ìŠ¤', 'ì›ì¬ë£Œ', 'ìƒì‚° ì„¤ë¹„',
            'ë§¤ì¶œ', 'ìˆ˜ì£¼', 'íŒë§¤', 'ê³ ê°'
        ]
        
        lines = markdown_text.split('\n')
        result_lines = []
        in_target_section = False
        
        for line in lines:
            stripped = line.strip()
            if stripped.startswith('#'):
                # í—¤ë”©ì—ì„œ í‚¤ì›Œë“œ í™•ì¸
                if any(keyword in stripped for keyword in target_keywords):
                    in_target_section = True
                    result_lines.append(line)
                elif stripped.startswith('##'):  # ëŒ€ì„¹ì…˜ ë³€ê²½
                    in_target_section = False
            elif in_target_section:
                result_lines.append(line)
        
        extracted = '\n'.join(result_lines)
        extracted_length = len(extracted.strip())
        original_length = len(markdown_text.strip())
        
        # í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì›ë¬¸ ì „ì²´ ë°˜í™˜ (ìµœì†Œ 200ì ë³´ì¥)
        if extracted_length < 200:
            logger.debug(f"í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŒ ({extracted_length}ì). ì›ë¬¸ ì „ì²´ ì‚¬ìš© ({original_length}ì)")
            return markdown_text
        
        # ìµœì†Œ ê¸¸ì´ ë³´ì¥ (100ì ë¯¸ë§Œì´ë©´ ì›ë¬¸ ë°˜í™˜)
        if extracted_length < 100:
            logger.debug(f"í•„í„°ë§ ê²°ê³¼ê°€ ìµœì†Œ ê¸¸ì´ ë¯¸ë§Œ ({extracted_length}ì). ì›ë¬¸ ì‚¬ìš©")
            return markdown_text
        
        logger.debug(f"í•„í„°ë§ ì„±ê³µ: {extracted_length}ì (ì›ë¬¸: {original_length}ì)")
        return extracted
    
    def _extract_mda_subsections(self, markdown_text: str) -> str:
        """ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨ ì„¹ì…˜ì—ì„œ í•µì‹¬ ë¶€ë¶„ë§Œ ì¶”ì¶œ"""
        lines = markdown_text.split('\n')
        result_lines = []
        stop_patterns = [r'íšŒê³„ê°ì‚¬ì¸ì˜\s*ê°ì‚¬ì˜ê²¬']
        
        for line in lines:
            stripped = line.strip()
            # ì¤‘ì§€ íŒ¨í„´ í™•ì¸
            if any(re.search(pattern, stripped, re.IGNORECASE) for pattern in stop_patterns):
                break
            # í‘œ ì œì™¸
            if stripped.startswith('|'):
                continue
            result_lines.append(line)
        
        extracted = '\n'.join(result_lines)
        return extracted if len(extracted.strip()) > 100 else markdown_text
    
    def _is_special_report_format(self, sub_docs: pd.DataFrame) -> bool:
        """
        íŠ¹ìˆ˜ ë³´ê³ ì„œ í˜•ì‹ì¸ì§€ ê°ì§€ (ì •ì •ì‹ ê³ , ì˜ì—…ë³´ê³ ì„œ ë“±)
        
        Args:
            sub_docs: í•˜ìœ„ ë¬¸ì„œ ëª©ë¡ DataFrame
        
        Returns:
            True: íŠ¹ìˆ˜ ë³´ê³ ì„œ í˜•ì‹, False: ì¼ë°˜ ë³´ê³ ì„œ í˜•ì‹
        """
        if sub_docs is None or len(sub_docs) == 0:
            return False
        
        # íŠ¹ìˆ˜ ë³´ê³ ì„œ í˜•ì‹ íŒ¨í„´
        special_patterns = [
            'ì •ì •ì‹ ê³ ',
            'ì •ì •ì‹ ê³ ì„œ',
            'ì •ì •ì‹ ê³ (ë³´ê³ )',
            'ì˜ì—…ë³´ê³ ì„œ',
            'ì˜ì—…ë³´ê³ ',
            'ì •ê´€',
            'ì´ì‚¬íšŒì˜ì‚¬ë¡',
        ]
        
        # ì‚¬ì—…ë³´ê³ ì„œ ì¼ë°˜ ì„¹ì…˜ íŒ¨í„´
        normal_patterns = [
            'ì‚¬ì—…ì˜ ë‚´ìš©',
            'ì‚¬ì—…ì˜ë‚´ìš©',
            'ì¬ë¬´ì— ê´€í•œ ì‚¬í•­',
            'ì¬ë¬´ì—ê´€í•œì‚¬í•­',
            'ì´ì‚¬ì˜ ê²½ì˜ì§„ë‹¨',
            'ì´ì‚¬ì˜ê²½ì˜ì§„ë‹¨',
        ]
        
        # ëª¨ë“  ì œëª© í™•ì¸
        titles = sub_docs['title'].astype(str).tolist() if 'title' in sub_docs.columns else []
        all_titles_text = ' '.join(titles)
        
        # íŠ¹ìˆ˜ íŒ¨í„´ ë°œê²¬ ì—¬ë¶€
        has_special = any(pattern in all_titles_text for pattern in special_patterns)
        has_normal = any(pattern in all_titles_text for pattern in normal_patterns)
        
        # íŠ¹ìˆ˜ íŒ¨í„´ì´ ìˆê³  ì¼ë°˜ íŒ¨í„´ì´ ì—†ìœ¼ë©´ íŠ¹ìˆ˜ ë³´ê³ ì„œ í˜•ì‹
        if has_special and not has_normal:
            logger.warning(f"íŠ¹ìˆ˜ ë³´ê³ ì„œ í˜•ì‹ ê°ì§€: {titles[:5]}")
            return True
        
        # ì„¹ì…˜ì´ 2ê°œ ì´í•˜ì´ê³  íŠ¹ìˆ˜ íŒ¨í„´ë§Œ ìˆìœ¼ë©´ íŠ¹ìˆ˜ ë³´ê³ ì„œ í˜•ì‹
        if len(sub_docs) <= 2 and has_special:
            logger.warning(f"íŠ¹ìˆ˜ ë³´ê³ ì„œ í˜•ì‹ ê°ì§€ (ì„¹ì…˜ 2ê°œ ì´í•˜): {titles}")
            return True
        
        return False
    
    def _try_extract_from_special_format(self, sub_docs: pd.DataFrame, ticker: str) -> Optional[str]:
        """
        íŠ¹ìˆ˜ ë³´ê³ ì„œ í˜•ì‹ì—ì„œ ì‚¬ì—… ë‚´ìš© ì¶”ì¶œ ì‹œë„ (ì˜ì—…ë³´ê³ ì„œ ë“±)
        
        Args:
            sub_docs: í•˜ìœ„ ë¬¸ì„œ ëª©ë¡
            ticker: ì¢…ëª©ì½”ë“œ
        
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        if sub_docs is None or len(sub_docs) == 0:
            return None
        
        combined_text = ""
        
        # ì˜ì—…ë³´ê³ ì„œ ì„¹ì…˜ ì°¾ê¸°
        for idx, row in sub_docs.iterrows():
            title = str(row.get('title', '')).strip()
            
            # ì˜ì—…ë³´ê³ ì„œ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
            if 'ì˜ì—…ë³´ê³ ì„œ' in title or 'ì˜ì—…ë³´ê³ ' in title:
                url = row.get('url')
                if pd.notna(url) and url and len(str(url)) > 5:
                    logger.info(f"{ticker}: ì˜ì—…ë³´ê³ ì„œ ì„¹ì…˜ ë°œê²¬ - {title}")
                    html = self.fetch_section_content(url)
                    if html:
                        md_text = self.clean_html_to_markdown(html)
                        md_text = self._extract_structured_content(md_text, html)
                        
                        if len(md_text.strip()) > 500:
                            combined_text += f"# 1. ì‚¬ì—…ì˜ ë‚´ìš©\n{md_text}\n\n"
                            logger.info(f"{ticker}: ì˜ì—…ë³´ê³ ì„œì—ì„œ ì‚¬ì—… ë‚´ìš© ì¶”ì¶œ ì„±ê³µ ({len(md_text)}ì)")
                            return combined_text
        
        return None

