"""
ë¬¸ì¥ ê¸°ë°˜ ë“œë¼ì´ë²„ ì‹œê·¸ë„ ì¶”ì¶œê¸° (MVP ìµœì¢… ë²„ì „)

KF-DeBERTa-multitaskë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ë‹¨ìœ„ë¡œ P/Q/C ë“œë¼ì´ë²„ ì‹œê·¸ë„ ì¶”ì¶œ
- ë¬¸ì¥ ê¸°ë°˜ P/Q/C ìë™ íƒœê¹…
- ë°©í–¥ì„± ì¶”ì¶œ
- ë¬¸ì¥ í•„í„°ë§
- ë™ì  Threshold
- íƒ€ì…ë³„/ì „ì²´ Top-K ì œí•œ
"""
import logging
import warnings
import numpy as np
from typing import Dict, List, Optional, Any, Tuple

# Pecab RuntimeWarning ë¬´ì‹œ (overflowëŠ” ê²½ê³ ì¼ ë¿ ì‹¤ì œ ì˜¤ë¥˜ ì•„ë‹˜)
warnings.filterwarnings('ignore', category=RuntimeWarning, module='pecab')
from app.utils.text_chunking import split_into_sentences
# KF-DeBERTa ëª¨ë¸ì€ ì œê±°ë¨ (Solar Embeddingìœ¼ë¡œ ëŒ€ì²´)
# from app.services.embedding_model_direct import get_direct_embedding_model

def get_direct_embedding_model():
    """KF-DeBERTa ëª¨ë¸ì€ ì œê±°ë¨ - None ë°˜í™˜"""
    logger.warning("KF-DeBERTa ëª¨ë¸ì€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. Solar Embeddingì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    return None
from app.models.sector_reference import (
    SECTOR_L2_DEFINITIONS,
    ECONVAR_MASTER
)

# Backward compatibility alias
SUB_SECTOR_DEFINITIONS = SECTOR_L2_DEFINITIONS

logger = logging.getLogger(__name__)

# ë“œë¼ì´ë²„ ì„ë² ë”© ìºì‹œ (ëª¨ë“ˆ ë ˆë²¨)
_driver_embedding_cache: Dict[str, Tuple[np.ndarray, Dict]] = {}

# =============================================================================
# P/Q/C í‚¤ì›Œë“œ ì •ì˜ (ë¬¸ì¥ íƒ€ì… ë¶„ë¥˜ìš©)
# =============================================================================

P_Q_C_KEYWORDS = {
    'P': [
        'ê°€ê²©', 'ë‹¨ê°€', 'ASP', 'ìŠ¤í”„ë ˆë“œ', 'í”„ë¦¬ë¯¸ì—„', 'ì›/', 'ë‹¬ëŸ¬/', 
        'ìƒìŠ¹', 'í•˜ë½', 'ì¸ìƒ', 'ì¸í•˜', 'ê°€ê²©ë³€ë™', 'ê°€ê²©ìƒìŠ¹', 'ê°€ê²©í•˜ë½',
        'ê³ ì •ê±°ë˜ê°€ê²©', 'ê±°ë˜ê°€ê²©', 'íŒë§¤ê°€ê²©', 'ë§¤ì¶œë‹¨ê°€', 'íŒë§¤ë‹¨ê°€'
    ],
    'Q': [
        'ìˆ˜ìš”', 'ë°œì£¼', 'ì¶œí•˜', 'íŒë§¤ëŸ‰', 'ê°€ë™ë¥ ', 'ìˆ˜ì¶œ', 'ìˆ˜ì…', 
        'ì¦ê°€', 'ê°ì†Œ', 'í™•ëŒ€', 'ì¶•ì†Œ', 'ìˆ˜ì£¼', 'ë°œì£¼ëŸ‰', 'ì¶œí•˜ëŸ‰',
        'ìƒì‚°ëŸ‰', 'ìˆ˜ìš”ì¦ê°€', 'ìˆ˜ìš”ê°ì†Œ', 'ë°œì£¼ì¦ê°€', 'ì¶œí•˜ì¦ê°€', 'íŒë§¤ëŸ‰ì¦ê°€'
    ],
    'C': [
        'ì›ê°€', 'ë¹„ìš©', 'ìˆ˜ìœ¨', 'íš¨ìœ¨', 'ë§ˆì§„', 'ì›ê°€ìœ¨', 'ê°œì„ ', 'ì•…í™”',
        'CAPEX', 'íˆ¬ì', 'ì›ê°€ìƒìŠ¹', 'ì›ê°€í•˜ë½', 'ë§ˆì§„ê°œì„ ', 'ë§ˆì§„ì•…í™”',
        'ìˆ˜ìœ¨ê°œì„ ', 'ìˆ˜ìœ¨ì•…í™”', 'ë¹„ìš©ì ˆê°', 'ë¹„ìš©ì¦ê°€', 'ì˜ì—…ì´ìµë¥ '
    ]
}

# íƒ€ì…ë³„/ì „ì²´ Top-K ì œí•œ ìƒìˆ˜
MAX_DRIVERS_PER_TYPE = 3  # P/Q/C íƒ€ì…ë³„ ìµœëŒ€ 3ê°œ ë“œë¼ì´ë²„
MAX_TOTAL_EVIDENCE = 9    # ì „ì²´ evidence ë¬¸ì¥ ìµœëŒ€ 9ê°œ

# =============================================================================
# í—¬í¼ í•¨ìˆ˜ë“¤
# =============================================================================

def classify_sentence_type(sentence: str) -> Optional[str]:
    """
    ë¬¸ì¥ ìì²´ì˜ P/Q/C íƒ€ì… íŒë‹¨ (ì •ê·œì‹ ê¸°ë°˜)
    
    Args:
        sentence: ë¶„ì„í•  ë¬¸ì¥
    
    Returns:
        'P', 'Q', 'C', ë˜ëŠ” None (ë¶ˆëª…í™•í•œ ê²½ìš°)
    """
    sentence_lower = sentence.lower()
    
    # ê° íƒ€ì…ë³„ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
    p_score = sum(1 for kw in P_Q_C_KEYWORDS['P'] if kw in sentence_lower)
    q_score = sum(1 for kw in P_Q_C_KEYWORDS['Q'] if kw in sentence_lower)
    c_score = sum(1 for kw in P_Q_C_KEYWORDS['C'] if kw in sentence_lower)
    
    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ íƒ€ì… ë°˜í™˜ (ë™ì ì´ë©´ None)
    scores = {'P': p_score, 'Q': q_score, 'C': c_score}
    max_score = max(scores.values())
    
    if max_score == 0:
        return None
    
    # ë™ì  ì²˜ë¦¬: ì—¬ëŸ¬ íƒ€ì…ì´ ê°™ìœ¼ë©´ None (ë¶ˆëª…í™•)
    max_types = [t for t, s in scores.items() if s == max_score]
    if len(max_types) > 1:
        return None
    
    return max_types[0]


def extract_direction(sentence: str) -> Optional[str]:
    """
    ë¬¸ì¥ì—ì„œ ë°©í–¥ì„±(ìƒìŠ¹/í•˜ë½) ì¶”ì¶œ
    
    Args:
        sentence: ë¶„ì„í•  ë¬¸ì¥
    
    Returns:
        'ì¦ê°€', 'ê°ì†Œ', ë˜ëŠ” None (ë¶ˆëª…í™•í•œ ê²½ìš°)
    """
    sentence_lower = sentence.lower()
    
    increase_keywords = ['ì¦ê°€', 'ìƒìŠ¹', 'í™•ëŒ€', 'ì¦ëŒ€', 'ì¸ìƒ', 'ê°œì„ ', 'í–¥ìƒ', 'ì¦ê°€ì„¸', 'ìƒìŠ¹ì„¸', 'í™•ì¥']
    decrease_keywords = ['ê°ì†Œ', 'í•˜ë½', 'ì¶•ì†Œ', 'ì¸í•˜', 'ì•…í™”', 'ê°ì†Œì„¸', 'í•˜ë½ì„¸', 'ë‘”í™”', 'ì¶•ì†Œ']
    
    has_increase = any(kw in sentence_lower for kw in increase_keywords)
    has_decrease = any(kw in sentence_lower for kw in decrease_keywords)
    
    if has_increase and not has_decrease:
        return 'ì¦ê°€'
    elif has_decrease and not has_increase:
        return 'ê°ì†Œ'
    else:
        return None  # ë¶ˆëª…í™•í•˜ê±°ë‚˜ ë‘˜ ë‹¤ í¬í•¨


def filter_candidate_sentences(
    sentences: List[str],
    sentence_sources: List[Tuple[str, str]]
) -> Tuple[List[str], List[Tuple[str, str]]]:
    """
    ê³µì‹œìŠ¤ëŸ¬ìš´ ë¬¸ì¥ë§Œ í•„í„°ë§
    
    í•„í„° ê¸°ì¤€:
    - ê¸¸ì´: 20~200ì ì‚¬ì´
    - ìˆ«ì í¬í•¨ (ì„ íƒì , ì™„í™”)
    - P/Q/C í‚¤ì›Œë“œ í¬í•¨
    
    Args:
        sentences: ì›ë³¸ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        sentence_sources: ë¬¸ì¥ ì¶œì²˜ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        (í•„í„°ë§ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸, í•„í„°ë§ëœ ì¶œì²˜ ë¦¬ìŠ¤íŠ¸)
    """
    filtered_sentences = []
    filtered_sources = []
    all_keywords = set()
    for kw_list in P_Q_C_KEYWORDS.values():
        all_keywords.update(kw_list)
    
    for sentence, source in zip(sentences, sentence_sources):
        # ê¸¸ì´ ì²´í¬
        if not (20 <= len(sentence) <= 200):
            continue
        
        # ìˆ«ì í¬í•¨ ì—¬ë¶€ (ì™„í™”: ì„ íƒì )
        has_number = any(ch.isdigit() for ch in sentence)
        
        # P/Q/C í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
        has_keyword = any(kw in sentence for kw in all_keywords)
        
        # ìˆ«ì ë˜ëŠ” í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ í†µê³¼ (ì™„í™”ëœ ê¸°ì¤€)
        if has_number or has_keyword:
            filtered_sentences.append(sentence)
            filtered_sources.append(source)
    
    return filtered_sentences, filtered_sources


def get_fallback_drivers_by_l1(major_sector: str) -> List[str]:
    """
    L1 ì„¹í„° ê¸°ë°˜ ë³´ìˆ˜ì  ë“œë¼ì´ë²„ í’€ (Fallbackìš©)
    
    L2 ê¸°ë°˜ ë“œë¼ì´ë²„ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©
    """
    L1_FALLBACK_DRIVERS = {
        'SEC_SEMI': ['DRAM_ASP', 'NAND_ASP', 'HBM_DEMAND', 'EXCHANGE_RATE_USD_KRW', 'SEMICONDUCTOR_CAPEX'],
        'SEC_AUTO': ['AUTO_SALES_KR', 'EV_SALES', 'EXCHANGE_RATE_USD_KRW', 'STEEL_PRICE'],
        'SEC_BATTERY': ['LITHIUM_PRICE', 'COBALT_PRICE', 'EV_SALES', 'ESS_DEMAND'],
        'SEC_IT': ['IT_SPENDING', 'CLOUD_GROWTH', 'SOFTWARE_DEMAND'],
        'SEC_FINANCE': ['INTEREST_RATE', 'LOAN_DEMAND', 'CARD_SPENDING'],
        'SEC_RETAIL': ['CONSUMER_SPENDING', 'RETAIL_SALES', 'E_COMMERCE_GROWTH'],
        'SEC_CHEM': ['NAPHTHA_PRICE', 'OIL_PRICE', 'PETROCHEMICAL_SPREAD'],
        'SEC_STEEL': ['STEEL_PRICE', 'IRON_ORE_PRICE', 'CONSTRUCTION_ACTIVITY'],
        'SEC_CONST': ['CONSTRUCTION_ACTIVITY', 'HOUSING_START', 'INTEREST_RATE'],
        'SEC_SHIP': ['BDI_INDEX', 'NEWBUILD_ORDER', 'SHIPPING_RATE'],
        'SEC_BIO': ['PHARMA_RND', 'FDA_APPROVAL', 'HEALTHCARE_SPENDING'],
        'SEC_GAME': ['GAME_USER_TRAFFIC', 'NEW_GAME_LAUNCH', 'MOBILE_APP_SALES'],
        'SEC_MEDIA': ['AD_SPENDING', 'CONTENT_EXPORT', 'BOX_OFFICE'],
        'SEC_TRAVEL': ['OUTBOUND_TOURISTS', 'AIR_PASSENGER_TRAFFIC', 'HOTEL_OCCUPANCY'],
        'SEC_FOOD': ['FOOD_PRICE_INDEX', 'GRAIN_PRICE', 'EXCHANGE_RATE_USD_KRW'],
        'SEC_FASHION': ['CLOTHING_SALES', 'CONSUMER_SENTIMENT', 'COTTON_PRICE'],
        'SEC_COSMETIC': ['COSMETIC_EXPORT', 'CHINA_CONSUMPTION', 'DUTY_FREE_SALES'],
        'SEC_MACH': ['MACHINERY_ORDER', 'CAPEX_INVESTMENT', 'EXCHANGE_RATE_USD_KRW'],
        'SEC_DEFENSE': ['DEFENSE_BUDGET', 'ARMS_EXPORT', 'GEOPOLITICAL_RISK'],
        'SEC_UTIL': ['ELECTRICITY_DEMAND', 'SMP_PRICE', 'LNG_PRICE'],
        'SEC_TELECOM': ['ARPU_MOBILE', '5G_PENETRATION', 'MARKETING_COST'],
    }
    
    drivers = L1_FALLBACK_DRIVERS.get(major_sector, [])
    if drivers:
        logger.info(f"[L1 Fallback] {major_sector} â†’ {len(drivers)}ê°œ ë“œë¼ì´ë²„ ì œê³µ (ë³´ìˆ˜ì  í’€)")
    return drivers


def get_candidate_drivers(
    major_sector: str,
    sub_sector: Optional[str]
) -> List[str]:
    """
    ì„¹í„°/ì„œë¸Œì„¹í„° ì»¨í…ìŠ¤íŠ¸ì— ë§ëŠ” ë“œë¼ì´ë²„ë§Œ ìˆ˜ì§‘
    
    ì ˆëŒ€ "ëª¨ë“  ì„¹í„°ì˜ ëª¨ë“  ë“œë¼ì´ë²„"ì™€ ë¹„êµí•˜ì§€ ì•Šë„ë¡ ë³´ì¥
    
    Args:
        major_sector: Major Sector ì½”ë“œ
        sub_sector: Sub-sector ì½”ë“œ
    
    Returns:
        ë“œë¼ì´ë²„ ì½”ë“œ ë¦¬ìŠ¤íŠ¸
    """
    standard_drivers = []
    
    # 1) SECTOR_L2_DEFINITIONSì—ì„œ í•´ë‹¹ ì„¹í„°ì˜ ë“œë¼ì´ë²„ë§Œ ê°€ì ¸ì˜¤ê¸°
    if major_sector in SUB_SECTOR_DEFINITIONS:
        if sub_sector and sub_sector in SUB_SECTOR_DEFINITIONS[major_sector]:
            sub_def = SUB_SECTOR_DEFINITIONS[major_sector][sub_sector]
            # âœ… ìˆ˜ì •: recommended_drivers + common_drivers ì‚¬ìš© (drivers í•„ë“œ ì—†ìŒ)
            recommended = sub_def.get('recommended_drivers', [])
            common = sub_def.get('common_drivers', [])
            standard_drivers = list(set(recommended + common))
            logger.debug(f"í‘œì¤€ ë“œë¼ì´ë²„ ìˆ˜ì§‘(L2): {sub_sector} â†’ {len(standard_drivers)}ê°œ")
        else:
            # sub_sectorê°€ Noneì´ë©´ major_sectorì˜ ëª¨ë“  sub_sector ë“œë¼ì´ë²„ ìˆ˜ì§‘
            logger.info(f"Sub-sector ì •ì˜ ì—†ìŒ ({major_sector}/{sub_sector}), major_sectorì˜ ëª¨ë“  ë“œë¼ì´ë²„ ìˆ˜ì§‘")
            all_drivers = set()
            for sub_def in SUB_SECTOR_DEFINITIONS[major_sector].values():
                all_drivers.update(sub_def.get('recommended_drivers', []))
                all_drivers.update(sub_def.get('common_drivers', []))
            standard_drivers = list(all_drivers)
            logger.debug(f"Major sector ë“œë¼ì´ë²„ ìˆ˜ì§‘(L2 ì¢…í•©): {major_sector} â†’ {len(standard_drivers)}ê°œ")
    
    # 2) âœ… L1 ê¸°ë°˜ Fallback (L2ì—ì„œ ë“œë¼ì´ë²„ë¥¼ ëª» ì°¾ì€ ê²½ìš°)
    if not standard_drivers:
        logger.warning(f"L2 ë“œë¼ì´ë²„ ì—†ìŒ, L1 Fallback ì‹œë„: {major_sector}")
        standard_drivers = get_fallback_drivers_by_l1(major_sector)
    
    # 3) ECONVAR_MASTERì—ì„œ ì§ì ‘ ì°¾ê¸° (ìµœì¢… Fallback, ìµœì†Œí•œì˜ í‚¤ì›Œë“œ ë§¤ì¹­)
    if not standard_drivers:
        logger.warning(f"L1 Fallbackë„ ì‹¤íŒ¨: {major_sector}/{sub_sector} â†’ ECONVAR_MASTER í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œë„")
        # ì„¹í„°ë³„ í‚¤ì›Œë“œ ë§¤ì¹­ (ìµœì†Œí•œì˜ fallback)
        sector_keywords_map = {
            'SEC_CARD': ['ì¹´ë“œ', 'ê²°ì œ', 'ì‹ ìš©', 'ì²´í¬'],
            'SEC_RETAIL': ['ìœ í†µ', 'ì†Œë§¤', 'ë§ˆíŠ¸', 'ì‡¼í•‘'],
            'SEC_IT': ['IT', 'ì†Œí”„íŠ¸ì›¨ì–´', 'í´ë¼ìš°ë“œ', 'SaaS'],
            'SEC_SEMI': ['ë°˜ë„ì²´', 'ë©”ëª¨ë¦¬', 'DRAM', 'NAND', 'HBM'],
            'SEC_AUTO': ['ìë™ì°¨', 'ì „ê¸°ì°¨', 'EV', 'ì™„ì„±ì°¨'],
            'SEC_BATTERY': ['ë°°í„°ë¦¬', '2ì°¨ì „ì§€', 'ì–‘ê·¹ì¬', 'ìŒê·¹ì¬'],
            'SEC_CHEM': ['í™”í•™', 'ì„ìœ í™”í•™', 'ì •ìœ ', 'ë‚˜í”„íƒ€'],
            'SEC_STEEL': ['ì² ê°•', 'ê°•íŒ', 'ê°•ì¬'],
        }
        
        if major_sector in sector_keywords_map:
            keywords = sector_keywords_map[major_sector]
            for driver_code, econvar_info in ECONVAR_MASTER.items():
                description = econvar_info.get('description', '') + ' ' + econvar_info.get('name_ko', '')
                if any(kw in description for kw in keywords):
                    standard_drivers.append(driver_code)
        
        if standard_drivers:
            logger.info(f"ECONVAR_MASTERì—ì„œ {len(standard_drivers)}ê°œ ë“œë¼ì´ë²„ ë°œê²¬ (í‚¤ì›Œë“œ ë§¤ì¹­)")
    
    if not standard_drivers:
        logger.error(f"[Step 4A] ë“œë¼ì´ë²„ í›„ë³´ ìˆ˜ì§‘ ì™„ì „ ì‹¤íŒ¨: {major_sector}/{sub_sector}")
    else:
        logger.info(f"[Step 4A] ë“œë¼ì´ë²„ í›„ë³´: {len(standard_drivers)}ê°œ")
    
    return standard_drivers


def dedupe_similar_sentences(
    sentences: List[Dict[str, Any]],
    similarity_threshold: float = 0.95
) -> List[Dict[str, Any]]:
    """
    ìœ ì‚¬í•œ ë¬¸ì¥ ì¤‘ë³µ ì œê±° (ì„ë² ë”© ê¸°ë°˜)
    
    Args:
        sentences: ë¬¸ì¥ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ [{"text": "...", "similarity": 0.85, ...}, ...]
        similarity_threshold: ì¤‘ë³µìœ¼ë¡œ íŒë‹¨í•  ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ 0.95)
    
    Returns:
        ì¤‘ë³µ ì œê±°ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    """
    if len(sentences) <= 1:
        return sentences
    
    try:
        # ë¬¸ì¥ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        sentence_texts = [s["text"] for s in sentences]
        
        # ì„ë² ë”© ìƒì„±
        model = get_direct_embedding_model()
        if model is None:
            logger.warning("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ì¤‘ë³µ ì œê±° ê±´ë„ˆëœ€")
            return sentences
        embeddings = model.encode(sentence_texts, convert_to_numpy=True)
        
        # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        similarities = np.dot(embeddings, embeddings.T)
        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
        similarities = similarities / (norms * norms.T)
        
        # ì¤‘ë³µ ì œê±° (ìƒìœ„ ë¬¸ì¥ ìš°ì„  ìœ ì§€)
        kept_indices = []
        for i in range(len(sentences)):
            is_duplicate = False
            for j in kept_indices:
                if similarities[i, j] >= similarity_threshold:
                    is_duplicate = True
                    break
            if not is_duplicate:
                kept_indices.append(i)
        
        return [sentences[i] for i in kept_indices]
    except Exception as e:
        logger.warning(f"ë¬¸ì¥ ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}, ì›ë³¸ ë°˜í™˜")
        return sentences


def cap_signals_per_type(
    signals_by_type: Dict[str, Dict[str, Any]]
) -> Dict[str, List[Dict]]:
    """
    íƒ€ì…ë³„/ì „ì²´ Top-K ì œí•œ ì ìš© (evidence ì¤‘ë³µ ì œê±° í¬í•¨)
    
    ê·œì¹™:
    - ê° íƒ€ì…ë³„ ìµœëŒ€ 3ê°œ ë“œë¼ì´ë²„
    - ì „ì²´ í•©ì‚° ìµœëŒ€ 9ê°œ evidence ë¬¸ì¥
    - evidence ë¬¸ì¥ ì¤‘ë³µ ì œê±° (ìœ ì‚¬ë„ 0.95 ì´ìƒ)
    
    Args:
        signals_by_type: íƒ€ì…ë³„ ë“œë¼ì´ë²„ ì‹œê·¸ë„ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        ì œí•œì´ ì ìš©ëœ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    result = {
        "price_signals": [],
        "quantity_signals": [],
        "cost_signals": []
    }
    
    total_evidence_count = 0
    
    for signal_type in ["price_signals", "quantity_signals", "cost_signals"]:
        signals = signals_by_type[signal_type]
        
        # Score ê¸°ì¤€ ì •ë ¬
        sorted_signals = sorted(
            signals.values(),
            key=lambda x: x["score"],
            reverse=True
        )
        
        # íƒ€ì…ë³„ ìµœëŒ€ 3ê°œê¹Œì§€ ì±„ìš°ë˜, ì „ì²´ 9ê°œ ì œí•œ ê³ ë ¤
        for signal in sorted_signals[:MAX_DRIVERS_PER_TYPE]:
            if total_evidence_count >= MAX_TOTAL_EVIDENCE:
                break
            
            # ê° ë“œë¼ì´ë²„ì˜ ìƒìœ„ ë¬¸ì¥ ì„ íƒ (ë‚¨ì€ ì—¬ìœ ë¶„ ê³ ë ¤)
            remaining_slots = MAX_TOTAL_EVIDENCE - total_evidence_count
            candidate_sentences = sorted(
                signal["sentences"],
                key=lambda x: x["similarity"],
                reverse=True
            )[:min(5, remaining_slots + 2)]  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ì—¬ìœ ë¶„ í™•ë³´
            
            # ğŸ”¥ ì‹ ê·œ: evidence ë¬¸ì¥ ì¤‘ë³µ ì œê±°
            deduped_sentences = dedupe_similar_sentences(candidate_sentences)
            top_sentences = deduped_sentences[:min(3, remaining_slots)]
            
            if top_sentences:  # ë¬¸ì¥ì´ ìˆì„ ë•Œë§Œ ì¶”ê°€
                # ë°©í–¥ì„± ì¶”ì¶œ (ì²« ë²ˆì§¸ ë¬¸ì¥ ê¸°ì¤€)
                direction = extract_direction(top_sentences[0]["text"]) if top_sentences else None
                
                result[signal_type].append({
                    "var": signal["var"],
                    "code": signal["code"],
                    "type": signal["type"],
                    "score": signal["score"],
                    "direction": direction,  # ğŸ”¥ ì‹ ê·œ: ë°©í–¥ì„± ì¶”ê°€
                    "evidence": [s["text"] for s in top_sentences],
                    "sentence_types": [classify_sentence_type(s["text"]) for s in top_sentences]  # ë””ë²„ê¹…ìš©
                })
                total_evidence_count += len(top_sentences)
    
    return result


def extract_driver_signals_from_sentences(
    company_detail: Any,  # CompanyDetail ê°ì²´
    major_sector: str,
    sub_sector: Optional[str] = None,
    sector_l2: Optional[str] = None  # â­ L2 ì •ë³´ ì¶”ê°€
) -> Dict[str, List[Dict]]:
    """
    ë¬¸ì¥ ë‹¨ìœ„ ì„ë² ë”© ê¸°ë°˜ P/Q/C ë“œë¼ì´ë²„ ì‹œê·¸ë„ ì¶”ì¶œ (MVP ìµœì¢… ë²„ì „)
    
    í”„ë¡œì„¸ìŠ¤:
    1. í…ìŠ¤íŠ¸ â†’ ë¬¸ì¥ ë‹¨ìœ„ split
    2. ë¬¸ì¥ í•„í„°ë§ (ê³µì‹œìŠ¤ëŸ¬ìš´ ë¬¸ì¥ë§Œ)
    3. ê° ë¬¸ì¥ì„ KF-DeBERTaë¡œ ì„ë² ë”©
    4. ì„¹í„°ë³„ ë“œë¼ì´ë²„ í›„ë³´ ìˆ˜ì§‘ (ì»¨í…ìŠ¤íŠ¸ í•œì •)
    5. ë“œë¼ì´ë²„ ì„¤ëª… ì„ë² ë”©
    6. ìœ ì‚¬ë„ ê³„ì‚° ë° ë™ì  Threshold ì ìš©
    7. ë¬¸ì¥ íƒ€ì…ê³¼ ë“œë¼ì´ë²„ íƒ€ì… ì •ë ¬
    8. íƒ€ì…ë³„/ì „ì²´ Top-K ì œí•œ
    
    Args:
        company_detail: CompanyDetail ê°ì²´
        major_sector: Major Sector ì½”ë“œ
        sub_sector: Sub-sector ì½”ë“œ
    
    Returns:
        {
            "price_signals": [
                {
                    "var": "DRAM ASP",
                    "code": "DRAM_ASP",
                    "type": "P",
                    "score": 0.85,
                    "direction": "ì¦ê°€",
                    "evidence": ["ë¬¸ì¥1", "ë¬¸ì¥2"]
                }
            ],
            "quantity_signals": [...],
            "cost_signals": [...]
        }
    """
    # â­ ë¡œê·¸ ê°•í™”: Step 4A ì‹œì‘
    logger.info(f"[Step 4A] ë“œë¼ì´ë²„ ì‹œê·¸ë„ ì¶”ì¶œ ì‹œì‘: {major_sector}/{sub_sector}" + (f" (L2: {sector_l2})" if sector_l2 else ""))
    
    # 1. í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬
    all_sentences = []
    sentence_sources = []  # ë¬¸ì¥ ì¶œì²˜ ì¶”ì 
    
    # biz_summary ë¬¸ì¥ ë¶„ë¦¬
    if company_detail.biz_summary:
        sentences = split_into_sentences(company_detail.biz_summary)
        all_sentences.extend(sentences)
        sentence_sources.extend([("biz_summary", s) for s in sentences])
    
    # products ì„¤ëª… ë¬¸ì¥ ë¶„ë¦¬
    if company_detail.products:
        for product in company_detail.products[:10]:
            product_text = str(product)
            sentences = split_into_sentences(product_text)
            all_sentences.extend(sentences)
            sentence_sources.extend([("products", s) for s in sentences])
    
    # keywords ì„¤ëª… ë¬¸ì¥ ë¶„ë¦¬
    if company_detail.keywords:
        for keyword in company_detail.keywords[:10]:
            keyword_text = str(keyword)
            sentences = split_into_sentences(keyword_text)
            all_sentences.extend(sentences)
            sentence_sources.extend([("keywords", s) for s in sentences])
    
    # raw_materials ì„¤ëª… ë¬¸ì¥ ë¶„ë¦¬
    if company_detail.raw_materials:
        for rm in company_detail.raw_materials[:10]:
            rm_text = str(rm)
            sentences = split_into_sentences(rm_text)
            all_sentences.extend(sentences)
            sentence_sources.extend([("raw_materials", s) for s in sentences])
    
    if not all_sentences:
        logger.warning("ë¬¸ì¥ ì¶”ì¶œ ì‹¤íŒ¨: í…ìŠ¤íŠ¸ ì—†ìŒ")
        return {
            "price_signals": [],
            "quantity_signals": [],
            "cost_signals": []
        }
    
    logger.info(f"âœ… [Step 4A] ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬ ì™„ë£Œ: {len(all_sentences)}ê°œ ë¬¸ì¥")
    
    # 2. ë¬¸ì¥ í•„í„°ë§ (ê³µì‹œìŠ¤ëŸ¬ìš´ ë¬¸ì¥ë§Œ)
    filtered_sentences, filtered_sources = filter_candidate_sentences(
        all_sentences, sentence_sources
    )
    logger.info(f"[Step 4A] í•„í„°ë§ í›„: {len(filtered_sentences)}ê°œ (ì „ì²´ {len(all_sentences)}ê°œ ì¤‘)")
    
    if not filtered_sentences:
        logger.warning("[Step 4A] í•„í„°ë§ í›„ ë¬¸ì¥ ì—†ìŒ")
        return {
            "price_signals": [],
            "quantity_signals": [],
            "cost_signals": []
        }
    
    logger.info(f"âœ… [Step 4A] ë¬¸ì¥ í•„í„°ë§ ì™„ë£Œ: {len(all_sentences)}ê°œ â†’ {len(filtered_sentences)}ê°œ")
    
    # 3. ì„¹í„°ë³„ ë“œë¼ì´ë²„ í›„ë³´ ìˆ˜ì§‘ (ì»¨í…ìŠ¤íŠ¸ í•œì •)
    standard_drivers = get_candidate_drivers(major_sector, sub_sector)
    
    if not standard_drivers:
        logger.warning(f"ë“œë¼ì´ë²„ í›„ë³´ ì—†ìŒ: {major_sector}/{sub_sector}")
        return {
            "price_signals": [],
            "quantity_signals": [],
            "cost_signals": []
        }
    
    logger.info(f"âœ… [Step 4A] ë“œë¼ì´ë²„ í›„ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(standard_drivers)}ê°œ")
    
    # 4. ê° ë¬¸ì¥ì„ KF-DeBERTaë¡œ ì„ë² ë”© (í•„í„°ë§ëœ ë¬¸ì¥ë§Œ)
    try:
        import time
        embedding_start = time.time()
        logger.info(f"ğŸ”„ [Step 4A] KF-DeBERTa ëª¨ë¸ ë¡œë”© ì¤‘...")
        model = get_direct_embedding_model()
        if model is None:
            logger.warning("ë“œë¼ì´ë²„ ì„ë² ë”© ì—†ì´ ìœ ì‚¬ë„ ê³„ì‚°ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {
                "price_signals": [],
                "quantity_signals": [],
                "cost_signals": []
            }
        model_load_time = time.time() - embedding_start
        if model_load_time > 1.0:  # 1ì´ˆ ì´ìƒ ê±¸ë ¸ìœ¼ë©´ ë¡œë”© ì‹œê°„ ë¡œê·¸
            logger.info(f"âœ… [Step 4A] KF-DeBERTa ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({model_load_time:.2f}ì´ˆ)")
        
        encode_start = time.time()
        logger.info(f"ğŸ”„ [Step 4A] ë¬¸ì¥ ì„ë² ë”© ìƒì„± ì¤‘... ({len(filtered_sentences)}ê°œ ë¬¸ì¥)")
        sentence_embeddings = model.encode(
            filtered_sentences,
            batch_size=16,
            convert_to_numpy=True
        )
        encode_time = time.time() - encode_start
        logger.info(f"âœ… [Step 4A] ë¬¸ì¥ ì„ë² ë”© ì™„ë£Œ: {len(sentence_embeddings)}ê°œ ({encode_time:.2f}ì´ˆ)")
    except Exception as e:
        logger.error(f"ë¬¸ì¥ ì„ë² ë”© ì‹¤íŒ¨: {e}")
        return {
            "price_signals": [],
            "quantity_signals": [],
            "cost_signals": []
        }
    
    # 4. ECONVAR_MASTERì˜ ë“œë¼ì´ë²„ ì„¤ëª…ì„ ì„ë² ë”© (ìºì‹± + ë°°ì¹˜ ì²˜ë¦¬)
    driver_embeddings = {}
    driver_info = {}
    driver_texts_to_encode = []  # ìºì‹œì— ì—†ëŠ” ë“œë¼ì´ë²„ë§Œ ë°°ì¹˜ë¡œ ì¸ì½”ë”©
    driver_codes_to_encode = []
    
    for driver_code in standard_drivers:
        econvar_info = ECONVAR_MASTER.get(driver_code, {})
        if not econvar_info:
            continue
        
        # ìºì‹œ í™•ì¸
        if driver_code in _driver_embedding_cache:
            driver_emb, cached_info = _driver_embedding_cache[driver_code]
            driver_embeddings[driver_code] = driver_emb
            driver_info[driver_code] = cached_info
        else:
            # ë“œë¼ì´ë²„ ì„¤ëª… í…ìŠ¤íŠ¸ êµ¬ì„±
            description = econvar_info.get('description', '')
            name_ko = econvar_info.get('name_ko', driver_code)
            driver_text = f"{name_ko} {description}"
            driver_texts_to_encode.append(driver_text)
            driver_codes_to_encode.append((driver_code, econvar_info))
    
    # ë°°ì¹˜ ì„ë² ë”© ì²˜ë¦¬ (ìºì‹œì— ì—†ëŠ” ë“œë¼ì´ë²„ë§Œ)
    if driver_texts_to_encode:
        try:
            import time
            batch_start = time.time()
            batch_embeddings = model.encode(
                driver_texts_to_encode,
                batch_size=len(driver_texts_to_encode),  # í•œ ë²ˆì— ì²˜ë¦¬
                convert_to_numpy=True
            )
            batch_time = time.time() - batch_start
            
            # ê²°ê³¼ ì €ì¥ ë° ìºì‹œ ì—…ë°ì´íŠ¸
            for (driver_code, econvar_info), driver_emb in zip(driver_codes_to_encode, batch_embeddings):
                driver_embeddings[driver_code] = driver_emb
                driver_info[driver_code] = econvar_info
                # ìºì‹œì— ì €ì¥
                _driver_embedding_cache[driver_code] = (driver_emb, econvar_info)
            
            if len(driver_texts_to_encode) > 1:
                logger.info(f"ë“œë¼ì´ë²„ ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(driver_texts_to_encode)}ê°œ ({batch_time:.3f}ì´ˆ, ìºì‹œ: {len(driver_embeddings) - len(driver_texts_to_encode)}ê°œ)")
        except Exception as e:
            logger.warning(f"ë“œë¼ì´ë²„ ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±
            for driver_code, econvar_info in driver_codes_to_encode:
                try:
                    description = econvar_info.get('description', '')
                    name_ko = econvar_info.get('name_ko', driver_code)
                    driver_text = f"{name_ko} {description}"
                    driver_emb = model.encode([driver_text], convert_to_numpy=True)[0]
                    driver_embeddings[driver_code] = driver_emb
                    driver_info[driver_code] = econvar_info
                    _driver_embedding_cache[driver_code] = (driver_emb, econvar_info)
                except Exception as e2:
                    logger.warning(f"ë“œë¼ì´ë²„ ì„ë² ë”© ì‹¤íŒ¨ ({driver_code}): {e2}")
                    continue
    
    logger.info(f"ë“œë¼ì´ë²„ ì„ë² ë”© ì™„ë£Œ: {len(driver_embeddings)}ê°œ (ìºì‹œ íˆíŠ¸: {len(driver_embeddings) - len(driver_texts_to_encode)}ê°œ)")
    
    # 5. ë¬¸ì¥-ë“œë¼ì´ë²„ ìœ ì‚¬ë„ ê³„ì‚° ë° ë¶„ë¥˜ (ë²¡í„°í™” ì—°ì‚°)
    driver_signals = {
        "price_signals": {},
        "quantity_signals": {},
        "cost_signals": {}
    }
    
    if not driver_embeddings:
        logger.warning("ë“œë¼ì´ë²„ ì„ë² ë”©ì´ ì—†ì–´ ìœ ì‚¬ë„ ê³„ì‚°ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        import time
        similarity_start = time.time()
        
        # ë²¡í„°í™” ì—°ì‚° ì¤€ë¹„
        sentence_embeddings_array = np.array(sentence_embeddings)  # (n, dim)
        driver_codes_list = list(driver_embeddings.keys())
        driver_embeddings_array = np.array([driver_embeddings[code] for code in driver_codes_list])  # (m, dim)
        
        # ë°°ì¹˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ë²¡í„°í™”)
        # similarities[i, j] = sentence iì™€ driver jì˜ ìœ ì‚¬ë„
        similarities = np.dot(sentence_embeddings_array, driver_embeddings_array.T)  # (n, m)
        
        # ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        norms_sentence = np.linalg.norm(sentence_embeddings_array, axis=1, keepdims=True)  # (n, 1)
        norms_driver = np.linalg.norm(driver_embeddings_array, axis=1)  # (m,)
        similarities = similarities / (norms_sentence * norms_driver)  # (n, m)
        
        # ê° ë¬¸ì¥ì˜ ìµœì  ë“œë¼ì´ë²„ ì°¾ê¸°
        best_indices = np.argmax(similarities, axis=1)  # (n,) - ê° ë¬¸ì¥ì˜ ìµœì  ë“œë¼ì´ë²„ ì¸ë±ìŠ¤
        best_similarities = similarities[np.arange(len(similarities)), best_indices]  # (n,) - ìµœì  ìœ ì‚¬ë„ ê°’
        
        similarity_time = time.time() - similarity_start
        if len(filtered_sentences) > 10:  # ë¬¸ì¥ì´ ë§ì„ ë•Œë§Œ ë¡œê·¸ ì¶œë ¥
            logger.info(f"ë²¡í„°í™” ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ: {len(filtered_sentences)}ê°œ ë¬¸ì¥ Ã— {len(driver_codes_list)}ê°œ ë“œë¼ì´ë²„ ({similarity_time:.3f}ì´ˆ)")
        
        # ğŸ”¥ ì‹ ê·œ: ìœ ì‚¬ë„ ë¶„í¬ ë¡œê¹…
        similarity_scores = best_similarities.tolist()
        logger.info(f"ğŸ“Š [ë“œë¼ì´ë²„ ì‹œê·¸ë„] ìœ ì‚¬ë„ ë¶„í¬: "
                    f"min={min(similarity_scores):.3f}, "
                    f"max={max(similarity_scores):.3f}, "
                    f"mean={np.mean(similarity_scores):.3f}, "
                    f"median={np.median(similarity_scores):.3f}, "
                    f"std={np.std(similarity_scores):.3f}, "
                    f"ìƒìœ„20%={np.percentile(similarity_scores, 80):.3f}")
        
        # ğŸ”¥ ì‹ ê·œ: ë™ì  Threshold ì ìš© (percentile 80 â†’ 70ìœ¼ë¡œ ì™„í™”)
        threshold = np.percentile(best_similarities, 70)  # 80 â†’ 70ìœ¼ë¡œ ì™„í™”
        threshold = max(0.4, min(0.6, threshold))
        logger.info(f"ğŸ“Š [ë“œë¼ì´ë²„ ì‹œê·¸ë„] ë™ì  Threshold: {threshold:.3f} (percentile 70, ê¸°ì¡´ ê³ ì •ê°’: 0.6)")
        
        threshold_mask = best_similarities > threshold
        
        # â­ Minimum Guarantee: ìµœì†Œ 3ê°œ ë³´ì¥
        matched_count = np.sum(threshold_mask)
        if matched_count < 3 and len(best_similarities) >= 3:
            # ìµœì†Œ 3ê°œ ë³´ì¥: ìƒìœ„ 3ê°œ ê°•ì œ ì„ íƒ
            logger.warning(f"âš ï¸ [Step 4A] Threshold ë§¤ì¹­ {matched_count}ê°œ < 3ê°œ, ìƒìœ„ 3ê°œ ê°•ì œ ì„ íƒ")
            top3_indices = np.argsort(best_similarities)[-3:]
            threshold_mask = np.zeros_like(best_similarities, dtype=bool)
            threshold_mask[top3_indices] = True
        
        # 7. ë“œë¼ì´ë²„ ì‹œê·¸ë„ ìˆ˜ì§‘ (ë¬¸ì¥ íƒ€ì…ê³¼ ë“œë¼ì´ë²„ íƒ€ì… ì •ë ¬)
        for idx in np.where(threshold_mask)[0]:
            sentence = filtered_sentences[idx]
            source_type, _ = filtered_sources[idx]
            best_match_idx = best_indices[idx]
            best_match = driver_codes_list[best_match_idx]
            best_similarity = float(best_similarities[idx])
            info = driver_info[best_match]
            var_type = info.get('type', '')
            
            # ğŸ”¥ ì‹ ê·œ: ë¬¸ì¥ ìì²´ì˜ P/Q/C íƒ€ì… íŒë‹¨
            sentence_type = classify_sentence_type(sentence)
            
            # ğŸ”¥ ì‹ ê·œ: ë¬¸ì¥ íƒ€ì…ê³¼ ë“œë¼ì´ë²„ íƒ€ì…ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            # ì¤‘ìš”: sentence_typeì´ Noneì´ë©´ íƒ€ì… ê²€ì‚¬ë¥¼ ìƒëµ (ì •ê·œì‹ ê¸°ë°˜ íƒœê¹…ì˜ recall í•œê³„ ë³´ì™„)
            # sentence_typeì´ ëª…í™•í•  ë•Œë§Œ íƒ€ì… ë¶ˆì¼ì¹˜ ì²´í¬í•˜ì—¬ ì •í™•ë„ í–¥ìƒ
            if sentence_type is not None and sentence_type != var_type:
                # íƒ€ì… ë¶ˆì¼ì¹˜: ìœ ì‚¬ë„ê°€ ë†’ì•„ë„ ìŠ¤í‚µ (ì •í™•ë„ í–¥ìƒ)
                logger.debug(f"íƒ€ì… ë¶ˆì¼ì¹˜ ìŠ¤í‚µ: ë¬¸ì¥='{sentence[:50]}...' (ë¬¸ì¥íƒ€ì…={sentence_type}, ë“œë¼ì´ë²„íƒ€ì…={var_type})")
                continue
            
            signal_key = f"{best_match}_{var_type}"
            
            if var_type == 'P':
                if signal_key not in driver_signals["price_signals"]:
                    driver_signals["price_signals"][signal_key] = {
                        "var": info.get('name_ko', best_match),
                        "code": best_match,
                        "type": "P",
                        "score": best_similarity,
                        "sentences": []
                    }
                driver_signals["price_signals"][signal_key]["sentences"].append({
                    "text": sentence,
                    "source": source_type,
                    "similarity": best_similarity
                })
                # ìµœê³  ì ìˆ˜ ì—…ë°ì´íŠ¸
                if best_similarity > driver_signals["price_signals"][signal_key]["score"]:
                    driver_signals["price_signals"][signal_key]["score"] = best_similarity
            
            elif var_type == 'Q':
                if signal_key not in driver_signals["quantity_signals"]:
                    driver_signals["quantity_signals"][signal_key] = {
                        "var": info.get('name_ko', best_match),
                        "code": best_match,
                        "type": "Q",
                        "score": best_similarity,
                        "sentences": []
                    }
                driver_signals["quantity_signals"][signal_key]["sentences"].append({
                    "text": sentence,
                    "source": source_type,
                    "similarity": best_similarity
                })
                if best_similarity > driver_signals["quantity_signals"][signal_key]["score"]:
                    driver_signals["quantity_signals"][signal_key]["score"] = best_similarity
            
            elif var_type == 'C':
                if signal_key not in driver_signals["cost_signals"]:
                    driver_signals["cost_signals"][signal_key] = {
                        "var": info.get('name_ko', best_match),
                        "code": best_match,
                        "type": "C",
                        "score": best_similarity,
                        "sentences": []
                    }
                driver_signals["cost_signals"][signal_key]["sentences"].append({
                    "text": sentence,
                    "source": source_type,
                    "similarity": best_similarity
                })
                if best_similarity > driver_signals["cost_signals"][signal_key]["score"]:
                    driver_signals["cost_signals"][signal_key]["score"] = best_similarity
    
    # 8. íƒ€ì…ë³„/ì „ì²´ Top-K ì œí•œ ì ìš©
    result = cap_signals_per_type(driver_signals)
    
    total_evidence = sum(len(s.get('evidence', [])) for s in result['price_signals'] + result['quantity_signals'] + result['cost_signals'])
    
    # ìµœì¢… ê²°ê³¼ ë¡œê¹…
    if not result['price_signals'] and not result['quantity_signals'] and not result['cost_signals']:
        logger.warning(f"[Step 4A] ìœ ì‚¬ë„ Threshold ë¯¸ë‹¬ ë˜ëŠ” ë§¤ì¹­ ì‹¤íŒ¨ (ë“œë¼ì´ë²„ 0ê°œ)")
    else:
        logger.info(f"âœ… [Step 4A] ë“œë¼ì´ë²„ ì‹œê·¸ë„ ì¶”ì¶œ ì™„ë£Œ: "
                    f"P={len(result['price_signals'])}, "
                    f"Q={len(result['quantity_signals'])}, "
                    f"C={len(result['cost_signals'])}, "
                    f"ì „ì²´ evidence={total_evidence}ê°œ")
    
    return result


