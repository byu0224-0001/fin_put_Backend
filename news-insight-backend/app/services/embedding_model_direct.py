"""
ì§ì ‘ ì„ë² ë”© ëª¨ë¸ - SentenceTransformer ì™„ì „ ìš°íšŒ
Meta Tensor ì˜¤ë¥˜ë¥¼ ì™„ì „íˆ íšŒí”¼í•˜ê¸° ìœ„í•´ AutoModel + mean pooling ì§ì ‘ êµ¬í˜„
"""
import logging
import os
import numpy as np
from typing import List, Union, Optional
import torch
from transformers import AutoModel, AutoTokenizer

# â­ ëª¨ë“ˆ ë¡œë“œ ì‹œì ì— accelerate ë¹„í™œì„±í™” (ê°€ì¥ ë¹ ë¥¸ ì‹œì ì— ì„¤ì •)
os.environ['TRANSFORMERS_NO_ACCELERATE'] = '1'
os.environ['ACCELERATE_USE_CPU'] = '1'

logger = logging.getLogger(__name__)

class DirectEmbeddingModel:
    """
    SentenceTransformerë¥¼ ì™„ì „íˆ ìš°íšŒí•˜ëŠ” ì§ì ‘ ì„ë² ë”© ëª¨ë¸
    AutoModel + mean poolingìœ¼ë¡œ ì„ë² ë”© ìƒì„±
    """
    
    def __init__(self, model_name: str = "upskyy/kf-deberta-multitask", device: str = None):
        """
        Args:
            model_name: HuggingFace ëª¨ë¸ ì´ë¦„
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', None=ìë™)
        """
        self.model_name = model_name
        self._model = None
        self._tokenizer = None
        self._device = None
        
        # ë””ë°”ì´ìŠ¤ ê²°ì •
        if device is None:
            if torch.cuda.is_available():
                device = "cuda"
            else:
                device = "cpu"
        
        self._load_model(device)
    
    def _load_model(self, target_device: str):
        """ëª¨ë¸ ë¡œë“œ (meta tensor ë¬¸ì œ ì™„ì „ íšŒí”¼)"""
        import time
        load_start = time.time()
        logger.info(f"ğŸ”„ [KF-DeBERTa] ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_name}")
        
        try:
            # 1. Tokenizer ë¡œë“œ
            tokenizer_start = time.time()
            logger.info(f"  ğŸ“¥ [KF-DeBERTa] Tokenizer ë¡œë”© ì¤‘...")
            self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            tokenizer_time = time.time() - tokenizer_start
            logger.info(f"  âœ… [KF-DeBERTa] Tokenizer ë¡œë”© ì™„ë£Œ ({tokenizer_time:.2f}ì´ˆ)")
            
            # 2. ëª¨ë¸ ë¡œë“œ (CPU ë¡œë“œ í›„ GPU ì´ë™)
            model_start = time.time()
            logger.info(f"  ğŸ“¥ [KF-DeBERTa] ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            # CPUì— ì§ì ‘ ë¡œë“œ (meta tensor ë¬¸ì œ ì™„ì „ íšŒí”¼)
            # torch_dtypeì„ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ dtypeìœ¼ë¡œ ì‹¤ì œ ë©”ëª¨ë¦¬ì— ë¡œë“œë¨
            self._model = AutoModel.from_pretrained(
                self.model_name,
                device_map=None,  # â­ device_map ì‚¬ìš© ì•ˆ í•¨
                low_cpu_mem_usage=False,
                torch_dtype=None  # â­ ëª…ì‹œì ìœ¼ë¡œ None ì§€ì •
            )
            
            # â­ ëª¨ë¸ì„ CPUë¡œ ëª…ì‹œì ìœ¼ë¡œ ì´ë™
            self._model = self._model.to('cpu')
            
            # â­ ë”ë¯¸ forward passë¡œ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì‹¤ì œ ë©”ëª¨ë¦¬ì— ë¡œë“œ
            # ì´ë ‡ê²Œ í•˜ë©´ meta tensorê°€ ì‹¤ì œ tensorë¡œ ë³€í™˜ë¨
            try:
                dummy_input = self._tokenizer("test", return_tensors="pt", padding=True, truncation=True)
                dummy_input = {k: v.to('cpu') for k, v in dummy_input.items()}
                with torch.no_grad():
                    _ = self._model(**dummy_input)
                logger.info("  âœ… [KF-DeBERTa] ë”ë¯¸ forward passë¡œ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì‹¤ì œ ë©”ëª¨ë¦¬ì— ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"  âš ï¸ [KF-DeBERTa] ë”ë¯¸ forward pass ì‹¤íŒ¨ (ë¬´ì‹œ ê°€ëŠ¥): {e}")
            
            self._device = "cpu"
            
            # GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)
            if target_device == "cuda" and torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                    self._model = self._model.to("cuda")
                    self._device = "cuda"
                    logger.info(f"  âœ… [KF-DeBERTa] ëª¨ë¸ GPU ì´ë™ ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"  âš ï¸ [KF-DeBERTa] GPU ì´ë™ ì‹¤íŒ¨, CPU ì‚¬ìš©: {e}")
                    self._device = "cpu"
            
            model_time = time.time() - model_start
            logger.info(f"  âœ… [KF-DeBERTa] ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({model_time:.2f}ì´ˆ, device: {self._device})")
            
            # 3. í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self._model.eval()
                
        except Exception as e:
            logger.error(f"âŒ [KF-DeBERTa] ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            # ëª¨ë¸ì´ Noneì´ ë˜ì§€ ì•Šë„ë¡ ì´ˆê¸°í™”
            self._model = None
            self._device = None
            raise
        
        total_time = time.time() - load_start
        logger.info(f"âœ… [KF-DeBERTa] ì „ì²´ ë¡œë”© ì™„ë£Œ (ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ)")
    
    @property
    def device(self) -> str:
        """í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        return self._device
    
    def _mean_pooling(self, model_output, attention_mask):
        """Mean pooling - ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ê³ ë ¤í•œ í‰ê·  í’€ë§"""
        token_embeddings = model_output[0]  # ì²« ë²ˆì§¸ ìš”ì†Œê°€ í† í° ì„ë² ë”©
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    def encode(
        self, 
        sentences: Union[str, List[str]], 
        batch_size: int = 32,
        convert_to_numpy: bool = True,
        normalize_embeddings: bool = True,
        show_progress_bar: bool = False
    ) -> Union[np.ndarray, torch.Tensor]:
        """
        ë¬¸ì¥ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        
        Args:
            sentences: ë‹¨ì¼ ë¬¸ì¥ ë˜ëŠ” ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            convert_to_numpy: numpy ë°°ì—´ë¡œ ë³€í™˜ ì—¬ë¶€
            normalize_embeddings: ì„ë² ë”© ì •ê·œí™” ì—¬ë¶€
            show_progress_bar: ì§„í–‰ í‘œì‹œì¤„ í‘œì‹œ ì—¬ë¶€ (í˜„ì¬ ë¯¸êµ¬í˜„)
        
        Returns:
            ì„ë² ë”© ë²¡í„° (numpy ë°°ì—´ ë˜ëŠ” torch í…ì„œ)
        """
        # ë‹¨ì¼ ë¬¸ì¥ ì²˜ë¦¬
        if isinstance(sentences, str):
            sentences = [sentences]
        
        all_embeddings = []
        
        with torch.no_grad():
            for i in range(0, len(sentences), batch_size):
                batch = sentences[i:i + batch_size]
                
                # í† í¬ë‚˜ì´ì§•
                encoded_input = self._tokenizer(
                    batch,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors='pt'
                )
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                encoded_input = {k: v.to(self._device) for k, v in encoded_input.items()}
                
                # ëª¨ë¸ ì¶”ë¡ 
                model_output = self._model(**encoded_input)
                
                # Mean pooling
                embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])
                
                # ì •ê·œí™”
                if normalize_embeddings:
                    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
                
                all_embeddings.append(embeddings.cpu())
        
        # ê²°ê³¼ í•©ì¹˜ê¸°
        all_embeddings = torch.cat(all_embeddings, dim=0)
        
        if convert_to_numpy:
            return all_embeddings.numpy()
        
        return all_embeddings
    
    def to(self, device: str):
        """ë””ë°”ì´ìŠ¤ ì´ë™"""
        if device == "cuda" and torch.cuda.is_available():
            self._model = self._model.to("cuda")
            self._device = "cuda"
        else:
            self._model = self._model.to("cpu")
            self._device = "cpu"
        return self
    
    def get_sentence_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self._model.config.hidden_size


# ì „ì—­ ëª¨ë¸ ìºì‹œ
_direct_embedding_model = None

def get_direct_embedding_model(model_name: str = "upskyy/kf-deberta-multitask", device: str = None) -> DirectEmbeddingModel:
    """
    ì§ì ‘ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìºì‹±)
    
    Args:
        model_name: HuggingFace ëª¨ë¸ ì´ë¦„
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', None=ìë™)
    
    Returns:
        DirectEmbeddingModel ì¸ìŠ¤í„´ìŠ¤
    
    Raises:
        Exception: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ
    """
    global _direct_embedding_model
    
    if _direct_embedding_model is None:
        _direct_embedding_model = DirectEmbeddingModel(model_name=model_name, device=device)
    
    return _direct_embedding_model
